<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lu Blog</title>
    <description>© 2020. Powered by Jekyll &amp; Dactl
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 22 Oct 2020 22:06:01 +0100</pubDate>
    <lastBuildDate>Thu, 22 Oct 2020 22:06:01 +0100</lastBuildDate>
    <generator>Jekyll v3.8.7</generator>
    
      <item>
        <title>Test Driven Development with Tensorflow</title>
        <description>&lt;p class=&quot;lead&quot;&gt;(30 min) This post demonstrates how &lt;em&gt;Test Driven Development (TDD)&lt;/em&gt; applies to
developing Machine Learning frameworks such as Tensorflow&lt;/p&gt;
&lt;p&gt;&amp;lt;!–-break-–&amp;gt;&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;I came from a data science background, where I frequently see
colleagues or even tech leads writing codes that are not tested at all. Near
the end of a product cycle, the ‘prototyped’ code/ tensorflow model are dumped
to software engineers, where they are almost always re-written or accepted as
black-box programs. Needless to say, the pain comes when the model is deployed,
code/models naturally break and noone can even tell where things went wrong.
Data science is becoming more and more of an integral part of the team, so
developing bug-free, reproducible code should naturally be part of a data
scientist’s job.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Test Driven Development&lt;/em&gt; or TDD is a development technique where you must
first write a test that &lt;strong&gt;fails&lt;/strong&gt; before you write new functional code. It is a
key part of agile software development process. There are ample tutorials
online on how to do TDD, but I cannot find many resources on how to do TDD on
machine learning frameworks such as Tensorflow. Personally, I think a lot of
data scientists come from academia, where rapid prototyping is always prefered
over code quality. This post therefore serves to demonstrate how model
development process can actually be improved by using TDD as part of the
workflow.&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Oct 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/posts/tdd</link>
        <guid isPermaLink="true">http://localhost:4000/posts/tdd</guid>
        
        <category>practical</category>
        
        <category>coding</category>
        
        
      </item>
    
      <item>
        <title>Variational Inference Part 2: Black-Box VI</title>
        <description>&lt;p class=&quot;lead&quot;&gt;(30 min) Part 2 of 2 series on variational inference. This part dives into the more practical black-box variational inference. We discuss the REINFORCE algorithm and gradient variance reduction techniques (including the &lt;em&gt;neural baseline&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&amp;lt;!–-break-–&amp;gt;&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;In &lt;a href=&quot;http://tlublog.com/posts/vi1&quot;&gt;part I&lt;/a&gt; of the series, we explored the origin of variational inference. As a quick recap, the posterior distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta |x)$&lt;/code&gt; is intractable to compute, as we need to explicitly compute the evidence term:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(X)=\int_{\theta} p(x \mid \theta) p(\theta)d\theta$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;this integration over the parameter space is very very expensive.&lt;/p&gt;

&lt;p&gt;To address this problem, we cook up a &lt;em&gt;variational distribution&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(\theta)$&lt;/code&gt; that &lt;em&gt;approximates&lt;/em&gt; the posterior. The problem of finding the posterior distribution therefore becomes an &lt;strong&gt;optimisation problem&lt;/strong&gt;. Specifically, we wish to minimise:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\text{KL}(q(\theta) \| p(\theta \mid x))$$&lt;/code&gt;
we discussed the implication of the ordering between &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$p$&lt;/code&gt; inside the brackets. In order to perform the optimisation, we showed that the objective function above is equivalent to &lt;strong&gt;maximising&lt;/strong&gt; the &lt;em&gt;evidence lower bound&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^{*}=\operatorname{argmax}_{q \in Q}\text{ELBO} = \int_\theta q(\theta) \log p(\theta, x)d\theta - \int_\theta q(\theta)\log q(\theta)$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$ = \operatorname{argmax}_{q \in Q} \mathbb{E}_{\theta \sim q}[\log p(\theta, x)] + \mathcal{H}(q(\theta))\quad [1]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;which intuitively speaking contains the maximum likelihood term &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta, x)$&lt;/code&gt; that prefers point estimation, and the entropy term &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{H}(q(\theta))$&lt;/code&gt; that prefers &lt;em&gt;diffusive&lt;/em&gt; estimation. Also note how this ELBO can be estimated easily with Monte-Carlo sampling. Since by construction, we know how to generate samples from &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta, x)$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We then introduced an &lt;strong&gt;analytical method&lt;/strong&gt; called &lt;em&gt;mean-field&lt;/em&gt; approximation, that falls under the framework of ELBO maximisation. The major problems with the &lt;em&gt;mean-field&lt;/em&gt; approach are that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It places heavy constraint on what our variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(\theta)$&lt;/code&gt; (it assumes it to be fully factorised)&lt;/li&gt;
  &lt;li&gt;Like me, you probably do not want to go through pages of maths on differentiating some disgusting looking equations (checkout the maths on Bayesian Mixture Model or Latent Dirchlet Allocation if interested)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post describes the more modern techniques used to do variational inference: the &lt;strong&gt;black-box&lt;/strong&gt; family. Specifically, we will look at the &lt;em&gt;reparameterisation trick&lt;/em&gt; and the famous &lt;em&gt;REINFORCE&lt;/em&gt; algorithm. We will also be discussing some variance reduction techniques such as the &lt;em&gt;baseline&lt;/em&gt; method. To make this post less dry, I will also be including some &lt;em&gt;Pyro&lt;/em&gt; (probabilistic programming language based on &lt;em&gt;PyTorch&lt;/em&gt;) just to demonstrate how easy black-box VI is.&lt;/p&gt;

&lt;h1 id=&quot;2-theory-of-black-box-vi&quot;&gt;2. Theory of Black-Box VI&lt;/h1&gt;

&lt;h2 id=&quot;21-why-is-elbo-backprop-hard&quot;&gt;2.1 Why Is ELBO Backprop Hard?&lt;/h2&gt;

&lt;p&gt;One natural question to ask is - why can’t we just do backpropogation through this ELBO objective function? Let us first write down our ELBO (in a more workable format):&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\mathrm{ELBO} = \mathbb{E}_{q_{\phi}(\theta)}\left[\log p(x, \theta)-\log q_{\phi}(\theta)\right]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is not too much change from equation [1], we made it more explicit that our variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; is parameterised by &lt;code class=&quot;highlighter-rouge&quot;&gt;$\phi$&lt;/code&gt;. Form simplicity, let’s call variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; the &lt;strong&gt;guide&lt;/strong&gt; (this is commonly used in literature). The problem is that &lt;strong&gt;ELBO is an expectation&lt;/strong&gt;, so we need to be able to compute unbiased estimates of&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nabla_{\theta, \phi} \mathrm{ELBO}=\nabla_{\theta, \phi} \mathbb{E}_{q_{\phi}(\theta)}\left[\log p(x, \theta)-\log q_{\phi}(\theta)\right]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, we &lt;strong&gt;can’t move the grad inside the expectation&lt;/strong&gt;, since our expectation is taken under our guide &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; that depends on &lt;code class=&quot;highlighter-rouge&quot;&gt;$\phi$&lt;/code&gt;. This problem can be framed in a more generic way, we can drop the distinction between &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\phi$&lt;/code&gt;, also let’s use &lt;code class=&quot;highlighter-rouge&quot;&gt;$f_\phi(\theta)$&lt;/code&gt; to denote any arbituary function within the bracket:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nabla_{\phi} \mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]  \quad [2]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Equation [2] describes the &lt;em&gt;gradient through an expectation&lt;/em&gt; and can be also found in the policy gradient algorithm in reinforcement learning. The remaining of the post aims to obtain &lt;strong&gt;unbiased estimates&lt;/strong&gt; of equation [2].&lt;/p&gt;

&lt;h2 id=&quot;22-reparameterisation-trick&quot;&gt;2.2 Reparameterisation Trick&lt;/h2&gt;

&lt;p&gt;The reparameterisation trick also known as path-wise gradients allow us to compute equation [2] by rewritting samples from the guide &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; in terms of a noise variable &lt;code class=&quot;highlighter-rouge&quot;&gt;$\epsilon$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]=\mathbb{E}_{q(\epsilon)}\left[f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;concretely, &lt;code class=&quot;highlighter-rouge&quot;&gt;$\epsilon \sim q(\epsilon)$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta = g_{\phi}(\epsilon)$&lt;/code&gt;. Crucially all the &lt;code class=&quot;highlighter-rouge&quot;&gt;$\phi$&lt;/code&gt; dependent terms have been moved inside of the expectation. This kind of reparameterisation can be done for many distributions (e.g. the normal distribution). An example of such reparameterisation can be highlighted by assuming that &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; is sampled from a Gaussian &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta \sim \mathcal{N}(\mu, \sigma)$&lt;/code&gt;. The function &lt;code class=&quot;highlighter-rouge&quot;&gt;$g_{\phi}(\varepsilon)$&lt;/code&gt; can then be expressed as&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$g_{\phi}(\varepsilon)=\mu_{\phi}+\varepsilon \sigma_{\phi}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\epsilon \sim \mathcal{N}(0, 1)$&lt;/code&gt;. Assuming &lt;code class=&quot;highlighter-rouge&quot;&gt;$f(.)$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$g(.)$&lt;/code&gt; are sufficiently smooth, we can now get unbiased estimates of the gradient of interest by taking a Monte Carlo estimate of this expectation:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nabla_{\phi} \mathbb{E}_{q(\epsilon)}\left[f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]=\mathbb{E}_{q(\epsilon)}\left[\nabla_{\phi} f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this case, not only do &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; need to come from specific distributions (i.e. the exponential family), we also need to be able to differentiate through &lt;code class=&quot;highlighter-rouge&quot;&gt;$f_\phi$&lt;/code&gt;. In the case where we have discrete latent variables, the gradient of &lt;code class=&quot;highlighter-rouge&quot;&gt;$f_\phi$&lt;/code&gt; is zero almost everywhere in the parameter space.&lt;/p&gt;

&lt;h2 id=&quot;23-reinforce-for-non-reparameterizable-random-variables&quot;&gt;2.3 REINFORCE for Non-Reparameterizable Random Variables&lt;/h2&gt;

&lt;p&gt;I am introducing REINFORCE under the scope of variational inference, however, this trick is most commonly seen in reinforcement learning (policy gradient). The concept of REINFORCE is quiet simple: &lt;strong&gt;re-write equation [2] to some form so we can obtain unbiased estimate via Monte-Carlo&lt;/strong&gt;. Specifically, we want to message a gradient of expectation into expectation of some gradient. We begin by expanding the terms:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nabla_{\phi}\text{ELBO}=\nabla_{\phi} \mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]=\nabla_{\phi} \int d\theta q_{\phi}(\theta) f_{\phi}(\theta)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;applying chain rule:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\int d \theta\left[\left(\nabla_{\phi} q_{\phi}(\theta)\right) f_{\phi}(\theta)+q_{\phi}(\theta)\left(\nabla_{\phi} f_{\phi}(\theta)\right)\right]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;next, we use the log-derivative trick:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nabla_{\phi} q_{\phi}(\theta)=q_{\phi}(\theta) \nabla_{\phi} \log q_{\phi}(\theta)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;to obtain&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nabla_{\phi}\text{ELBO}=\mathbb{E}_{q_{\phi}(\theta)}\left[\left(\nabla_{\phi} \log q_{\phi}(\theta)\right) f_{\phi}(\theta)+\nabla_{\phi} f_{\phi}(\theta)\right] \quad [3]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Equation [3] is the REINFORCE equation. It is quiet disgusting looking and I do not have any intuition to what it represent (frankly, the ELBO objective took me a while to digest). But just by looking at it, we can now obtain unbiased estimates of our ELBO gradient! One way to package this result (for implementation) is by introducing a surrogate function:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\text { surrogate objective }= \log q_{\phi}(\theta) \overline{f_{\phi}(\theta)}+f_{\phi}(\theta)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;to be passed through the autograd, where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\overline{f_{\phi}(\theta)}$&lt;/code&gt; is held as a constant (detached during autograd). Equation [3] therefore becomes:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nabla_{\phi} \mathrm{ELBO}=\mathbb{E}_{q_{\phi}(\theta)}\left[\nabla_{\phi}(\text { surrogate objective })\right] \quad [4]$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It would be good to finish the story here. Unfortunately, equation [4] suffers from &lt;strong&gt;high variance&lt;/strong&gt; for a range of &lt;code class=&quot;highlighter-rouge&quot;&gt;$f(.)$&lt;/code&gt;. So in although we can theoretically obtain unbiased estimates of the true gradient, in reality, noone is going to wait for 10,000 samples per backprop iteration. We therefore &lt;strong&gt;need ways to reduce the variance of our estimated ELBO gradient&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&quot;3-practical-vi-with-pyro&quot;&gt;3. Practical VI with Pyro&lt;/h1&gt;
&lt;p&gt;Let’s demonstrate how black-box VI actually works using Pyro. We can build a very dumb model that estimates whether a coin is biased based on the number of tosses. The good thing with this model is that - there exists a analytical solution. Unfortunately, this blog is not about Pyro, so the code won’t be explained in depth. However, the code is pretty much copy-pasted from the &lt;a href=&quot;https://pyro.ai/examples/svi_part_ii.html&quot;&gt;Pyro tutorial&lt;/a&gt; itself. And frankly Pyro is so high level, anyone who know some PyTorch/numpy can understand it.&lt;/p&gt;

&lt;p&gt;Say we wish to find the posterior distribution of the parameter &lt;code class=&quot;highlighter-rouge&quot;&gt;$p$&lt;/code&gt;, which governs the probability that a coin toss yields a head:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$X\sim \text{Bernouli}(p)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we place beta prior on &lt;code class=&quot;highlighter-rouge&quot;&gt;$p$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p\sim \text{Beta}(10, 10)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;first, we toss the coin 10 times and observe 6 heads and 4 tails:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributions.constraints&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.infer&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trace_ELBO&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.distributions&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N_STEPS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# number of gradient update
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_HEADS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# observed number of heads
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_TAILS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# clear param store
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clear_param_store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create some data with some observed heads and some observed tails
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_HEADS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_TAILS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_HEADS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_HEADS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We then build this &lt;strong&gt;generative&lt;/strong&gt; model describing how we may draw coin toss observations from the prior:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alpha0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;beta0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;latent_fairness&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data_loop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# observe datapoint i using the bernoulli likelihood
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;obs_{}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;with &lt;em&gt;latent_fairness&lt;/em&gt; denoting &lt;code class=&quot;highlighter-rouge&quot;&gt;$p$&lt;/code&gt;, the parameter whose distribution we are interested in estimating. Note we used &lt;code class=&quot;highlighter-rouge&quot;&gt;pyro.plate&lt;/code&gt; to mark conditional independence in our observations, this is because &lt;code class=&quot;highlighter-rouge&quot;&gt;$x\in X$&lt;/code&gt; are i.i.d observations. Not only is this step important for model scaling (many observations), but it also helps with ELBO gradient variance reduction (see section below).&lt;/p&gt;

&lt;p&gt;The analytical posterior can be calculated:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$P(p|X)\sim \text{Beta}(10+h, 10+t)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;$h$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$t$&lt;/code&gt; are the number of observed head and tails respectively.&lt;/p&gt;

&lt;p&gt;For the purpose of demonstrating SVI, we specify a &lt;em&gt;variational posterior distribution&lt;/em&gt;, also called a guide in pyro &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;, where&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q\sim \text{Beta}(\alpha, \beta)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we need to ensure that &lt;code class=&quot;highlighter-rouge&quot;&gt;$\alpha$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\beta$&lt;/code&gt; are constrained to be positive&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;alpha_q&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;15.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;constraint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;positive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;beta_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;beta_q&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;15.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;constraint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;positive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# sample latent_fairness from the distribution Beta(alpha_q, beta_q)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;latent_fairness&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;it is important that in the guide, parameters names match for those we want to approximate&lt;/strong&gt; (in this case, the &lt;em&gt;latent_fairness&lt;/em&gt;). Finally, we use Adam optimiser, specifying ELBO loss and just do inference!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# setup the optimizer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adam_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00025&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;betas&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adam_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# setup the inference algorithm
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Trace_ELBO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_STEPS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# do gradient steps
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N_STEPS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;svi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s it! To evaluate model correctness:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# grab the learned variational parameters
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;alpha_q&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;beta_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;beta_q&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inferred_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inferred_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inferred_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Posterior alpha and beta values are {:.1f} and {:.1f} respectively&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                                                    &lt;span class=&quot;n&quot;&gt;beta_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;The fairness of the coin is {:.3f} ± {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inferred_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inferred_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We get:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Posterior alpha and beta values are 16.1 and 13.9 respectively&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;The fairness of the coin is 0.536 ± 0.090&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;which is pretty much the same as our analytical result (16 and 14 for the posterior alpha, beta values).&lt;/p&gt;

&lt;h1 id=&quot;4-elbo-gradient-variance-reduction&quot;&gt;4. ELBO Gradient Variance Reduction&lt;/h1&gt;

&lt;h2 id=&quot;41-rao-blackwellization&quot;&gt;4.1 Rao-Blackwellization&lt;/h2&gt;

&lt;h2 id=&quot;42-baselines&quot;&gt;4.2 Baselines&lt;/h2&gt;

&lt;p&gt;Another strategy for reducing variance in the ELBO gradient estimator goes under the name of baselines. The key point is to note that for any constant &lt;code class=&quot;highlighter-rouge&quot;&gt;$b$&lt;/code&gt;,&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\mathbb{E}_{q_{\phi}(\theta)}\left[\nabla_{\phi}\left(\log q_{\phi}(\theta) \times b\right)\right]=0$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;the proof can be done in one line:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\mathbb{E}_{q_{\phi}(\theta)}\left[\nabla_{\phi} \log q_{\phi}(\theta)\right]=\int d \theta q_{\phi}(\theta) \nabla_{\phi} \log q_{\phi}(\theta)=\int d \theta \nabla_{\phi} q_{\phi}(\theta)=\nabla_{\phi} \int d \theta q_{\phi}(\theta)=\nabla_{\phi} 1=0$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;hence we can replace our surrogate objective&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\log q_{\phi}(\theta) \overline{f_{\phi}(\theta)}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\log q_{\phi}(\theta)\left( \overline{f_{\phi}(\theta)} - b\right)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Therefore, the mean of our gradient estimator does not change, but the variance gets reduced (it’s a bit more involved to show this, but intuitively, if we track the mean of &lt;code class=&quot;highlighter-rouge&quot;&gt;$\overline{f_{\phi}(\theta)}$&lt;/code&gt; then the variance should decrease).&lt;/p&gt;

&lt;p&gt;So our variance reduction task becomes predicting what &lt;code class=&quot;highlighter-rouge&quot;&gt;$\overline{f_{\phi}(\theta)}$&lt;/code&gt; is at each time-step. One simplest baseline is to simply perform a moving average of recent samples of &lt;code class=&quot;highlighter-rouge&quot;&gt;$\overline{f_{\phi}(\theta)}$&lt;/code&gt;. In Pyro, this looks like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;My_Dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;infer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;baseline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'use_decaying_avg_baseline'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                     &lt;span class=&quot;s&quot;&gt;'baseline_beta'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Of course, since the task to reduce ELBO variance essentially becomes prediction &lt;code class=&quot;highlighter-rouge&quot;&gt;$\overline{f_{\phi}(\theta)}$&lt;/code&gt;, why not use an auxiliary neural-network to do this for us? The idea of &lt;strong&gt;neural-baseline&lt;/strong&gt; is born. This in Pyro is not too much work. First, we construct some multilayer perceptron (or any DNN of your choice):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BaselineNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# add more layers/activations
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;for our variational distribution (guide), we simply define:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;baseline_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaselineNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_baseline&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;baseline_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ... other computations ...
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;My_Dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;infer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;baseline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nn_baseline'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;baseline_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                         &lt;span class=&quot;s&quot;&gt;'nn_baseline_input'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Under the hood Pyro constructs a loss of the form:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\text { baseline loss } \equiv(\overline{f_{\phi}(\mathbf{z})}-b)^{2}$$&lt;/code&gt;
which intuitively makes sense. However, there is no theorem that suggests this is the optimal loss function to use in this context, but in practice works well.&lt;/p&gt;

&lt;p&gt;Also in practice it can be important to use a different set of learning hyperparameters for baseline parameters (normally, the lr for baseline is higher than the actual model). In Pyro this is done through:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;per_param_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'baseline'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'baseline'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;per_param_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;The main take-away from this post is that - ELBO gradient is not straight forward to obtain. In a lot of cases, we turn this gradient of expectation into an expectation of some gradient (the REINFORCE algorithm). This makes ELBO gradient &lt;em&gt;sampled/estimated&lt;/em&gt;, not &lt;em&gt;analytically obtained&lt;/em&gt;, if unfortunately suffers from from high variance. To combat this, we &lt;strong&gt;mark all conditional independence when we can&lt;/strong&gt;, and when that fails, we create a &lt;em&gt;baseline&lt;/em&gt; that somehow estimates values of the function at each update-step.&lt;/p&gt;

&lt;p&gt;Pyro makes all the aforementioned stuff easy. The &lt;code class=&quot;highlighter-rouge&quot;&gt;SVI&lt;/code&gt; class implements ELBO gradient re-parameterisation when possible, and handles the REINFORCE algorithm automatically. Variational inference therefore is almost identical to normal gradient update.&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Jul 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/posts/vi2</link>
        <guid isPermaLink="true">http://localhost:4000/posts/vi2</guid>
        
        <category>theory</category>
        
        <category>bayesian</category>
        
        
      </item>
    
      <item>
        <title>Few-shot Learning</title>
        <description>&lt;p class=&quot;lead&quot;&gt;(30 min) Literature review of popular methods in few-shot learning. With a focus on metric-learning and meta-learning.&lt;/p&gt;
&lt;p&gt;&amp;lt;!–-break-–&amp;gt;&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;It is an understatement to say that modern Deep Neural Networks (DNNs) require large amount of data during training. This can be shown both in practice and through &lt;a href=&quot;https://mostafa-samir.github.io/ml-theory-pt2/&quot;&gt;Learning Theory&lt;/a&gt;. For example, the classic benchmark datasets such as &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt; contains 14 million images (for 1000 classes), language models such as &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;GTP3&lt;/a&gt; are literally trained on THE INTERNET.&lt;/p&gt;

&lt;p&gt;While these DNNs’ performance have been impressive, occasionally achieving super-human level accuracy/performance. In real life, data is not as abundant and easy to obtain. The problem is exacerbated when &lt;strong&gt;labelled&lt;/strong&gt; data is required (because £££). Another key challenge in a real-life setting is that model output requirement changes over time. For example, imagine working for an identity-document classification company; at launch, your company may only support 4 types of IDs. Over time, the number of supported IDs will grow. Re-training the model itself is an art and can be very time consuming.&lt;/p&gt;

&lt;p&gt;When quality data is scarce and model reusability is key, &lt;strong&gt;few-shot&lt;/strong&gt; learning have been employed successfully to discover patterns in data and make beneficial predictions. When combined with &lt;strong&gt;active learning&lt;/strong&gt; can often bring a competitive edge at the inception of an ML company!&lt;/p&gt;

&lt;h1 id=&quot;2-problem-definition-and-datasets&quot;&gt;2. Problem Definition and Datasets&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Few-shot classification&lt;/em&gt; is an instantiation of meta-learning in the field of supervised learning (I think the distinction between few-shot and meta-learning is rather arbitrary). The best way to understand what few-shot learning is trying to achieve is by examining a few common datasets used to train and evaluate few-shot models. Let’s begin by looking at the &lt;em&gt;K-shot N-class classification task&lt;/em&gt;:&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;em&gt;K-shot N-class classification task&lt;/em&gt;: during training, we are provided with &lt;strong&gt;several datasets&lt;/strong&gt; each containing &lt;strong&gt;&lt;em&gt;N&lt;/em&gt; classes with &lt;em&gt;K&lt;/em&gt; examples&lt;/strong&gt;, at test time, we are given &lt;strong&gt;a new dataset&lt;/strong&gt;, and asked to make predictions based on the labelled examples.&lt;/p&gt;

&lt;p&gt;The term few-shot (or &lt;em&gt;K&lt;/em&gt;-shot) therefore means the number of examples provided per class (or task). Pictorially, this is shown as a &lt;strong&gt;dataset of datasets&lt;/strong&gt;:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://i.filmot.com/XJtP39p.png&quot; alt=&quot;drawing&quot; width=&quot;850&quot; /&gt;
&lt;figcaption&gt;
Fig 1. Typical dataset used for evaluating few-shot models. Note the unusualness of this &quot;dataset of datasets&quot;. Image is taken from &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html&quot;&gt;LiLian's blog&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/center&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;To be more concrete, the &lt;a href=&quot;https://github.com/brendenlake/omniglot/&quot;&gt;Omniglot&lt;/a&gt; dataset contains 50 different alphabets, each containing different number (around 20) characters. Each character is drawn by 20 people, amounting to a total of 1623 characters.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://i.filmot.com/qKEb5Mn.png&quot; alt=&quot;drawing&quot; width=&quot;700&quot; /&gt;
&lt;figcaption&gt;
Fig 2. Omniglot dataset
&lt;/figcaption&gt;
&lt;/center&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Going back to the earlier definition, each alphabet’s OCR problem can be characterised as an &lt;em&gt;S&lt;/em&gt;-class 20-shots. Where &lt;em&gt;S&lt;/em&gt; corresponds to the size of each alphabet.&lt;/p&gt;

&lt;p&gt;One of the most challenging dataset used for few-shots learning is the &lt;a href=&quot;https://www.kaggle.com/whitemoon/miniimagenet&quot;&gt;mini-ImageNet&lt;/a&gt; which contains 100 classes, each with only 600 examples. It is worth mentioning, the way train-val-test split for few-shots learning is &lt;strong&gt;done on the classes&lt;/strong&gt;, in this case, we typically have a 64-16-20 split.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://i.filmot.com/79Yb3nb.png&quot; alt=&quot;drawing&quot; width=&quot;700&quot; /&gt;
&lt;figcaption&gt;
Fig 3. Mini-ImageNet dataset
&lt;/figcaption&gt;
&lt;/center&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Hopefully I have conveyed how challenging few-shot learning is. Personally, I think few-shot is more general than the typical supervised DNNs. People normally call this type of learning “learning to learn”, since our model needs to either have weights/architecture that generalises across different tasks, or to be able to rapidly change the weights to adapt to a new task.&lt;/p&gt;

&lt;h1 id=&quot;3-methodsliterature-overview&quot;&gt;3. Methods/Literature Overview&lt;/h1&gt;

&lt;p&gt;One rather obvious (though practically difficult) thing to achieve few-shots learning is through &lt;strong&gt;data augmentation&lt;/strong&gt;. The motivation is simple: we do not have enough example data per class, let’s just generate more. People can really go crazy on data augmentation, recent methods use ideas such as &lt;a href=&quot;https://papers.nips.cc/paper/7504-metagan-an-adversarial-approach-to-few-shot-learning&quot;&gt;GANs&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1904.03472&quot;&gt;Sailency-guided Hallucinations&lt;/a&gt;. Since we can always perform data augmentation in conjunction with other approaches (and frankly, the concept is not new), this class of method will not be discussed in this post.&lt;/p&gt;

&lt;p&gt;The other approach which is done by most ML practitioners is &lt;strong&gt;transfer learning&lt;/strong&gt;. The idea of taking an off-the-shelf model trained by big research groups on millions of data points, then fine tune the last few layer’s weights on our own dataset is definitely not new. Therefore, we will also omit discussing these. In fact, for most methods demonstrated, assume we always do some sort of transfer learning via models like VGG net.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metric-based learning&lt;/strong&gt; such as the &lt;a href=&quot;https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf&quot;&gt;&lt;em&gt;Siamese networks&lt;/em&gt;&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1606.04080&quot;&gt;&lt;em&gt;Matching networks&lt;/em&gt;&lt;/a&gt; will be described in depth here. The fundamental idea is that these methods learn a distance metric between the querying data point and the support set (labelled examples).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimisation-based learning&lt;/strong&gt; such as the &lt;a href=&quot;https://openreview.net/pdf?id=rJY0-Kcll&quot;&gt;&lt;em&gt;LSTM Meta-Learner&lt;/em&gt;&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1703.03400&quot;&gt;&lt;em&gt;Model-Agnostic Meta-Learning&lt;/em&gt;&lt;/a&gt; modifies the classic gradient descent. Encouraging the loss function to converge within a small number of optimisation steps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model-based learning&lt;/strong&gt; designs models specifically for fast-learning. This is normally achieved through making the network having explicit memories (much like Neural Turing Machines). These models are interesting but because they are quiet contrived, we will omit their discussions here.&lt;/p&gt;

&lt;p&gt;Finally, we will briefly look at &lt;strong&gt;Bayesian methods&lt;/strong&gt; for few-shot learning, including &lt;a href=&quot;http://bayesiandeeplearning.org/2018/papers/10.pdf&quot;&gt;VERSA&lt;/a&gt; by Richard Turner et al.&lt;/p&gt;

&lt;h1 id=&quot;4-metric-based-approach&quot;&gt;4. Metric-Based Approach&lt;/h1&gt;

&lt;p&gt;The core-idea behind metric-based few-shot approaches can be thought as performing &lt;strong&gt;mixture modelling&lt;/strong&gt; with some deep network. Let’s define a support set &lt;code class=&quot;highlighter-rouge&quot;&gt;$S = \{X, Y\}$&lt;/code&gt; which is a set of &lt;strong&gt;labelled examples&lt;/strong&gt; during training and/or inference. At a high-level, the metric-based approach produces outputs as weighted sums of support set labels:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$P_{\theta}(y \mid \mathbf{x}, S)=\sum_{\left(\mathbf{x}_{i}, y_{i}\right) \in S} k_{\theta}\left(\mathbf{x}, \mathbf{x}_{i}\right) y_{i}\quad\quad [1]$$&lt;/code&gt;
where the weight is generated by some kernel function &lt;code class=&quot;highlighter-rouge&quot;&gt;$k_\theta$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;41-siamese-network&quot;&gt;4.1 Siamese Network&lt;/h2&gt;
&lt;p&gt;The most naive form of metric-based few-shot learning is the Siamese network. Where we simply use a CNN &lt;code class=&quot;highlighter-rouge&quot;&gt;$f_\theta$&lt;/code&gt; (i.e. VGG network) to extract features from two images. Then &lt;code class=&quot;highlighter-rouge&quot;&gt;$l_1$&lt;/code&gt; (or any other differentiable distance metric like cosine distance) is computed between the two the embeddings:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/Ei8dDIO.png&quot; alt=&quot;drawing&quot; width=&quot;450&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The distance is converted to a pseudo-probability by a linear feedforward layer and a sigmoid (no theoretical grounds here…). To put the animation above in a mathematical form (with &lt;code class=&quot;highlighter-rouge&quot;&gt;$l_1$&lt;/code&gt; distance metric):
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p\left(x_{i}, x_{j}\right)=\sigma\left(\mathbf{W}\left|f_{\theta}\left(x_{i}\right)-f_{\theta}\left(x_{j}\right)\right|\right)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathbf{W}$&lt;/code&gt; can be trained in conjunction with the CNN via binary cross entropy loss (whether the two images are of the same class or not).&lt;/p&gt;

&lt;p&gt;To classify a query image, we do a ‘brute-force search’, by running our query image and &lt;em&gt;every class of support images&lt;/em&gt; through the network above. Final prediction is simply an argmax over the &lt;em&gt;probability that the query image has the same class as the support image&lt;/em&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\hat{c}_{S}(x)=c\left(\arg \max _{x_{i} \in S} p\left(x, x_{i}\right)\right)$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$c(x)$&lt;/code&gt; is the class label of an image &lt;code class=&quot;highlighter-rouge&quot;&gt;$x$&lt;/code&gt;. This is animated below:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/n2ZygDK.png&quot; alt=&quot;drawing&quot; width=&quot;450&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Of course, you can shoe-horn the formula above into the format of equation &lt;code class=&quot;highlighter-rouge&quot;&gt;$[1]$&lt;/code&gt; by performing softmax over all the &lt;code class=&quot;highlighter-rouge&quot;&gt;$p\left(x, x_{i}\right)$&lt;/code&gt; pairs (i.e. re-normalising). But this does not change the performance of the method.&lt;/p&gt;

&lt;h2 id=&quot;42-relation-network&quot;&gt;4.2 Relation Network&lt;/h2&gt;

&lt;p&gt;One obvious downside of the Siamese network is the choice of the distance function being quiet arbitrary. Why did we use &lt;code class=&quot;highlighter-rouge&quot;&gt;$l_1$&lt;/code&gt; over &lt;code class=&quot;highlighter-rouge&quot;&gt;$l_2$&lt;/code&gt;? One other disadvantage is that distances are calculated independently across the support set, which ignores higher-level interaction between the query image and the support set.&lt;/p&gt;

&lt;p&gt;Relation network overcomes these disadvantages by using another convolution network acting as the distance function. It’s inputs are the concatenated features between the query image and &lt;strong&gt;all images in the support set&lt;/strong&gt; (one per class). This is animated below:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/UbMjRGh.png&quot; alt=&quot;drawing&quot; width=&quot;450&quot; /&gt;&lt;/center&gt;

&lt;p&gt;One note on the loss function, since we are more interested in the relationship between the query image and the support set, which is more like a regression; MSE loss is used instead of cross entropy:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\mathcal{L}=\sum_{\left(x_{i}, \mathbf{x}_{j}, y_{i}, y_{j}\right) \in S}\left(g_\theta(z)-1_{y_{i}=y_{j}}\right)^{2}$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$z$&lt;/code&gt; is the concatenated output by the feature extractor &lt;code class=&quot;highlighter-rouge&quot;&gt;$f_\phi$&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;42-prototyping-networks&quot;&gt;4.2 Prototyping Networks&lt;/h2&gt;

&lt;p&gt;One major disadvantage of both methods above is that &lt;strong&gt;we have been using one example image per class&lt;/strong&gt;. This is a &lt;code class=&quot;highlighter-rouge&quot;&gt;$K$&lt;/code&gt;-shot learning when &lt;code class=&quot;highlighter-rouge&quot;&gt;$K=1$&lt;/code&gt;, i.e. 1-shot learning. We typically want a model that increases in performance as labelled examples increase (larger and larger &lt;code class=&quot;highlighter-rouge&quot;&gt;$K$&lt;/code&gt; and different numbers of &lt;code class=&quot;highlighter-rouge&quot;&gt;$K$&lt;/code&gt; per class). Methods such as &lt;strong&gt;Matching networks&lt;/strong&gt; use an (Bi-)LSTM to ‘read’ the entirety of the query-set.&lt;/p&gt;

&lt;p&gt;A simpler, more intuitive method is the idea of &lt;em&gt;prototyping&lt;/em&gt;. This idea is similar to manifold learning, where a &lt;em&gt;prototype&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$v_c$&lt;/code&gt; is the &lt;strong&gt;average&lt;/strong&gt; of all examples of the same class &lt;code class=&quot;highlighter-rouge&quot;&gt;$S_c$&lt;/code&gt; in some latent space:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$v_{c}=\frac{1}{\left|S_{c}\right|} \sum_{\left(x_{i}, y_{i}\right) \in S_{c}} f_{\theta}\left(x_{i}\right)$$&lt;/code&gt;
In the animation below, we see how a &lt;strong&gt;prototyping network&lt;/strong&gt; works, with red squares on the latent map (right) representing prototypes for the three different classes.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/H4wVk7g.png&quot; alt=&quot;drawing&quot; width=&quot;700&quot; /&gt;&lt;/center&gt;

&lt;p&gt;At test-time, the distribution over classes for a given test input &lt;code class=&quot;highlighter-rouge&quot;&gt;$x$&lt;/code&gt; is a softmax over the inverse of distances between the test data embedding and prototype vectors:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(y=c \mid x)=\operatorname{softmax}\left(-d_{\varphi}\left(f_{\theta}(x), v_{c}\right)\right)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is slightly more general than just picking the argmin over distances between prototypes and the query image’s latent embedding. Since the distance function &lt;code class=&quot;highlighter-rouge&quot;&gt;$d_\psi$&lt;/code&gt; can be another neural network with learnt weights.&lt;/p&gt;

&lt;h1 id=&quot;5-optimisation-based-approach&quot;&gt;5. Optimisation-Based Approach&lt;/h1&gt;

&lt;p&gt;Another way to tackle few-shot learning is through modifying how parameters are updated within DL models. This approach is termed optimisation-based. The key concept is these methods utilise &lt;strong&gt;another network&lt;/strong&gt;, i.e. a meta network, that &lt;strong&gt;learns how to train a deep learning model&lt;/strong&gt; across different tasks.&lt;/p&gt;

&lt;h2 id=&quot;51-lstm-meta-learner&quot;&gt;5.1 LSTM Meta-Learner&lt;/h2&gt;

&lt;p&gt;The standard optimisation algorithms used to train deep neural networks take the form:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\theta_{t}=\theta_{t-1}-\alpha_{t} \nabla_{\theta_{t-1}} \mathcal{L}_{t}$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta_{t-1}$&lt;/code&gt; are the parameters of the model after &lt;code class=&quot;highlighter-rouge&quot;&gt;$t-1$&lt;/code&gt; updates, &lt;code class=&quot;highlighter-rouge&quot;&gt;$\alpha_t$&lt;/code&gt; is the learning rate at time &lt;code class=&quot;highlighter-rouge&quot;&gt;$t$&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{L}_{t}$&lt;/code&gt; is the loss optimised by the model and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\nabla_{\theta_{t-1}} \mathcal{L}_{t}$&lt;/code&gt; denotes the gradient of the loss. &lt;a href=&quot;https://openreview.net/pdf?id=rJY0-Kcll&quot;&gt;Ravi &amp;amp; Larochelle (2017)&lt;/a&gt; observed that, this update resembles the update for the cell state in an LSTM::
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t}$$&lt;/code&gt;
if &lt;code class=&quot;highlighter-rouge&quot;&gt;$f_{t}=1, c_{t-1}=\theta_{t-1}, i_{t}=\alpha_{t}$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\tilde{c}_{t}=-\nabla_{\theta_{t-1}} \mathcal{L}_{t}$&lt;/code&gt;. Thus, they proposed training a meta-learner LSTM to learn an update rule for training a learner network. The cell state of the LSTM is the parameters of the learner i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;$c_{t}=\theta_{t}$&lt;/code&gt;, and the candidate cell state &lt;code class=&quot;highlighter-rouge&quot;&gt;$\tilde{c}_{t}=\nabla_{\theta_{t-1}} \mathcal{L}_{t}$&lt;/code&gt;.
The best way to understand the LSTM meta-learner is by first looking at the animation below and then the algorithm shown in the original paper:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/labmcjV.png&quot; alt=&quot;drawing&quot; width=&quot;800&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/NdCIkmQ.png&quot; alt=&quot;drawing&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{D}_{meta-train}$&lt;/code&gt; dataset is the dataset of datasets we discussed in the second section. Basically, we sample &lt;strong&gt;a dataset&lt;/strong&gt; (1 shot, 5 class in the animation above) and use that to update the &lt;strong&gt;learner&lt;/strong&gt;’s parameters via the meta-learner (whose parameters are fixed). We then use the &lt;strong&gt;test set&lt;/strong&gt; of the sampled dataset to &lt;strong&gt;update meta-learner’s parameters&lt;/strong&gt;. It is a bit weird to train on the test set. But remember in this setting, our true test set is another dataset of datasets. We repeat the procedure above by sampling another dataset from &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{D}_{meta-train}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One key challenge for this method is - the learner has tens of thousands of parameters, which the meta-learner needs to output. To prevent an explosion of meta-learner parameters, they employed some sort of parameter sharing. I.e. meta-learner only modifies a few “key layers” within the learner network. The original paper has more in-depth description on how this is achieved.&lt;/p&gt;

&lt;h2 id=&quot;52-maml&quot;&gt;5.2 MAML&lt;/h2&gt;

&lt;p&gt;MAML stands for &lt;strong&gt;Model-Agnostic Meta-Learning&lt;/strong&gt; (&lt;a href=&quot;https://arxiv.org/abs/1703.03400&quot;&gt;Finn, et al. 2017&lt;/a&gt;), is a fairly general optimisation algorithm, that can be used not only for few-shot classification, but also generalises to reinforcement learning. It is quiet easy to implement also, and achieves SOTA (or used to) on a variety of tasks.&lt;/p&gt;

&lt;p&gt;The high level ideal learning for MAML is updating parameters so that we always &lt;strong&gt;update towards &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta^*$&lt;/code&gt; which are the parameters that are “closest” to the optimal parameters for all the subtasks&lt;/strong&gt;. Hence fine-tuning is more efficient. Pictorially, this is shown in the animation below, where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta_i^*$&lt;/code&gt; are optimal parameters for the &lt;code class=&quot;highlighter-rouge&quot;&gt;$i$&lt;/code&gt;th task.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/bZ9bIvV.png&quot; alt=&quot;drawing&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The algorithm for MAML is very simple. Algorithm below is taken from the original paper. Unfortunately, a different set of notations are used, but &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\mathcal{T})$&lt;/code&gt; here literally means the meta training dataset (dataset of datasets) as in meta-LSTM.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/xp0g0NY.png&quot; alt=&quot;drawing&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
</description>
        <pubDate>Sun, 21 Jun 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/posts/few-shot</link>
        <guid isPermaLink="true">http://localhost:4000/posts/few-shot</guid>
        
        <category>practical</category>
        
        <category>deep-learning</category>
        
        
      </item>
    
      <item>
        <title>Variational Inference Part 1: Intro and Mean-Field</title>
        <description>&lt;p class=&quot;lead&quot;&gt;(30 min) Part 1 of 2 series on variational inference. This part first introduces high-level concepts and the mean-field approximation. We then demonstrate the mean-field theory on a Bayesian Gaussian Mixture Model (Bayesian GMM).&lt;/p&gt;
&lt;p&gt;&amp;lt;!–-break-–&amp;gt;&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Bayesian inference offers an attractive story with principled uncertainty estimation and the ability to combine expert knowledge into the model. For reasons that will hopefully become clear later, the challenge in Bayesian inference is to be able to provide both fast and scalable products during both development and serving. In this post, I will summarise what variational inference is trying to solve, it’s benefits and potential pitfalls. I will then introduce the mean-field method, and use it to solve a Bayesian Gaussian Mixture Model. In the &lt;a href=&quot;http://tlublog.com/posts/vi2&quot;&gt;second post&lt;/a&gt;, I will elaborate on black-box variational inference methods.&lt;/p&gt;

&lt;p&gt;VI is an advanced topic, so intermediate-level probability theory and ML knowledge are assumed. Knowledge of the &lt;a href=&quot;https://www.youtube.com/watch?v=ErfnhcEV1O8&quot;&gt;&lt;em&gt;Kullback–Leibler&lt;/em&gt;&lt;/a&gt; (KL) divergence is also expected. To better understand Bayesian GMM (which will be maths heavy), I would like to recommend my previous post on &lt;a href=&quot;https://xl402.github.io/posts/em&quot;&gt;Expectation Maximisation&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/watch?v=REypj2sy_5U&amp;amp;t=329s&quot;&gt;this&lt;/a&gt; video for intuition. Let’s dive in!&lt;/p&gt;

&lt;h1 id=&quot;2-theory-of-vi&quot;&gt;2. Theory of VI&lt;/h1&gt;
&lt;h2 id=&quot;21-what-is-it-trying-to-solve&quot;&gt;2.1 What Is It Trying to Solve?&lt;/h2&gt;
&lt;p&gt;Let’s start with the classic bayesian inference equation:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(\theta|X) = \dfrac{p(X|\theta)p(\theta)}{p(X)}$$&lt;/code&gt;
in words:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\text{posterior} = \dfrac{\text{likelihood}\times\text{prior}}{\text{evidence}}$$&lt;/code&gt;
following this equation, there are three steps involving developing a Bayesian inference model.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Build a &lt;strong&gt;generative model&lt;/strong&gt;: choose prior and choose likelihood, i.e. how can our data be generated using a probabilistic model? (GMM for discovering mixtures, LDA for discovering topics). This step results in an easily evaluated joint distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(X, \theta)$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Compute posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta|X)$&lt;/code&gt; (from what distribution do our parameters actually come from after observing the data? As suppose to our belief, i.e. prior distribution)&lt;/li&gt;
  &lt;li&gt;Report a summary, e.g. posterior means and (co)variances. Inform your boss/client about the ‘insights’ provided by the model, i.e. “on average 30% of our data come from distribution A, with an uncertainty of ±5% etc.”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of the three steps alone requires a separate blog post(s). &lt;strong&gt;We only focus on step 2&lt;/strong&gt;, i.e. how do we compute the posterior? This is in fact a very hard problem because of the normalising &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(X)$&lt;/code&gt; (evidence) term. The only way to evaluate it exactly is to compute:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(X) = \int_\theta p(X|\theta)p(\theta)$$&lt;/code&gt;
suppose in our model, our parameter space includes things like “cluster mean” &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mu_1, \cdots \mu_M$&lt;/code&gt;, “cluster variance” &lt;code class=&quot;highlighter-rouge&quot;&gt;$\sigma^2_1, \cdots \sigma^2_M$&lt;/code&gt;, etc, this integral over &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; becomes:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(X) = \int_{\mu_1}\cdots \int_{\mu_M}\int_{\sigma^2_1}\cdots \int_{\sigma^2_M} p(X|\mu_1, \cdots \sigma^2_M)d\mu_1\cdots d\sigma^2_M$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;not really looking pretty… since we are integrating over some pretty complicated, high dimensional function over a high dimensional space. As with a lot of real life problems, this integration typically has no closed form, and it will be extremely inefficient to use numerical integration tools due to the high dimensionality.
The gold standard approach to obtain the posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta|X)$&lt;/code&gt; is to use Markov Chain Monte Carlo (MCMC), which deserves a separate blog post. But in short MCMC provides an unbiased estimate of the posterior, i.e. if you run it for long enough, you are guaranteed to become arbitrarily close to the posterior. But we run into the same problem of “running it for long enough”, turns out, for large dataset and parameters, “long enough” is “too long to be acceptable for development/production”. What would be a &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;reliable&lt;/strong&gt; (good enough approximate) of the posterior? Here comes variational inference!&lt;/p&gt;

&lt;h2 id=&quot;22-vi-overview&quot;&gt;2.2 VI Overview&lt;/h2&gt;
&lt;p&gt;Essentially, variational inference turns the &lt;em&gt;numerical integral&lt;/em&gt; problem into an &lt;em&gt;optimisation&lt;/em&gt; problem. Remember what we want to obtain is the posterior distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta|X)$&lt;/code&gt;, which can be pretty complicated looking (we in fact don’t normally know what it looks like). In VI, we try to approximate this distribution with a &lt;em&gt;nice&lt;/em&gt; distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(\theta)$&lt;/code&gt; (more on &lt;em&gt;“nice”&lt;/em&gt; later, for now, picture Gaussian). Graphically speaking, left figure below shows what we might want to achieve:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/umR5NCD.png&quot; alt=&quot;drawing&quot; width=&quot;800&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;To think about this in terms of optimisation: the posterior distribution lives in some complicated space, we wish to find the optimum distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^*(\theta)$&lt;/code&gt; which is &lt;em&gt;closest&lt;/em&gt; to the posterior, and come from a family of &lt;em&gt;“nice”&lt;/em&gt; distribution (pictorially shown on the right). i.e.
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^* = \arg\min_{q\in Q}c(q(\theta), p(\theta|X))$$&lt;/code&gt;
with &lt;code class=&quot;highlighter-rouge&quot;&gt;$c$&lt;/code&gt; being the “closeness” function. Two things we need to further define - what is &lt;em&gt;“nice”&lt;/em&gt; and what is &lt;em&gt;“close”&lt;/em&gt;?
Let’s talk about &lt;em&gt;“nice”&lt;/em&gt; first. Recall what we really want at the end of the day is to be able to report &lt;strong&gt;mean&lt;/strong&gt; and &lt;strong&gt;variance&lt;/strong&gt; of the (approximated) posterior distribution. Hence the family of distributions &lt;code class=&quot;highlighter-rouge&quot;&gt;$Q$&lt;/code&gt; are typically:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;low dimensional (which motivates the idea of mean-field as we will see later)&lt;/li&gt;
  &lt;li&gt;often belong to the &lt;a href=&quot;https://stats.stackexchange.com/questions/411893/advantages-of-the-exponential-family-why-should-we-study-it-and-use-it&quot;&gt;exponential family&lt;/a&gt; (Gaussian, exponential, Bernoulli, categorical, beta, Dirichlet + pretty much every distribution that came up during our undergraduate course except for binomial and multinomial)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next on the topic of &lt;em&gt;“close”&lt;/em&gt;, note I did not use the word &lt;em&gt;distance&lt;/em&gt;, since &lt;em&gt;distance&lt;/em&gt; between two distributions entails both symmetry and positive definiteness. The default “closeness” function &lt;code class=&quot;highlighter-rouge&quot;&gt;$c()$&lt;/code&gt; that is used in VI is in fact the Kullback-Leibler (KL) divergence:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$KL(q(\theta)|| p(\theta | X))$$&lt;/code&gt;
which is definitely &lt;strong&gt;not symmetric&lt;/strong&gt;. While it seems to be a reasonable function to choose for measuring closeness between two distributions, we can certainly question:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;why KL? Why not other metrics?&lt;/li&gt;
  &lt;li&gt;why did we choose the order of divergence to be &lt;code class=&quot;highlighter-rouge&quot;&gt;$q||p$&lt;/code&gt; but not &lt;code class=&quot;highlighter-rouge&quot;&gt;$p||q$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;the short answer for the first question is that - because it makes maths simpler and it is &lt;strong&gt;shown&lt;/strong&gt; to provide good approximations. The complete maths for VI is as follows, our objective is:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^{*}=\operatorname{argmin}_{q \in Q} \operatorname{KL}(q(\theta) \| p(\theta | X))$$&lt;/code&gt;
we know under KL measurement, if &lt;code class=&quot;highlighter-rouge&quot;&gt;$Q$&lt;/code&gt; covers all the distributions there are, we just need to set &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(\theta)=p(\theta | X)$&lt;/code&gt; to achieve &lt;code class=&quot;highlighter-rouge&quot;&gt;$\operatorname{KL}=0$&lt;/code&gt;. But &lt;strong&gt;we do not know what the posterior is&lt;/strong&gt;, nor do we want to calculate it via that intractable integral. Instead, we write KL in it’s full form:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\operatorname{KL}(q(\theta) \| p(\theta | X)) = \int_\theta q(\theta) \log \frac{q(\theta)}{p(\theta | X)} d \theta \quad [1]$$&lt;/code&gt;
and re-write the posterior:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\operatorname{KL}(\cdots)=\int_\theta q(\theta) \log \frac{q(\theta) p(X)}{p(\theta, X)} d \theta=\log p(X)-\int_\theta q(\theta) \log \frac{p(\theta, X)}{q(\theta)} d \theta \quad [2]$$&lt;/code&gt;
and recall &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(X)$&lt;/code&gt; term is intractable, but it does not depend on &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;, hence to find the optimum &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; we can completely avoid this intractable term! &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta, X)$&lt;/code&gt; can be evaluated easily since we needed it to construct our model in the first place (step one of the three procedures in the previous section).  Our objective function becomes:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^* = \operatorname{argmax}_{q \in Q}\int_\theta q(\theta) \log \frac{p(\theta, X)}{q(\theta)} d \theta$$&lt;/code&gt;
(argmax because we are minimising the negative). This objective function is given the confusing name “Evidence lower bound” (ELBO), because if we look at equation &lt;code class=&quot;highlighter-rouge&quot;&gt;$[2]$&lt;/code&gt;, KL divergence is none-negative, the evidence term &lt;code class=&quot;highlighter-rouge&quot;&gt;$\log p(X)$&lt;/code&gt; is a constant, hence the remaining term is a lower bound on the evidence term.
I find this ELBO objective more intuitive by expanding it fully:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^{*}=\operatorname{argmax}_{q \in Q}\text{ELBO} = \int_\theta q(\theta) \log p(\theta, X)d\theta - \int_\theta q(\theta)\log q(\theta)$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$ = \operatorname{argmax}_{q \in Q} \mathbb{E}_{\theta \sim q}[\log p(\theta, X)] + \mathcal{H}(q(\theta))$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{H}(q(\theta))$&lt;/code&gt; is the entropy of our posterior approximation (a.k.a variational distribution). We see maximising the ELBO objective has two contradictive and competing sub-objectives:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathbb{E}_{\theta \sim q}[\log p(\theta, X)]$&lt;/code&gt; can be seen as maxising the likelihood of our dataset being generated by our parameters (drawn from &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;) and our model (fixed). &lt;strong&gt;Prefers point estimates of &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{H}(q(\theta))$&lt;/code&gt; entropy term, which &lt;strong&gt;prefers uncertain estimates of &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is these two competing objectives that makes Bayesian inference so attractive, and less prone to overfitting! i.e. we want a good fit to our dataset but also uncertainty in our parameters.&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;To summarise, variational inference aims to approximate the intractable posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta|X)$&lt;/code&gt; with some “nice” distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^*(\theta)$&lt;/code&gt;. Approximation is turned into an optimisation problem, where the objective function is ELBO: &lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^{*}=\operatorname{argmax}_{q \in Q} \mathrm{ELBO} = \operatorname{argmax}_{q \in Q} \int_{\theta} q(\theta) \log \frac{p(\theta, X)}{q(\theta)} d \theta$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now we circle back to an earlier problem we raised - why did we take &lt;code class=&quot;highlighter-rouge&quot;&gt;$\text{KL}(q||p)$&lt;/code&gt;? Why not &lt;code class=&quot;highlighter-rouge&quot;&gt;$\text{KL}(p||q)$&lt;/code&gt;. In short, inspecting equation &lt;code class=&quot;highlighter-rouge&quot;&gt;$[1]$&lt;/code&gt;, had we taken the divergence the other way round, we would end up taking expectation over our intractable posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta|X)$&lt;/code&gt;. This sort of maximisation is actually still possible, and is termed &lt;em&gt;Belief-Propagation&lt;/em&gt; (which will not be discussed here). What is the effect of the order? Observe that:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\mathrm{KL}(q(x) \| p(x)) = \int_x q(x)\dfrac{q(x)}{p(x)} dx $$&lt;/code&gt;
in the cases where &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(x)$&lt;/code&gt; is very close or equal to zero (i.e. low-likelihood regions), the only way to minimise the divergence is to &lt;strong&gt;ensure &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(x)$&lt;/code&gt; is close to zero at these locations&lt;/strong&gt;. Hence we say that &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathrm{KL}(q\| p)$&lt;/code&gt; is &lt;strong&gt;zero-seeking&lt;/strong&gt;. Similar argument may be made for &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathrm{KL}(p\| q)$&lt;/code&gt; which is called &lt;strong&gt;zero-avoiding&lt;/strong&gt;. This is more intuitive through graphical visualisation. Suppose we want to approximate a bimodal distribution with a Gaussian, graphs below shows the behaviour of the two minimisation methods:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/GTYI3gm.png&quot; alt=&quot;drawing&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;where the blue line is the true posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\theta|X)$&lt;/code&gt;, red lines are our variational approximations. So methods with &lt;code class=&quot;highlighter-rouge&quot;&gt;$q||p$&lt;/code&gt; tends to &lt;strong&gt;under estimate&lt;/strong&gt; the posterior variance, c.f. over estimation in &lt;code class=&quot;highlighter-rouge&quot;&gt;$p||q$&lt;/code&gt;. Both can be quiet bad if you think about the real-life impact (under-estimating the uncertainty in getting cancer vs. over-estimating it). A lot of research is done on finding alternative methods/metrics, which of course is beyond the scope of this post.&lt;/p&gt;

&lt;h2 id=&quot;23-mean-field-approximation&quot;&gt;2.3 Mean-Field Approximation&lt;/h2&gt;

&lt;p&gt;Recall we wish to approximate the posterior using a family of &lt;em&gt;“nice”&lt;/em&gt; distributions &lt;code class=&quot;highlighter-rouge&quot;&gt;$Q$&lt;/code&gt;, one of the criteria is low dimensionality. &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; contains all parameters in our model, hence it is typically high dimensional. What if we can instead fully factorise it:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$Q_{\text{MF}}:=\left\{q: q(\theta)=\prod_{j=1}^{J} q_{j}\left(\theta_{j}\right)\right\}$$&lt;/code&gt;
so now we explicitly assume independence between parameters in our &lt;strong&gt;approximated distribution&lt;/strong&gt;. This gives us the form of mean-field (MF) variational inference. Note this is &lt;strong&gt;NOT&lt;/strong&gt; equivalent to assuming parameters are actually independent from one another.  &lt;strong&gt;Hence MF is not a modelling assumption&lt;/strong&gt;, but an approximation assumption.
This factorized form of variational inference corresponds to an approximation framework developed in physics called &lt;em&gt;mean field theory&lt;/em&gt;. Now we seek the distribution within the family &lt;code class=&quot;highlighter-rouge&quot;&gt;$Q_\text{MF}$&lt;/code&gt; for which ELBO is the largest, substitution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(\theta)$&lt;/code&gt; into ELBO:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$ \text{ELBO} = \int_\theta \prod_j q_j(\theta_j) \left\{ \log p(X,\theta) - \sum_j \log q_j (\theta) \right\} d\theta$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It can be shown (maths is quiet involved, so ignored here), that the general expression for the optimal solution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^*_k (\theta_k)$&lt;/code&gt; satisfies:&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\log q_k^*(\theta_k) = \mathbb{E}_{\theta \sim q_{j\neq k}}[ \log p(X, \theta)] + \text{const.}\quad [3]$$&lt;/code&gt;
which says that the log of the optimal solution for the factor &lt;code class=&quot;highlighter-rouge&quot;&gt;$q_k$&lt;/code&gt; is obtained by considering the log of the joint distribution and then taking the expectation with respect to all the other factors &lt;code class=&quot;highlighter-rouge&quot;&gt;$\{q_j\} \;\forall j\neq k$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This means we can apply &lt;strong&gt;coordinate ascent&lt;/strong&gt; to obtain &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^*(\theta)$&lt;/code&gt; by iteratively setting each factor &lt;code class=&quot;highlighter-rouge&quot;&gt;$q_{k}\left(\theta_{k}\right)$&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^*_{k}\left(\theta_{k}\right)$&lt;/code&gt; according to equation [3]. The bad news is that - in order to do MF, you have to analytically derive each update steps (just like expectation maximisation for Gaussian Mixture Models), note in equation [3] the constant term is effectively the normalising denominator if you take exponent on both side to get the actual density function &lt;code class=&quot;highlighter-rouge&quot;&gt;$q_{k}^{*}\left(\theta_{k}\right)$&lt;/code&gt;. In practice, people just find &lt;code class=&quot;highlighter-rouge&quot;&gt;$q_{k}^{*}\left(\theta_{k}\right)$&lt;/code&gt; by inspection, after writing down the analytical form of [3].&lt;/p&gt;

&lt;p&gt;Suppose we have gone through the trouble, deriving the update rule for each parameter in our model, and we obtain our final &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^*(\theta)$&lt;/code&gt;. One natural question to ask: how correct are we from the true posterior? &lt;a href=&quot;https://www.inference.org.uk/itprnn/book.pdf&quot;&gt;Mackay’s book&lt;/a&gt; introduces a very simple problem: given a set of one dimensional data points &lt;code class=&quot;highlighter-rouge&quot;&gt;$X = [x_0, \cdots x_N]$&lt;/code&gt;, we would like to model the distribution with a Gaussian and report it’s mean and variance.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I mean … if you are sane, you would probably do maximum likelihood estimation and guess that &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mu_{\text{MLE}} = \bar{X}$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\sigma^2_{\text{MLE}} = \text{Var}[X]$&lt;/code&gt;. But what if we want to go Bayesian? Let’s place a Gamma prior on the precision of the normal distribution, and a normal prior on our mean:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/3Y3vDVF.png&quot; alt=&quot;drawing&quot; width=&quot;800&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Because we have used conjugate priors, we can actually get an analytical posterior via computing:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(\mu, \sigma |X) = \dfrac{p(X|\mu,\sigma)p(\mu, \sigma)}{p(X)}$$&lt;/code&gt;
(the exact format is actually quiet involved). One can show that this definitely does not factorize, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(\mu, \sigma | X)\neq f_1(\mu)f_2(\sigma)$&lt;/code&gt;. For those who are insane, we can actually use mean-field approximation on this posterior (for which we know the analytical form), i.e. we will consider a factorised variational approximation to the posterior given by:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q(\mu, \sigma) = q_\mu (\mu)q_\sigma (\sigma)$$&lt;/code&gt;
one can painstakingly derive the update rules according to equation [3] and do coordinate descent to obtain:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/ulKVOW2.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;in figures above, green contours represent our true posterior, with x-axis representing posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mu$&lt;/code&gt; and y-axis being posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$\sigma^{-1}$&lt;/code&gt;. Blue lines are our variational mean-field approximation at each time-step. Red lines show our converged posterior approximation. As a sanity check this shows that MF provides a decent fit for our toy example (moments seem to match)!
What about models where we do not know the true posterior? (i.e. most real-life models)? Recall in section 2.1 we mentioned that another common method to obtain unbiased approximations of the posterior is through MCMC. If we trust MCMC (where the convergence checking is an art itself), then we can check our VI approximated mean values for parameters against MCMC’s mean. Figures below are from &lt;a href=&quot;https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/24305/Fosdick_washington_0250E_12238.pdf?sequence=1&amp;amp;isAllowed=y&quot;&gt;Fosdick 2013&lt;/a&gt; where we see for a complicated model, VI and MCMC posteriors have good agreement.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/g3Y6zZp.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Note from figures above, VB stands for &lt;em&gt;variational Bayes&lt;/em&gt;, which is used interchangeably with &lt;em&gt;variational inference&lt;/em&gt;. &lt;strong&gt;Do not be misled by figures above, there are plenty cases where VI is unable to provide good posterior estimations!&lt;/strong&gt; Therefore, one possible way to check VI accuracy is to use the one-time more computationally expensive MCMC (for the experiment above, MCMC took over one day), once you trust your VI (which in this case, takes less than 20 minutes), you can ditch MCMC and work with VI from there.&lt;/p&gt;

&lt;h1 id=&quot;3-vi-in-practice-bayesian-gmm&quot;&gt;3. VI in Practice: Bayesian GMM&lt;/h1&gt;

&lt;h2 id=&quot;31-bayesian-gmm-vs-gmm-theory-and-derivation&quot;&gt;3.1 Bayesian GMM VS GMM: Theory and “Derivation”&lt;/h2&gt;
&lt;p&gt;Mixture of Gaussians is a form of &lt;em&gt;latent variable models&lt;/em&gt; where we wish to infer a set of &lt;em&gt;hidden variables&lt;/em&gt; from the observed data. In our post &lt;a href=&quot;https://xl402.github.io/posts/em&quot;&gt;Expectation Maximisation Deep Dive&lt;/a&gt;, we investigated in depth the update rules associated with maximum likelihood estimation of parameters for a Gaussian Mixture Model (GMM). As a recap, we have &lt;code class=&quot;highlighter-rouge&quot;&gt;$N$&lt;/code&gt; observed data points whose positions &lt;code class=&quot;highlighter-rouge&quot;&gt;$x_n$&lt;/code&gt; are influenced by the cluster they come from. We call &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_n$&lt;/code&gt;, a K-dimensional binary random variable that “one-hot encodes” which cluster the data point comes from (total number of clusters is &lt;code class=&quot;highlighter-rouge&quot;&gt;$K$&lt;/code&gt;). Hence:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(x_n|z_{nk}=1) =  \mathcal{N}(x_n|\mu_k ,\Sigma_k)$$&lt;/code&gt;
We also define the concept of mixing proportion, which is the marginal distribution over &lt;code class=&quot;highlighter-rouge&quot;&gt;$z$&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(z_{nk}=1)=\pi_k, \quad p(z_n) = \prod_{k=1}^K \pi_k^{z_{nk}}$$&lt;/code&gt;
Hence we can write down the joint distribution of &lt;code class=&quot;highlighter-rouge&quot;&gt;$x_n, z_n$&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(x_n, z_n) = p(x_n|z_n)p(z_n) = \prod_{k=1}^K \pi_k^{z_{nk}} \mathcal{N}\left(x_{n} | \mu_{k}, \Sigma_{k}\right)^{z_{nk}} $$&lt;/code&gt;
This model be compactly represented using the graphical representation shown in the left figure below.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/gQD80i2.png&quot; alt=&quot;drawing&quot; width=&quot;650&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Note in basic GMM, we did not place a prior over any model parameters (&lt;code class=&quot;highlighter-rouge&quot;&gt;$\pi,\; \mu, \; \Sigma\;$&lt;/code&gt;), we instead use maximum likelihood (ML) to find ML estimates (MLE) of these parameters. We will show in the next section, MLE can easily lead to overfitting. To make a GMM Bayesian, all we do is to place priors over model parameters, i.e. let the mixing proportion &lt;code class=&quot;highlighter-rouge&quot;&gt;$\pi$&lt;/code&gt; (which is multinomial) be drawn from a symmetric Dirchlet distribution:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(\pi) = \text{Dir}(\pi|\alpha_0)$$&lt;/code&gt;
the mean and precision of each Gaussian mixture are drawn from an independent Gaussian-Wishart prior (Wishart is the conjugate prior to precision matrices):
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(\mu, \Lambda)=p(\mu|\Lambda)p(\Lambda)=\prod_{k=1}^K \mathcal{N}(\mu_k|m_0, (\beta_0 \Lambda_k)^{-1})\mathcal{W} (\Lambda_k|W_0, \nu_0)$$&lt;/code&gt;
where parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\alpha_0, \beta_0, m_0, \nu_0, W_0$&lt;/code&gt; govern the shape of prior distributions. Conditional distributions for &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_n$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$x_n$&lt;/code&gt; are almost identical to one described by the GMM:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(z_n|\pi) = \prod_{k=1}^{K} \pi_{k}^{z_{n k}}$$&lt;/code&gt;
and
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$ p(x_n|z_n,\mu, \Lambda) = \prod_{k=1}^{K} \mathcal{N}\left(x_{n} | \mu_{k} ,\Lambda_{k}^{-1}\right)^{z_{n k}}$$&lt;/code&gt;
Right figure above shows the complete graphical representation of this Bayesian GMM model.&lt;/p&gt;

&lt;p&gt;In order to formulate a VI treatment of this model, we explicitly write down the joint distribution over all random variables:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(X, Z, \pi, \mu, \Lambda) = p(X|Z, \mu, \Lambda)p(Z|\pi)p(\pi)p(\mu|\Lambda)p(\Lambda) \quad [4]$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$X = \{x_0, \cdots x_N\}$&lt;/code&gt; is our observed dataset and similarly &lt;code class=&quot;highlighter-rouge&quot;&gt;$Z = \{z_0, \cdots, z_N\}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We wish to obtain the posterior distribution:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$p(Z, \pi, \mu, \Lambda|X)$$&lt;/code&gt; which is intractable. Instead, consider a variational distribution which factorises between the latent variables and the parameters so that:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q(Z, \pi, \mu, \Lambda) = q(Z) q(\pi, \mu ,\Lambda)$$&lt;/code&gt;
this follows the mean-field approach to VI, turns out, this is the only assumption we need to make to obtain a tractable solution to our Bayesian mixture model! Following the mean-field framework (equation [3]), to update the factor &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(Z)$&lt;/code&gt;, set
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\log q^{\star}(Z)=\mathbb{E}_{\pi, \mu, \Lambda}[\log p({X}, {Z}, \pi, \mu, \Lambda)]+\mathrm{const.}$$&lt;/code&gt;
the exact form of &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^*(Z)$&lt;/code&gt; requires quiet a lot of maths, but essentially we just substitute the joint density term (equation [4]) in and take expectations w.r.t. the parameters. Jumping to the solution:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^{\star}(\mathbf{Z})=\prod_{n=1}^{N} \prod_{k=1}^{K} r_{n k}^{z_{n k}}$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$r_{n k} \propto \pi_{k}\left|\boldsymbol{\Lambda}_{k}\right|^{1 / 2} \exp \left\{-\frac{1}{2}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)\right\}$&lt;/code&gt; is quiet intuitive, i.e. it is the “responsibility” term (almost identical to the GMM model), that indicates the contribution from each cluster towards our observed data points.
Next, we obtain the variational posterior for &lt;code class=&quot;highlighter-rouge&quot;&gt;$\pi, \mu, \Lambda$&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\ln q^{\star}({\pi}, {\mu}, {\Lambda}) = \mathbb{E}_Z[\log p(X, Z, \pi, \mu, \Lambda)]+\text { const. }$$&lt;/code&gt;
it can be shown that:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^{\star}({\pi})=\operatorname{Dir}({\pi} | {\alpha}), \quad \alpha_k = \alpha_0+N_k \quad [5]$$&lt;/code&gt;
and
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$q^{\star}\left({\mu}_{k}, {\Lambda}_{k}\right)=\mathcal{N}\left({\mu}_{k} | {m}_{k},\left(\beta_{k} {\Lambda}_{k}\right)^{-1}\right) \mathcal{W}\left({\Lambda}_{k} | {W}_{k}, \nu_{k}\right) \quad [6]$$&lt;/code&gt;
where&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$ N_{k}=\sum_{n=1}^{N} r_{n k}, \quad\quad \bar{x}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k} {x}_{n}, \quad\quad {S}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k}\left({x}_{n}-\bar{x}_{k}\right)\left({x}_{n}-\overline{x}_{k}\right)^{\mathrm{T}}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;define the “pseudo count”, “cluster mean” and “cluster variance respectively”;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\beta_{k}=\beta_{0}+N_{k}$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$${m}_{k}=\frac{1}{\beta_{k}}\left(\beta_{0} {m}_{0}+N_{k} \overline{x}_{k}\right)$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$${W}_{k}^{-1}={W}_{0}^{-1}+N_{k} {S}_{k}+\frac{\beta_{0} N_{k}}{\beta_{0}+N_{k}}\left(\overline{x}_{k}-{m}_{0}\right)\left(\overline{x}_{k}-{m}_{0}\right)^{\mathrm{T}}$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\nu_{k}=\nu_{0}+N_{k}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;are used to parameterise the VI posterior Gaussian clusters &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^{\star}\left(\mu_{k}, \Lambda_{k}\right)$&lt;/code&gt;. We should not be too daunted by the form of our VI posterior. We see the form of both [5] and [6] makes sense, since our priors were Dirchlet and Gaussian-Wishart, and our VI posterior ended up being the same kind of distributions. We also see that the VI posteriors are parameterised by both prior parameters like &lt;code class=&quot;highlighter-rouge&quot;&gt;$\alpha_0, \beta_0, W_0$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\nu_0$&lt;/code&gt; and taking into account of observed data.&lt;/p&gt;

&lt;p&gt;It is worth noting the computational cost between GMM and Bayesian GMM, by inspecting the update equations above, there is nothing fundamentally different in Bayesian GMM (computationally speaking), the most expensive step are matrix inverts, which also manifests in GMM.&lt;/p&gt;

&lt;h2 id=&quot;32-implementation-and-results&quot;&gt;3.2 Implementation and Results&lt;/h2&gt;

&lt;p&gt;For my sanity, unlike the GMM algorithm, I will not be implementing Bayesian GMM from scratch. Instead, &lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt; has readily available &lt;code class=&quot;highlighter-rouge&quot;&gt;BayesianGaussianMixture&lt;/code&gt; module that does all the hard work for us. Starting with the imports:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets.samples_generator&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.mixture&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GaussianMixture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BayesianGaussianMixture&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;where we use &lt;code class=&quot;highlighter-rouge&quot;&gt;make_blobs&lt;/code&gt; function to generate our dataset:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;cluster_std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# reshape to simplify plotting indexing&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Both GMM and Bayesian GMM are implemnted and presented for comparision:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;gmm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GaussianMixture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gmm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gmm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;bgmm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BayesianGaussianMixture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;weight_concentration_prior_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dirichlet_distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;init_params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'random'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bgmm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;blabels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bgmm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s it! Forget about the maths above. Figures below show the fitted model and posterior for cluster size equal to 3 (sorry about the notation, &lt;code class=&quot;highlighter-rouge&quot;&gt;$M$&lt;/code&gt; in the plots means the same as &lt;code class=&quot;highlighter-rouge&quot;&gt;$K$&lt;/code&gt;, i.e. number of clusters). We see both GMM and Bayesian GMM provide very good estimates.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/26hk5pY.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The power of Bayesian GMM is observed when &lt;strong&gt;you do not know how many clusters there are&lt;/strong&gt;, which is a very common thing to high dimensional, unknown dataset. Let’s set number of clusters to be equal to 20. Figures below show that GMM completely overfits the dataset, whereas Bayesian GMM is not fooled!&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/tN24Yck.png&quot; alt=&quot;drawing&quot; width=&quot;800&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Indeed, if we plot out the posterior mixing proportions, we see GMM places almost uniform proportion across all clusters, whereas Bayesian GMM has managed to identify that there are only three clusters!&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/YS03aiH.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Of course, one can argue that through cross-validation, we can tune the optimum number of clusters for GMM. But a) Bayesian GMM has the same computational cost as GMM and b) We can obtain uncertainty in posterior parameters which is an added bonus! So in short, Bayesian GMM beats GMM :)&lt;/p&gt;

&lt;p&gt;An obvious downside of the mean-field approach is the maths one needs to go through (we have in fact omitted 90% of the actual derivation), another issue lies in scalability. Part 2 of the post will cover the concept of black-box inference.&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Jun 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/posts/vi1</link>
        <guid isPermaLink="true">http://localhost:4000/posts/vi1</guid>
        
        <category>theory</category>
        
        <category>bayesian</category>
        
        
      </item>
    
      <item>
        <title>Expectation Maximisation Deep Dive</title>
        <description>&lt;p class=&quot;lead&quot;&gt;(45min) This post covers the in-depth theory regarding the EM algorithm, with python implementation for a Gaussian Mixture Model from scratch.&lt;/p&gt;
&lt;p&gt;&amp;lt;!–-break-–&amp;gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#theory-of-em&quot;&gt;Theory of EM&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;2.1 &lt;a href=&quot;#what-is-it-trying-to-solve?&quot;&gt;What is it trying to solve?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;2.2 &lt;a href=&quot;#2.2&quot;&gt;Graphical representation of the solution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;2.3 &lt;a href=&quot;#2.3&quot;&gt;Detailed derivation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;EM in Practice: Gaussian Mixture Model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;3.1 &lt;a href=&quot;#3.1&quot;&gt;Derivation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;3.2 &lt;a href=&quot;#3.2&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;1-introduction-&quot;&gt;1. Introduction &lt;a name=&quot;introduction&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Expectation maximisation (EM) is one of those things that I was taught several times at University, each time it was explained slightly differently, and I was confused by it every single time. The confusion is mostly because we never implemented any of the EM algorithms through Python, and EMs were always introduced to solve a &lt;strong&gt;specific&lt;/strong&gt; problem.&lt;/p&gt;

&lt;p&gt;Without a doubt, EM serves as a foundation for understanding variational inference. Here, I will try to go in-depth and derive and &lt;em&gt;E&lt;/em&gt; and &lt;em&gt;M&lt;/em&gt; steps under a &lt;strong&gt;general&lt;/strong&gt; framework. Later I will derive and implement a &lt;em&gt;Gaussian Mixture Model&lt;/em&gt; using EM.&lt;/p&gt;

&lt;p&gt;Maths behind this post are based on Coursera’s &lt;a href=&quot;https://www.coursera.org/learn/bayesian-methods-in-machine-learning&quot;&gt;&lt;em&gt;Bayesian Methods for Machine Learning&lt;/em&gt;&lt;/a&gt; specialisation course. Intermediate-level probability theory and ML knowledge are assumed. Knowledge of the &lt;a href=&quot;https://www.youtube.com/watch?v=ErfnhcEV1O8&quot;&gt;&lt;em&gt;Kullback–Leibler&lt;/em&gt;&lt;/a&gt; (KL) divergence and &lt;a href=&quot;https://www.youtube.com/watch?v=HfCb1K4Nr8M&quot;&gt;&lt;em&gt;Jensen’s inequality&lt;/em&gt;&lt;/a&gt;  are also expected (link to videos explaining the concepts).&lt;/p&gt;

&lt;h1 id=&quot;2-theory-of-em-&quot;&gt;2. Theory of EM &lt;a name=&quot;theory-of-em&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id=&quot;21-what-is-it-trying-to-solve-&quot;&gt;2.1 What is it trying to solve? &lt;a name=&quot;what-is-it-trying-to-solve?&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;EM aims to tackle a family of models termed &lt;strong&gt;latent variable models&lt;/strong&gt; (examples like GMM, LDA, etc). Their graphical representation can be compactly represented as:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/U6RLrrJ.png&quot; alt=&quot;drawing&quot; width=&quot;350&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;$x_i$&lt;/code&gt; is our data-points in the dataset &lt;code class=&quot;highlighter-rouge&quot;&gt;$X = \{x_0, \cdots x_N\}$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_i$&lt;/code&gt; is the latent variables associated with each datapoint. The edge represents that &lt;code class=&quot;highlighter-rouge&quot;&gt;$x_i$&lt;/code&gt; is conditioned by &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_i$&lt;/code&gt;. Albeit this graphical model looks simple, to fully specify the model, we need to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Specify prior distribution of &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_i$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Specify the conditional distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(x_i|z_i)$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;both these two distributions can be arbitrarily complex (which is a problem during both model fitting and inference). If we &lt;strong&gt;summarise&lt;/strong&gt; all the model parameters associated with our problem to be &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;, our goal becomes finding a set of parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; which &lt;strong&gt;maximises the likelihood&lt;/strong&gt; of our dataset &lt;code class=&quot;highlighter-rouge&quot;&gt;$X$&lt;/code&gt; given our model:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\max_{\theta}p(X|\theta)
$$&lt;/code&gt;
We assume our datapoints are &lt;em&gt;i.i.d&lt;/em&gt; under our model, therefore:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\max_{\theta}p(X|\theta) = \max_\theta \prod p(x_i|\theta)
$$&lt;/code&gt;
simplifying the product terms with a sum by maximising the log-likelihood:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\max_{\theta}\log p(X|\theta) = \max_\theta \sum_i \log p(x_i|\theta)
$$&lt;/code&gt;
now we explicitly add in the conditional distribution using the rule of marginalisation:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\max_{\theta}\sum_i \log\left(\sum_c p(x_i, z_i=c|\theta)\right)
$$&lt;/code&gt;
where for simplicity, we assume &lt;strong&gt;discrete distribution&lt;/strong&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_i$&lt;/code&gt; (derivation holds if you replace the sum with an integral). The support for &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_i$&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;$[0, 1, \cdots, c]$&lt;/code&gt;, i.e. it is categorical.&lt;/p&gt;

&lt;p&gt;That’s it! EM tries to find &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; that maximises the likelihood of our data, and through some maths we have shown:&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\max _{\theta} p(X | \theta) = \max _{\theta} \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Of course, one question is why can’t we directly differentiate the (log) likelihood? As we will see later with GMM, we cannot obtain an analytical equation for &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; via direct differentiation!&lt;/p&gt;

&lt;h2 id=&quot;22-graphical-representation-of-the-solution-&quot;&gt;2.2 Graphical representation of the solution &lt;a name=&quot;2.2&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Here we jump straight into the solution of EM and obtain a graphical representation of the iterative update rule.&lt;/p&gt;

&lt;p&gt;We wish to find a set of parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; which maximises the likelihood &lt;code class=&quot;highlighter-rouge&quot;&gt;$\log(p(X|\theta))$&lt;/code&gt;. This distribution can be arbitrarily complex, therefore at each timestep, we approximate this likelihood using a simpler distribution that forms a &lt;em&gt;lower bound&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{L}(q, \theta)$&lt;/code&gt;, which is &lt;strong&gt;below the likelihood&lt;/strong&gt; at all &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; values. At timestep &lt;code class=&quot;highlighter-rouge&quot;&gt;$k$&lt;/code&gt;, the EM update rule are as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;-step: fix &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;, find some arbitrary &lt;em&gt;variational distribution&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z_i)$&lt;/code&gt; which maximises the lower-bound, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^{k+1} = \arg\max_{q}\mathcal{L}(\theta^k, q)$&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;M&lt;/strong&gt;-step: fix &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;, find model parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; that maximises our current lower-bound, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta^{k+1} = \arg\max_\theta \mathcal{L}(\theta, q^{k+1})$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing to get our head around is that in &lt;strong&gt;E&lt;/strong&gt;-step, we are essentially taking derivative w.r.t. a &lt;strong&gt;function&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z_i)$&lt;/code&gt;. These two steps can be seen more clearly in the picture below.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/8egXBCs.gif&quot; alt=&quot;drawing&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Derivation later will show exactly how we have arrived at &lt;strong&gt;E&lt;/strong&gt; and &lt;strong&gt;M&lt;/strong&gt; steps, but jumping ahead, these two steps are equivalent to:&lt;/p&gt;

&lt;div class=&quot;notice&quot;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;-step: set &lt;em&gt;variational distribution&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z_i)$&lt;/code&gt; to be equal (or approximately equal) to the posterior distribution, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;$q^k(z_i) = p(z_i|\theta_i^k, x_i)$&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;$k$&lt;/code&gt; denotes the timestep in our iteration.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;M&lt;/strong&gt;-step: under our distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z_i)$&lt;/code&gt;, find a set of parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; which maximises the data likelihood, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta^{k} = \arg\max_\theta \sum_i \mathbb{E}_{z_i\sim q^k}\left[\log p(x_i, z_i | \theta)\right]$&lt;/code&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Hence EM algorithm boils down to the fact that we cannot directly maximise our likelihood function, hence we maximise its lower-bound under our own &lt;strong&gt;variational distribution&lt;/strong&gt; which is a simple function. At each timestep, we approximate or set this function to the posterior distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(Z|X,\theta)$&lt;/code&gt; (&lt;strong&gt;E&lt;/strong&gt;-step), and we maximises the likelihood of our data-points under this function instead (&lt;strong&gt;M&lt;/strong&gt;-step). Alternating these two procedures, we are guaranteed to reach a &lt;strong&gt;local optima&lt;/strong&gt; (in the picture above, depending on initialisation, we could have easily ended up on the left peak as our final solution).&lt;/p&gt;

&lt;h2 id=&quot;23-detailed-derivation-&quot;&gt;2.3 Detailed Derivation &lt;a name=&quot;2.3&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;e-step&quot;&gt;E-Step&lt;/h3&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;$\;\arg\max _{q\left(z_{i}\right)} \mathcal{L}\left(\theta, q\right)=p\left(z_{i} | x_{i}, \theta\right)$&lt;/code&gt;, to minimise the gap between our lower-bound and the likelihood under current parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;, set our variational distribution to be the posterior.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: first, lets define how exactly a lower-bound arises and where on earth this &lt;strong&gt;variational distribution&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; comes from. Decompose the likelihood:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\log p(X | \theta) = \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)$$&lt;/code&gt;
Idea here is we cannot directly maximise this equation (if you can, then do not use EM ;)), instead, we multiply top and bottom by a distribution of our choice &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; (coz why not?):
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\log p(X | \theta) = \sum_{i} \log\left( \sum_{c} q\left(z_{i}=c\right) \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)$$&lt;/code&gt;
which is equivalent to:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\log p(X|\theta) = \sum_{i} \log \mathbb{E}_{z_{i} \sim q} \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}$$&lt;/code&gt;
now we can utilise Jensen’s inequality to obtain a lower bound for this equality. As a reminder, for any concave function (such as &lt;code class=&quot;highlighter-rouge&quot;&gt;$\log(x)$&lt;/code&gt; in our case), &lt;code class=&quot;highlighter-rouge&quot;&gt;$f (\mathbb{E}[x]) \geqslant \mathbb{E}[f (x)]$&lt;/code&gt;. We can therefore write the equation above as:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\log p(X|\theta)\geq \sum_{i} \mathbb{E}_{z_{i}\sim q} \log \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}
$$&lt;/code&gt;
expanding the expectation out:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\log p(X|\theta)\geq\sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} = \mathcal{L}(\theta, q)
$$&lt;/code&gt;
Our objective has the become &lt;strong&gt;minimising the gap&lt;/strong&gt; between this lower-bound by optimising our arbitrary distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\arg\min_q \text{Objective} = \arg\min_q \left( \log p(X|\theta) - \mathcal{L}(\theta, q)\right)
$$&lt;/code&gt;
more maths:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\text{Objective} = \sum_{i} \log p\left(x_{i} | \theta\right) - \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$&lt;/code&gt;
for the first term, we multiply it with &lt;code class=&quot;highlighter-rouge&quot;&gt;$1 = \sum_c q(z_i=c)$&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
 =\sum_{i} \left(\log p\left(x_{i} | \theta\right)\sum_{c} q\left(z_{i}=c\right)\right) - \sum_{i} \sum_{c} \cdots
$$&lt;/code&gt;
now we can move both summations out:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
=\sum_{i}\sum_{c}\left(\log p\left(x_{i} | \theta\right) q\left(z_{i}=c\right) - q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} \right)
$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
 =\sum_{i} \sum_{c} \left(q\left(z_{i}=c\right) \left(\log p\left(x_{i} | \theta\right) - \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)\right)
$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p\left(x_{i}, z_{i}=c | \theta\right)}\right)\right)\right)
$$&lt;/code&gt;
re-write the joint distribution of the denominator inside the log:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p(x_i|\theta)p(z_i=c|\theta, x_i)}\right)\right)\right)
$$&lt;/code&gt;
cancelling out the &lt;code class=&quot;highlighter-rouge&quot;&gt;$p\left(x_{i} | \theta\right)$&lt;/code&gt; we obtain:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\log \left(\frac{ q\left(z_{i}=c\right)}{ p\left(z_{i}=c | \theta, x_{i}\right)}\right)\right)
$$&lt;/code&gt;
observe that the summation under &lt;code class=&quot;highlighter-rouge&quot;&gt;$c$&lt;/code&gt; term corresponds to the KL divergence between &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$p$&lt;/code&gt;, we therefore obtain:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\text{Objective} = \text{KL}(q(z_i)|| p(z_i|\theta, x_i))
$$&lt;/code&gt;
after all this faff, we see to minimise the gap (objective) between the likelihood and our lower-bound is equivalent to:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\arg \min _{q}(\log p(X | \theta)-\mathcal{L}(\theta, q)) = \arg \min _{q}\mathrm{KL}\left(q\left(z_{i}\right) \| p\left(z_{i} | \theta, x_{i}\right)\right)
$$&lt;/code&gt;
so effectively, &lt;strong&gt;E&lt;/strong&gt;-step becomes an optimisation problem with the objective function being the KL divergence between variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; w.r.t. model posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$p\left(z_{i} | \theta, x_{i}\right)$&lt;/code&gt;. There is &lt;strong&gt;no guarantee&lt;/strong&gt; that the posterior has a closed-form representation. This is where the rich literature of &lt;strong&gt;variational inference&lt;/strong&gt; comes in. Fortunately, with a GMM (Gaussian mixture model), the posterior has a closed form solution! Hence at each iteration, we are able to set &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; to be equal to the posterior.&lt;/p&gt;

&lt;h3 id=&quot;m-step&quot;&gt;M-Step&lt;/h3&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; given a fixed variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;, maximise the data and latent variable’s joint likelihood w.r.t. model parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;$\arg\max_\theta \mathcal{L}(\theta, q) = \arg \max _{\theta} \mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]$&lt;/code&gt; where latent variables &lt;strong&gt;come from our variational ditribution&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Recall from &lt;strong&gt;E&lt;/strong&gt;-step, maximising the lower-bound is essentially maximising the likelihood w.r.t. &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;, the only difference is &lt;strong&gt;we can easily perform this maximisation&lt;/strong&gt; since we know our distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt; (normally it is a Gaussian distribution). Writing the whole expectation term out:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\arg\max_\theta \mathcal{L}(\theta, q) = \arg\max_\theta \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$&lt;/code&gt;
splitting the terms within the log:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right) \left(\log p\left(x_{i}, z_{i}=c | \theta\right) - \log q\left(z_{i}=c\right)\right)
$$&lt;/code&gt;
the last term is independent of &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right)\log p\left(x_{i}, z_{i}=c | \theta\right) + \text{const}.
$$&lt;/code&gt;
condensing the sum over &lt;code class=&quot;highlighter-rouge&quot;&gt;$c$&lt;/code&gt; into expectation, dropping the constant:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\arg \max _{\theta}\sum_{i}\mathbb{E}_{z_i}\left[ \log p(x_i,z_i=c|\theta)\right] = \mathbb{E}_{Z\sim q}\left[ \log p(X,Z|\theta)\right]
$$&lt;/code&gt;
So the &lt;strong&gt;M&lt;/strong&gt;-step is simply maximising the joint distribution between collection of latent variables &lt;code class=&quot;highlighter-rouge&quot;&gt;$Z$&lt;/code&gt; and data points &lt;code class=&quot;highlighter-rouge&quot;&gt;$X$&lt;/code&gt; &lt;strong&gt;under the variational distribution&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\hat{\theta} = \arg \max _{\theta}\mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]
$$&lt;/code&gt;
Again, we definitely know what &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(X, Z | \theta)$&lt;/code&gt; is, since we needed it to set up our graphical model. However, maximisation w.r.t. &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; can be non-trivial. In the case of GMM, with a lot of maths, we can obtain closed form solutions for model parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;3-em-in-practice-gaussian-mixture-model-&quot;&gt;3. EM in Practice: Gaussian Mixture Model &lt;a name=&quot;3&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Here we unleash the EM algorithm on Gaussian Mixture Model (GMM), For a quick intuition on how GMM update rules work, here is a link to &lt;a href=&quot;https://www.youtube.com/watch?v=REypj2sy_5U&amp;amp;t=329s&quot;&gt;&lt;em&gt;StatQuest’s&lt;/em&gt;&lt;/a&gt; video. I have to say, update rule for GMM is fairly intuitive, and videos like this is a tiny bit misleading. As it is not easy to shoehorn the general EM algorithm into this sort of intuition. Instead, we will not skip (too much) on the maths and will derive exactly how GMM update rules arise from EM.&lt;/p&gt;

&lt;h2 id=&quot;31-derivation-&quot;&gt;3.1 Derivation &lt;a name=&quot;3.1&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;model-definition&quot;&gt;Model Definition&lt;/h3&gt;

&lt;p&gt;Remember the graph earlier?&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/U6RLrrJ.png&quot; alt=&quot;drawing&quot; width=&quot;350&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Here we define exactly what the model parameters are for a GMM. First let’s look at what GMM is trying to achieve:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/AGe3avl.png&quot; alt=&quot;drawing&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Left figure above is the data points that you have obtained, through some prior knowledge or by inspecting the plots, &lt;strong&gt;you believe&lt;/strong&gt; that the data points come from &lt;strong&gt;three Gaussian distributions&lt;/strong&gt;, and you wish to fit the Gaussian centres and variances to the data points (shown on the right).&lt;/p&gt;

&lt;p&gt;To apply latent variable modelling, we think about “how do we &lt;strong&gt;generate&lt;/strong&gt; these data points with a probabilistic model?”. Let &lt;code class=&quot;highlighter-rouge&quot;&gt;$z_i$&lt;/code&gt; follow a categorical distribution (three categories) which define the proportion of data points that belong to each cluster. When &lt;strong&gt;conditioned&lt;/strong&gt; on the categories, the data point &lt;code class=&quot;highlighter-rouge&quot;&gt;$x_i$&lt;/code&gt; follows a gaussian distribution. Model parameters are therefore &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta = [\pi_1, \cdots, \pi_c, \mu_1, \cdots, \mu_c, \sigma^2_1, \cdots \sigma^2_c]$&lt;/code&gt;, which are the distribution over categories, mean and standard deviaiton of each cluster centre. We restrict ourselves to fitting &lt;strong&gt;isotropic&lt;/strong&gt; Gaussians, although in practice, you can find the MLE covariance matrix (maths will of course be more disgusting).&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/1Mb2T0E.png&quot; alt=&quot;drawing&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Given this mode, we can write down the &lt;strong&gt;likelihood of our data points&lt;/strong&gt; under the model:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
p(x_i |\theta) = \sum_c p(x_i|z_i=c, \theta)p(z_i=c|\theta) = \sum_c p(x_i|z_i=c, \theta) \pi_c
$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
p(X |\theta) = \prod_i \sum_c p(x_i|z_i=c, \theta) \pi_c= \prod_i \sum_c \mathcal{N}(x_i;\mu_c, \sigma^2_c) \pi_c
$$&lt;/code&gt;
This completes our model definition for a GMM. &lt;strong&gt;Why don’t we directly maximise the likelihood w.r.t. model parameters?&lt;/strong&gt; Let’s see:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\log p(X |\theta) = \sum_i \log \sum_{c} \mathcal{N}\left(x_{i} ; \mu_{c}, \sigma_{c}^{2}\right) \pi_{c}
$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\nabla_\theta \log p(X|\theta) = \sum_i  \nabla_\theta \log\left( \sum_{c} \mathcal{N}\left(x_{i} ; \mu_{c}, \sigma_{c}^{2}\right) \pi_{c}\right)
$$&lt;/code&gt;
And we are stuck… good luck dealing with the log of sum of exponential term above. This is why EM is used to solve GMMs.&lt;/p&gt;

&lt;h3 id=&quot;e-step-1&quot;&gt;E-Step&lt;/h3&gt;

&lt;p&gt;Recall in EM, the &lt;strong&gt;E&lt;/strong&gt;-step is equivalent to setting our variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z_i)$&lt;/code&gt; to be equal or approximately equal to the posterior &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(z_i|x_i,\theta)$&lt;/code&gt;. In the case of GMM we can obtain the analytical posterior. Applying Bayes’ rule:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
q(z_i=c) = p(z_i=c|x_i,\theta) = \dfrac{p(x_i|z_i=c,\theta)p(z_i=c|\theta)}{\sum_k p(x_i|z_i=k,\theta)p(z_i=k|\theta)}
$$&lt;/code&gt;
Don’t be confused by the index &lt;code class=&quot;highlighter-rouge&quot;&gt;$k$&lt;/code&gt; in the denominator, we are just marginalising over all cluster centre categories (in our example, 1 to 3). In &lt;strong&gt;E-step&lt;/strong&gt; we &lt;strong&gt;fix&lt;/strong&gt; all model parameters, the posterior above therefore takes the form:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
q(z_i=c) = \dfrac{\mathcal{N}(x_i|\mu_c,\sigma^2_c)\pi_c}{\sum_k \mathcal{N}(x_i|\mu_k,\sigma^2_k)\pi_k}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;This result is fairly intuitive -
you are effectively asking: what is the chance of this data point belonging to cluster &lt;code class=&quot;highlighter-rouge&quot;&gt;$c$&lt;/code&gt;, given the current cluster centres and deviations.&lt;/p&gt;
&lt;p&gt;For each data point, we do a soft (probabilistic) assignment on which cluster it comes from. Which turns out to be the probability of it &lt;strong&gt;being generated&lt;/strong&gt; by cluster &lt;code class=&quot;highlighter-rouge&quot;&gt;$c$&lt;/code&gt; (numerator), divided by the sum of probabilities of it being generated by all clusters (denominator).&lt;/p&gt;

&lt;h3 id=&quot;m-step-1&quot;&gt;M-Step&lt;/h3&gt;

&lt;p&gt;Recall in &lt;strong&gt;M&lt;/strong&gt;-step, we maximise the joint probability distribution under our own variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z_i)$&lt;/code&gt;. I will just copy-paste from the previous section:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\theta^* = \arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log p\left(x_{i}, z_{i}=c | \theta\right) = \arg\max_\theta \mathcal{G}
$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{G}$&lt;/code&gt; is just a short hand for the joint density, and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt; are the mixture proportion &lt;code class=&quot;highlighter-rouge&quot;&gt;$\pi_c$&lt;/code&gt; terms, the Gaussian centres &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mu_c$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\sigma^2_c$&lt;/code&gt; terms.&lt;/p&gt;

&lt;p&gt;Note that here, &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z_i)$&lt;/code&gt; is fixed, even though from the &lt;strong&gt;E&lt;/strong&gt;-step they depend on our parameters. Writing &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{G}$&lt;/code&gt; out fully:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\mathcal{G} = \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log\left(\dfrac{1}{\sigma_c \sqrt{2\pi}}\exp\left(\dfrac{-(x_i-\mu_c)^2}{2\sigma_c^2}\right)\pi_c\right)
$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
= \sum_{i} \sum_{c} q\left(z_{i}=c\right)  \left(\log \dfrac{\pi_c}{\sigma_c\sqrt{2\pi}} - \dfrac{(x_i-\mu_c)^2}{2\sigma_c^2}\right)
$$&lt;/code&gt;
This is all we need to obtain the analytical gradient with respect to model parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;$\theta$&lt;/code&gt;. For example, to get &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mu_k$&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;$k$&lt;/code&gt;th cluster mean):
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\nabla_{\mu_k}\mathcal{G} = \sum_i q(z_i=k) \dfrac{x_i-\mu_k}{\sigma_c^2}
$$&lt;/code&gt;
since for all &lt;code class=&quot;highlighter-rouge&quot;&gt;$c\neq k$&lt;/code&gt; the gradient is zero. Setting above to zero (solve for optimum &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mu_k$&lt;/code&gt;):
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\mu_k = \dfrac{\sum_i q(z_i=k)x_i}{\sum_i q(z_i=k)}
$$&lt;/code&gt;
The same can be done to &lt;code class=&quot;highlighter-rouge&quot;&gt;$\sigma_k^2$&lt;/code&gt; (via high school calculus) to obtain:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\sigma_k^2 =\frac{\sum_{i}\left(x_{i}-\mu_{k}\right)^{2} q\left(z_{i}=k\right)}{\sum_{i} q\left(z_{i}=k\right)}
$$&lt;/code&gt;
To get optimum mixing proportions &lt;code class=&quot;highlighter-rouge&quot;&gt;$\pi_k$&lt;/code&gt; is more complicated, since we need to do &lt;strong&gt;constrained optimisation&lt;/strong&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;$\pi_k \geq 0$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$\sum_c \pi_c=1$&lt;/code&gt; since it needs to be a probability distribution. This can be done through &lt;a href=&quot;https://www.youtube.com/watch?v=yuqB-d5MjZA&quot;&gt;Lagrange multipliers&lt;/a&gt;. Here we actually omit the maths and jump straight into the result:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\pi_k = \dfrac{\sum_i q(z_i=k)}{N}
$$&lt;/code&gt;
where &lt;code class=&quot;highlighter-rouge&quot;&gt;$N$&lt;/code&gt; is the total number of data points. The three equations above should all be fairly intuitive, the common theme is to &lt;strong&gt;re-estimate model parameters with our ‘self-labelled’ data points&lt;/strong&gt; (labelled by our own variational distribution &lt;code class=&quot;highlighter-rouge&quot;&gt;$q$&lt;/code&gt;). One could have jumped straight into these formula (I couldn’t, but I’m sure some mathmos can). But here, we followed the EM algorithm in order to &lt;strong&gt;maximise likelihood of data points under our model&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&quot;32-implementation-&quot;&gt;3.2 Implementation &lt;a name=&quot;3.2&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Of course, any sane person would call &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn.mixture.GaussianMixture&lt;/code&gt; and just do &lt;code class=&quot;highlighter-rouge&quot;&gt;GMM.fit(X)&lt;/code&gt;. But that is way too efficient. We start with some standard import:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# bit confusing, one is used to draw from, the other is used for evaluation
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mv_n&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.testing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;assert_array_almost_equal&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;321-dataground-truth-generation&quot;&gt;3.2.1 Data/Ground Truth Generation&lt;/h3&gt;
&lt;p&gt;First, let’s generate our dataset. Define the ground truth parameters that we wish to estimate:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# total number of data points
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# cluster mean d = 2
# cluster standard deviation
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;.05&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;.1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;.15&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# mixing proportions
# needs to be a valid categorical distribution
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The dataset is drawn according to our model:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Reshape to our N x d dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# visualize data generated&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;we observe the dataset at the beginning of the section (three fairly distinct clusters).&lt;/p&gt;

&lt;p&gt;We also need to initialise guesses for our model parameters. Here we sample from a normal distribution for the cluster centres, and guess identity covariance matrix and uniform mixing proportion:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;322-e-step&quot;&gt;3.2.2 E-Step&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;E&lt;/strong&gt;-step update rule is reproduced here for convenience:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
q\left(z_{i}=c\right)=\frac{\mathcal{N}\left(x_{i} | \mu_{c}, \sigma_{c}^{2}\right) \pi_{c}}{\sum_{k} \mathcal{N}\left(x_{i} | \mu_{k}, \sigma_{k}^{2}\right) \pi_{k}}
$$&lt;/code&gt;
the corresponding code is simply:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;e_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# q(z)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# p(x|z)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# density function evaluation at our data points
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# weight by mixing proportion
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As a sanity check, under the variational distribution, likelihood of each data point for all clusters should sum to one:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;assert_array_almost_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;323-m-step&quot;&gt;3.2.3 M-Step&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;M&lt;/strong&gt;-step update rule is copied here for all three classes of parameters:
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\mu_{k}=\frac{\sum_{i} q\left(z_{i}=k\right) x_{i}}{\sum_{i} q\left(z_{i}=k\right)}
$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$
\sigma_{k}^{2}=\frac{\sum_{i}\left(x_{i}-\mu_{k}\right)^{T}(x_{i}-\mu_{k}) q\left(z_{i}=k\right)}{\sum_{i} q\left(z_{i}=k\right)}$$&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$$\pi_{k}=\frac{\sum_{i} q\left(z_{i}=k\right)}{N}
$$&lt;/code&gt;
Note we slightly modified covariance matrix update rule so it involves an outer product (realised I probably should have worked in vector form earlier, but oh well). The code becomes:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;m_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;q_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# q summed over all data points
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# update cluster mean
&lt;/span&gt;        &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_sum&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# update cluster std
&lt;/span&gt;        &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:])&lt;/span&gt;\
                        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_sum&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# update mixing proportions
&lt;/span&gt;        &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;μ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;324-wrapping-up&quot;&gt;3.2.4 Wrapping Up&lt;/h3&gt;
&lt;p&gt;In theory, we need to check for convergence by evaluation the variational lower bound. This is omitted here. Instead, we run simulation for 150 time steps and obtain:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/M7VCEve.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;&lt;/center&gt;

&lt;p&gt;So as anticipated, our GMM indeed fitted three Gaussians onto the data points. Visually the fit is not bad. But how do the estimated parameters fair with the ground truth? We visualise the phase portrait for cluster means (2D) and variances within the covariance matrix (also 2D) across iterations. Plots below show the ground truth of model parameters in black, and the trajectories GMM generated across the iteration. We observe very good agreement between the predicted locations and ground truth!&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://i.filmot.com/M27QOCA.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;&lt;/center&gt;

&lt;p&gt;In summary, we have show how EM algorithm at heart does maximum likelihood estimation of the dataset being generated by a given model. Again it is worth reflecting on the fact that neither &lt;strong&gt;E&lt;/strong&gt; or &lt;strong&gt;M&lt;/strong&gt; step guarantees closed form solution. This sets up the motivation for future blog posts regarding variational inference and Monte Carlo based methods. Finally, this post wouldn’t be complete if I didn’t include a link to Carl Rasmussen’s &lt;a href=&quot;http://mlg.eng.cam.ac.uk/teaching/4f13/1819/expectation%20maximization.pdf&quot;&gt;lecture notes&lt;/a&gt; on EM (it is super condensed, but good as a quick memory refresher). The &lt;a href=&quot;https://www.youtube.com/watch?v=rVfZHWTwXSA&amp;amp;t=4230s&quot;&gt;best talk&lt;/a&gt; I can find online is given by Andrew Ng, which explains both the overall concept really well and does not shy away from the derivations.&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Jun 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/posts/em</link>
        <guid isPermaLink="true">http://localhost:4000/posts/em</guid>
        
        <category>theory</category>
        
        <category>bayesian</category>
        
        
      </item>
    
  </channel>
</rss>
