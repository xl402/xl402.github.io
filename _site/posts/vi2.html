<!DOCTYPE html>
<html lang="en">
   <head>
       <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, user-scalable=no">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<title>Variational Inference Part 2: Black-Box VI &#9642; Lu Blog</title>
<!--
<meta name="description" content="(30 min) Part 2 of 2 series on variational inference. This part dives into the more practical black-box variational inference. We discuss the REINFORCE algorithm and gradient variance reduction techniques (including the neural baseline).
">
-->
<meta name="description" content="© 2020. Powered by Jekyll & Dactl
">
<meta name="keywords" content="theory, bayesian">
<link rel="canonical" href="http://localhost:4000/posts/vi2">
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Variational Inference Part 2: Black-Box VI" />
<meta name="twitter:description" content="© 2020. Powered by Jekyll & Dactl
" />
<meta name="twitter:image" content="http://localhost:4000" />
<meta name="author" content="">
<link rel="author" href="">
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Variational Inference Part 2: Black-Box VI">
<meta property="og:description" content="© 2020. Powered by Jekyll & Dactl
">
<meta property="og:url" content="http://localhost:4000/posts/vi2">
<meta property="og:site_name" content="Lu Blog">
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source Serif Pro:400,400italic,700,700italic">
<style>
    html {
      font-family: "Source Serif Pro", -apple-system, BlinkMacSystemFont, "Helvetica Neue", sans-serif;
    }
</style>
<link rel="stylesheet" href="/assets/css/bf.css">
   <body>
       <div class="wrapper" id="blep">
          <header>
   <div class="menu">
     <div class="logo">
        <a href="/">Lu Blog</a>
        <object type="image/svg+xml" data="" class="logosvg">Your browser does not support svg images</object>
     </div>
       <ul>
           <li><a href="/">home</a>
           <li><a href="/about">about</a>
           <li><a href="/archive">archive</a>
       </ul>
   </div>
   <div class="social">
     <ul>
       <li>
            <a href="https://www.linkedin.com/in/tomlu97" target="_blank" class="smaller">
              <span class="icon-stackoverflow"></span>
            </a>
       <li>
            <a href="https://github.com/xl402" target="_blank" class="smaller">
              <span class="icon-github"></span>
            </a>
       <li>
            <a href="#" onclick="switchTheme()" title="Switch theme?">
              <span class="icon-invert_colors" id="theme-switcher"></span>
            </a>
     </ul>
   </div>
</header>
<article class="post">
<style>
  .vi2 {
    margin-bottom: 2em;
  }
  .vi2::before {
    background-image: url('https://filmot.com/myX95GO.jpg');
    background-size: cover;
    -webkit-filter: grayscale(1) brightness(0.5) contrast(0.5);
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    z-index: -2;
  }
  .vi2::after {
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    background-color: 0.8);
    /*mix-blend-mode: darken;
    mix-blend-mode: color-burn;
    mix-blend-mode: hard-light;*/
    mix-blend-mode: overlay;
    z-index: -1;
  }
  .vi2-container,
  .vi2-container::before,
  .vi2-container::after {
    height: 25rem;
  }
  .vi2-bleed-container,
  .vi2-bleed-container::before,
  .vi2-bleed-container::after {
    height: 25rem;
  }
    @media (max-width: 48rem) {
        .vi2-container,
        .vi2-container::before,
        .vi2-container::after{
            height: 20rem;
            margin-bottom: 3rem;
        }
    }
</style>
 <div class="post-title-container
  vi2 vi2-bleed-container bleed-hero-container
    ">
    <!--Post hero image source-->
   <div class="heading-container hero-heading hero-heading-post">
     <h1>
          Variational Inference Part 2: Black-Box VI
     </h1>
     <div class="post-meta">
        <span>05/07/2020</span>
        <span>
              <a href="/tag/theory">#theory</a>
              <a href="/tag/bayesian">#bayesian</a>
        </span>
     </div>
   </div>
 </div>
   <p class="lead">(30 min) Part 2 of 2 series on variational inference. This part dives into the more practical black-box variational inference. We discuss the REINFORCE algorithm and gradient variance reduction techniques (including the <em>neural baseline</em>).
<p>
<h1 id="1-introduction">1. Introduction</h1>
<p>In <a href="http://tlublog.com/posts/vi1">part I</a> of the series, we explored the origin of variational inference. As a quick recap, the posterior distribution <code class="highlighter-rouge">$p(\theta |x)$</code> is intractable to compute, as we need to explicitly compute the evidence term:
<p><code class="highlighter-rouge">$$p(X)=\int_{\theta} p(x \mid \theta) p(\theta)d\theta$$</code>
<p>this integration over the parameter space is very very expensive.
<p>To address this problem, we cook up a <em>variational distribution</em> <code class="highlighter-rouge">$q(\theta)$</code> that <em>approximates</em> the posterior. The problem of finding the posterior distribution therefore becomes an <strong>optimisation problem</strong>. Specifically, we wish to minimise:
<code class="highlighter-rouge">$$\text{KL}(q(\theta) \| p(\theta \mid x))$$</code>
we discussed the implication of the ordering between <code class="highlighter-rouge">$q$</code> and <code class="highlighter-rouge">$p$</code> inside the brackets. In order to perform the optimisation, we showed that the objective function above is equivalent to <strong>maximising</strong> the <em>evidence lower bound</em>:
<p><code class="highlighter-rouge">$$q^{*}=\operatorname{argmax}_{q \in Q}\text{ELBO} = \int_\theta q(\theta) \log p(\theta, x)d\theta - \int_\theta q(\theta)\log q(\theta)$$</code>
<code class="highlighter-rouge">$$ = \operatorname{argmax}_{q \in Q} \mathbb{E}_{\theta \sim q}[\log p(\theta, x)] + \mathcal{H}(q(\theta))\quad [1]$$</code>
<p>which intuitively speaking contains the maximum likelihood term <code class="highlighter-rouge">$p(\theta, x)$</code> that prefers point estimation, and the entropy term <code class="highlighter-rouge">$\mathcal{H}(q(\theta))$</code> that prefers <em>diffusive</em> estimation. Also note how this ELBO can be estimated easily with Monte-Carlo sampling. Since by construction, we know how to generate samples from <code class="highlighter-rouge">$p(\theta, x)$</code>.
<p>We then introduced an <strong>analytical method</strong> called <em>mean-field</em> approximation, that falls under the framework of ELBO maximisation. The major problems with the <em>mean-field</em> approach are that:
<ul>
 <li>It places heavy constraint on what our variational distribution <code class="highlighter-rouge">$q(\theta)$</code> (it assumes it to be fully factorised)
 <li>Like me, you probably do not want to go through pages of maths on differentiating some disgusting looking equations (checkout the maths on Bayesian Mixture Model or Latent Dirchlet Allocation if interested)
</ul>
<p>This post describes the more modern techniques used to do variational inference: the <strong>black-box</strong> family. Specifically, we will look at the <em>reparameterisation trick</em> and the famous <em>REINFORCE</em> algorithm. We will also be discussing some variance reduction techniques such as the <em>baseline</em> method. To make this post less dry, I will also be including some <em>Pyro</em> (probabilistic programming language based on <em>PyTorch</em>) just to demonstrate how easy black-box VI is.
<h1 id="2-theory-of-black-box-vi">2. Theory of Black-Box VI</h1>
<h2 id="21-why-is-elbo-backprop-hard">2.1 Why Is ELBO Backprop Hard?</h2>
<p>One natural question to ask is - why can’t we just do backpropogation through this ELBO objective function? Let us first write down our ELBO (in a more workable format):
<p><code class="highlighter-rouge">$$\mathrm{ELBO} = \mathbb{E}_{q_{\phi}(\theta)}\left[\log p(x, \theta)-\log q_{\phi}(\theta)\right]$$</code>
<p>This is not too much change from equation [1], we made it more explicit that our variational distribution <code class="highlighter-rouge">$q$</code> is parameterised by <code class="highlighter-rouge">$\phi$</code>. Form simplicity, let’s call variational distribution <code class="highlighter-rouge">$q$</code> the <strong>guide</strong> (this is commonly used in literature). The problem is that <strong>ELBO is an expectation</strong>, so we need to be able to compute unbiased estimates of
<p><code class="highlighter-rouge">$$\nabla_{\theta, \phi} \mathrm{ELBO}=\nabla_{\theta, \phi} \mathbb{E}_{q_{\phi}(\theta)}\left[\log p(x, \theta)-\log q_{\phi}(\theta)\right]$$</code>
<p>Unfortunately, we <strong>can’t move the grad inside the expectation</strong>, since our expectation is taken under our guide <code class="highlighter-rouge">$q$</code> that depends on <code class="highlighter-rouge">$\phi$</code>. This problem can be framed in a more generic way, we can drop the distinction between <code class="highlighter-rouge">$\theta$</code> and <code class="highlighter-rouge">$\phi$</code>, also let’s use <code class="highlighter-rouge">$f_\phi(\theta)$</code> to denote any arbituary function within the bracket:
<p><code class="highlighter-rouge">$$\nabla_{\phi} \mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]  \quad [2]$$</code>
<p>Equation [2] describes the <em>gradient through an expectation</em> and can be also found in the policy gradient algorithm in reinforcement learning. The remaining of the post aims to obtain <strong>unbiased estimates</strong> of equation [2].
<h2 id="22-reparameterisation-trick">2.2 Reparameterisation Trick</h2>
<p>The reparameterisation trick also known as path-wise gradients allow us to compute equation [2] by rewritting samples from the guide <code class="highlighter-rouge">$q$</code> in terms of a noise variable <code class="highlighter-rouge">$\epsilon$</code>:
<p><code class="highlighter-rouge">$$\mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]=\mathbb{E}_{q(\epsilon)}\left[f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]$$</code>
<p>concretely, <code class="highlighter-rouge">$\epsilon \sim q(\epsilon)$</code> and <code class="highlighter-rouge">$\theta = g_{\phi}(\epsilon)$</code>. Crucially all the <code class="highlighter-rouge">$\phi$</code> dependent terms have been moved inside of the expectation. This kind of reparameterisation can be done for many distributions (e.g. the normal distribution). An example of such reparameterisation can be highlighted by assuming that <code class="highlighter-rouge">$\theta$</code> is sampled from a Gaussian <code class="highlighter-rouge">$\theta \sim \mathcal{N}(\mu, \sigma)$</code>. The function <code class="highlighter-rouge">$g_{\phi}(\varepsilon)$</code> can then be expressed as
<p><code class="highlighter-rouge">$$g_{\phi}(\varepsilon)=\mu_{\phi}+\varepsilon \sigma_{\phi}$$</code>
<p>where <code class="highlighter-rouge">$\epsilon \sim \mathcal{N}(0, 1)$</code>. Assuming <code class="highlighter-rouge">$f(.)$</code> and <code class="highlighter-rouge">$g(.)$</code> are sufficiently smooth, we can now get unbiased estimates of the gradient of interest by taking a Monte Carlo estimate of this expectation:
<p><code class="highlighter-rouge">$$\nabla_{\phi} \mathbb{E}_{q(\epsilon)}\left[f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]=\mathbb{E}_{q(\epsilon)}\left[\nabla_{\phi} f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]$$</code>
<p>In this case, not only do <code class="highlighter-rouge">$\theta$</code> need to come from specific distributions (i.e. the exponential family), we also need to be able to differentiate through <code class="highlighter-rouge">$f_\phi$</code>. In the case where we have discrete latent variables, the gradient of <code class="highlighter-rouge">$f_\phi$</code> is zero almost everywhere in the parameter space.
<h2 id="23-reinforce-for-non-reparameterizable-random-variables">2.3 REINFORCE for Non-Reparameterizable Random Variables</h2>
<p>I am introducing REINFORCE under the scope of variational inference, however, this trick is most commonly seen in reinforcement learning (policy gradient). The concept of REINFORCE is quiet simple: <strong>re-write equation [2] to some form so we can obtain unbiased estimate via Monte-Carlo</strong>. Specifically, we want to message a gradient of expectation into expectation of some gradient. We begin by expanding the terms:
<p><code class="highlighter-rouge">$$\nabla_{\phi}\text{ELBO}=\nabla_{\phi} \mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]=\nabla_{\phi} \int d\theta q_{\phi}(\theta) f_{\phi}(\theta)$$</code>
<p>applying chain rule:
<p><code class="highlighter-rouge">$$\int d \theta\left[\left(\nabla_{\phi} q_{\phi}(\theta)\right) f_{\phi}(\theta)+q_{\phi}(\theta)\left(\nabla_{\phi} f_{\phi}(\theta)\right)\right]$$</code>
<p>next, we use the log-derivative trick:
<p><code class="highlighter-rouge">$$\nabla_{\phi} q_{\phi}(\theta)=q_{\phi}(\theta) \nabla_{\phi} \log q_{\phi}(\theta)$$</code>
<p>to obtain
<p><code class="highlighter-rouge">$$\nabla_{\phi}\text{ELBO}=\mathbb{E}_{q_{\phi}(\theta)}\left[\left(\nabla_{\phi} \log q_{\phi}(\theta)\right) f_{\phi}(\theta)+\nabla_{\phi} f_{\phi}(\theta)\right] \quad [3]$$</code>
<p>Equation [3] is the REINFORCE equation. It is quiet disgusting looking and I do not have any intuition to what it represent (frankly, the ELBO objective took me a while to digest). But just by looking at it, we can now obtain unbiased estimates of our ELBO gradient! One way to package this result (for implementation) is by introducing a surrogate function:
<p><code class="highlighter-rouge">$$\text { surrogate objective }= \log q_{\phi}(\theta) \overline{f_{\phi}(\theta)}+f_{\phi}(\theta)$$</code>
<p>to be passed through the autograd, where <code class="highlighter-rouge">$\overline{f_{\phi}(\theta)}$</code> is held as a constant (detached during autograd). Equation [3] therefore becomes:
<p><code class="highlighter-rouge">$$\nabla_{\phi} \mathrm{ELBO}=\mathbb{E}_{q_{\phi}(\theta)}\left[\nabla_{\phi}(\text { surrogate objective })\right] \quad [4]$$</code>
<p>It would be good to finish the story here. Unfortunately, equation [4] suffers from <strong>high variance</strong> for a range of <code class="highlighter-rouge">$f(.)$</code>. So in although we can theoretically obtain unbiased estimates of the true gradient, in reality, noone is going to wait for 10,000 samples per backprop iteration. We therefore <strong>need ways to reduce the variance of our estimated ELBO gradient</strong>.
<h1 id="3-practical-vi-with-pyro">3. Practical VI with Pyro</h1>
<p>Let’s demonstrate how black-box VI actually works using Pyro. We can build a very dumb model that estimates whether a coin is biased based on the number of tosses. The good thing with this model is that - there exists a analytical solution. Unfortunately, this blog is not about Pyro, so the code won’t be explained in depth. However, the code is pretty much copy-pasted from the <a href="https://pyro.ai/examples/svi_part_ii.html">Pyro tutorial</a> itself.
<p>Say we wish to find the posterior distribution of the parameter <code class="highlighter-rouge">$p$</code>, which governs the probability that a coin toss yields a head:
<p><code class="highlighter-rouge">$$X\sim \text{Bernouli}(p)$$</code>
<p>we place beta prior on <code class="highlighter-rouge">$p$</code>:
<p><code class="highlighter-rouge">$$p\sim \text{Beta}(10, 10)$$</code>
<p>first, we toss the coin 10 times and observe 6 heads and 4 tails:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributions.constraints</span> <span class="kn">as</span> <span class="nn">constraints</span>
<span class="kn">from</span> <span class="nn">pyro.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">pyro.infer</span> <span class="kn">import</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span>
<span class="kn">import</span> <span class="nn">pyro.distributions</span> <span class="kn">as</span> <span class="nn">dist</span>

<span class="n">N_STEPS</span> <span class="o">=</span> <span class="mi">5000</span> <span class="c"># number of gradient update</span>
<span class="n">N_HEADS</span> <span class="o">=</span> <span class="mi">6</span> <span class="c"># observed number of heads</span>
<span class="n">N_TAILS</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c"># clear param store</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">clear_param_store</span><span class="p">()</span>

<span class="c"># create some data with some observed heads and some observed tails</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N_HEADS</span><span class="o">+</span><span class="n">N_TAILS</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">N_HEADS</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N_HEADS</span><span class="p">)</span></code></pre></figure>
<p>We then build this <strong>generative</strong> model describing how we may draw coin toss observations from the prior:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">alpha0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10.0</span><span class="p">)</span>
    <span class="n">beta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10.0</span><span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s">"latent_fairness"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="n">alpha0</span><span class="p">,</span> <span class="n">beta0</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s">"data_loop"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
        <span class="c"># observe datapoint i using the bernoulli likelihood</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s">"obs_{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">dist</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span></code></pre></figure>
<p>with <em>latent_fairness</em> denoting <code class="highlighter-rouge">$p$</code>, the parameter whose distribution we are interested in estimating. Note we used <code class="highlighter-rouge">pyro.plate</code> to mark conditional independence in our observations, this is because <code class="highlighter-rouge">$x\in X$</code> are i.i.d observations. Not only is this step important for model scaling (many observations), but it also helps with ELBO gradient variance reduction (see section below).
<p>The analytical posterior can be calculated:
<p><code class="highlighter-rouge">$$P(p|X)\sim \text{Beta}(10+h, 10+t)$$</code>
<p>where <code class="highlighter-rouge">$h$</code> and <code class="highlighter-rouge">$t$</code> are the number of observed head and tails respectively.
<p>For the purpose of demonstrating SVI, we specify a <em>variational posterior distribution</em>, also called a guide in pyro $q$, where
<p><code class="highlighter-rouge">$$q\sim \text{Beta}(\alpha, \beta)$$</code>
<p>we need to ensure that $\alpha$ and $\beta$ are constrained to be positive
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">guide</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">alpha_q</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s">"alpha_q"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">15.0</span><span class="p">),</span>
                         <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="n">beta_q</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s">"beta_q"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">15.0</span><span class="p">),</span>
                        <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="c"># sample latent_fairness from the distribution Beta(alpha_q, beta_q)</span>
    <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s">"latent_fairness"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="n">alpha_q</span><span class="p">,</span> <span class="n">beta_q</span><span class="p">))</span></code></pre></figure>
<p><strong>it is important that in the guide, parameters names match for those we want to approximate</strong> (in this case, the <em>latent_fairness</em>). Finally, we use Adam optimiser, specifying ELBO loss and just do inference!
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># setup the optimizer</span>
<span class="n">adam_params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"lr"</span><span class="p">:</span> <span class="mf">0.00025</span><span class="p">,</span> <span class="s">"betas"</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)}</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">adam_params</span><span class="p">)</span>

<span class="c"># setup the inference algorithm</span>
<span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">guide</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">())</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N_STEPS</span><span class="p">)</span>

<span class="c"># do gradient steps</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N_STEPS</span><span class="p">)):</span>
    <span class="n">losses</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">svi</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code></pre></figure>
<p>That’s it! To evaluate model correctness:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># grab the learned variational parameters</span>
<span class="n">alpha_q</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s">"alpha_q"</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">beta_q</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s">"beta_q"</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">inferred_mean</span> <span class="o">=</span> <span class="n">alpha_q</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha_q</span> <span class="o">+</span> <span class="n">beta_q</span><span class="p">)</span>
<span class="n">factor</span> <span class="o">=</span> <span class="n">beta_q</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha_q</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">alpha_q</span> <span class="o">+</span> <span class="n">beta_q</span><span class="p">))</span>
<span class="n">inferred_std</span> <span class="o">=</span> <span class="n">inferred_mean</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">factor</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Posterior alpha and beta values are {:.1f} and {:.1f} respectively</span><span class="se">\n</span><span class="s">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha_q</span><span class="p">,</span>
                                                                                    <span class="n">beta_q</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The fairness of the coin is {:.3f} ± {:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">inferred_mean</span><span class="p">,</span> <span class="n">inferred_std</span><span class="p">))</span></code></pre></figure>
<p>We get:
<p><code class="highlighter-rouge">Posterior alpha and beta values are 16.1 and 13.9 respectively</code>
<code class="highlighter-rouge">The fairness of the coin is 0.536 ± 0.090</code>
<p>which is pretty much the same as our analytical result (16 and 14 for the posterior alpha, beta values).
<h1 id="4-elbo-gradient-variance-reduction">4. ELBO Gradient Variance Reduction</h1>
<h2 id="41-rao-blackwellization">4.1 Rao-Blackwellization</h2>
<h2 id="42-baselines">4.2 Baselines</h2>
<p>Another strategy for reducing variance in the ELBO gradient estimator goes under the name of baselines. The key point is to note that for any constant <code class="highlighter-rouge">$b$</code>,
<p><code class="highlighter-rouge">$$\mathbb{E}_{q_{\phi}(\theta)}\left[\nabla_{\phi}\left(\log q_{\phi}(\theta) \times b\right)\right]=0$$</code>
<p>the proof can be done in one line:
<p><code class="highlighter-rouge">$$\mathbb{E}_{q_{\phi}(\theta)}\left[\nabla_{\phi} \log q_{\phi}(\theta)\right]=\int d \theta q_{\phi}(\theta) \nabla_{\phi} \log q_{\phi}(\theta)=\int d \theta \nabla_{\phi} q_{\phi}(\theta)=\nabla_{\phi} \int d \theta q_{\phi}(\theta)=\nabla_{\phi} 1=0$$</code>
<p>hence we can replace our surrogate objective
<p><code class="highlighter-rouge">$$\log q_{\phi}(\theta) \overline{f_{\phi}(\theta)}$$</code>
<p>with
<p><code class="highlighter-rouge">$$\log q_{\phi}(\theta)\left( \overline{f_{\phi}(\theta)} - b\right)$$</code>
<p>Therefore, the mean of our gradient estimator does not change, but the variance gets reduced (it’s a bit more involved to show this, but intuitively, if we track the mean of <code class="highlighter-rouge">$\overline{f_{\phi}(\theta)}$</code> then the variance should decrease).
<p>So our variance reduction task becomes predicting what <code class="highlighter-rouge">$\overline{f_{\phi}(\theta)}$</code> is at each time-step. One simplest baseline is to simply perform a moving average of recent samples of <code class="highlighter-rouge">$\overline{f_{\phi}(\theta)}$</code>. In Pyro, this looks like:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s">"z"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">My_Dist</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
                <span class="n">infer</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">baseline</span><span class="o">=</span><span class="p">{</span><span class="s">'use_decaying_avg_baseline'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
                                     <span class="s">'baseline_beta'</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">}))</span></code></pre></figure>
<p>Of course, since the task to reduce ELBO variance essentially becomes prediction <code class="highlighter-rouge">$\overline{f_{\phi}(\theta)}$</code>, why not use an auxiliary neural-network to do this for us? The idea of <strong>neural-baseline</strong> is born. This in Pyro is not too much work. First, we construct some multilayer perceptron (or any DNN of your choice):
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">BaselineNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_input</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_hidden</span><span class="p">,</span><span class="n">dim_hidden</span><span class="p">)</span>
        <span class="c"># add more layers/activations</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">out</span></code></pre></figure>
<p>for our variational distribution (guide), we simply define:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">baseline_module</span> <span class="o">=</span> <span class="n">BaselineNN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">guide</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">pyro</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="s">"my_baseline"</span><span class="p">,</span> <span class="n">baseline_module</span><span class="p">)</span>
    <span class="c"># ... other computations ...</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s">"z"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">My_Dist</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
                    <span class="n">infer</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">baseline</span><span class="o">=</span><span class="p">{</span><span class="s">'nn_baseline'</span><span class="p">:</span> <span class="n">baseline_module</span><span class="p">,</span>
                                         <span class="s">'nn_baseline_input'</span><span class="p">:</span> <span class="n">x</span><span class="p">}))</span></code></pre></figure>
<p>Under the hood Pyro constructs a loss of the form:
<code class="highlighter-rouge">$$\text { baseline loss } \equiv(\overline{f_{\phi}(\mathbf{z})}-b)^{2}$$</code>
which intuitively makes sense. However, there is no theorem that suggests this is the optimal loss function to use in this context, but in practice works well.
<p>Also in practice it can be important to use a different set of learning hyperparameters for baseline parameters (normally, the lr for baseline is higher than the actual model). In Pyro this is done through:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">per_param_args</span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="s">'baseline'</span> <span class="ow">in</span> <span class="n">param_name</span> <span class="ow">or</span> <span class="s">'baseline'</span> <span class="ow">in</span> <span class="n">module_name</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"lr"</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"lr"</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">per_param_args</span><span class="p">)</span></code></pre></figure>
</article>
<aside class="related">
<h2>Related posts</h2>
<ul class="related-posts">
   <li>
    <a href="/posts/vi1">
        <span>Variational Inference Part 1: Intro and Mean-Field</span>
        <small>16/06</small>
    </a>
   <li>
    <a href="/posts/em">
        <span>Expectation Maximisation Deep Dive</span>
        <small>12/06</small>
    </a>
</aside>
           <footer>
    <span>© 2020. Powered by Jekyll & Dactl
</span>
    <span>written by Tom Lu</span>
</footer>
       </div>
<script type="text/javascript" src="/assets/js/theme.js"></script>
<script type="text/javascript" src="/assets/js/barefoot.js"></script>
<script src="//yihui.org/js/math-code.js"></script>
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
