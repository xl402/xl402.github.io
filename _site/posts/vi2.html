<!DOCTYPE html>
<html lang="en">
   <head>
       <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, user-scalable=no">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<title>Variational Inference Part 2 &#9642; Lu Blog</title>
<!--
<meta name="description" content="(30 min) Part 2 of 2 series on variational inference. This part dives into the more practical black-box variational inference. We discuss the REINFORCE algorithm and gradient variance reduction techniques (including the neural baseline).
">
-->
<meta name="description" content="© 2020. Powered by Jekyll & Dactl
">
<meta name="keywords" content="theory, bayesian">
<link rel="canonical" href="http://localhost:4000/posts/vi2">
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Variational Inference Part 2" />
<meta name="twitter:description" content="© 2020. Powered by Jekyll & Dactl
" />
<meta name="twitter:image" content="http://localhost:4000" />
<meta name="author" content="">
<link rel="author" href="">
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Variational Inference Part 2">
<meta property="og:description" content="© 2020. Powered by Jekyll & Dactl
">
<meta property="og:url" content="http://localhost:4000/posts/vi2">
<meta property="og:site_name" content="Lu Blog">
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source Serif Pro:400,400italic,700,700italic">
<style>
    html {
      font-family: "Source Serif Pro", -apple-system, BlinkMacSystemFont, "Helvetica Neue", sans-serif;
    }
</style>
<link rel="stylesheet" href="/assets/css/bf.css">
   <body>
       <div class="wrapper" id="blep">
          <header>
   <div class="menu">
     <div class="logo">
        <a href="/">Lu Blog</a>
        <object type="image/svg+xml" data="" class="logosvg">Your browser does not support svg images</object>
     </div>
       <ul>
           <li><a href="/">home</a>
           <li><a href="/about">about</a>
           <li><a href="/archive">archive</a>
       </ul>
   </div>
   <div class="social">
     <ul>
       <li>
            <a href="https://www.linkedin.com/in/tomlu97" target="_blank" class="smaller">
              <span class="icon-stackoverflow"></span>
            </a>
       <li>
            <a href="https://github.com/xl402" target="_blank" class="smaller">
              <span class="icon-github"></span>
            </a>
       <li>
            <a href="#" onclick="switchTheme()" title="Switch theme?">
              <span class="icon-invert_colors" id="theme-switcher"></span>
            </a>
     </ul>
   </div>
</header>
<article class="post">
<style>
  .vi2 {
    margin-bottom: 2em;
  }
  .vi2::before {
    background-image: url('https://filmot.com/myX95GO.jpg');
    background-size: cover;
    -webkit-filter: grayscale(1) brightness(0.5) contrast(0.5);
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    z-index: -2;
  }
  .vi2::after {
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    background-color: 0.8);
    /*mix-blend-mode: darken;
    mix-blend-mode: color-burn;
    mix-blend-mode: hard-light;*/
    mix-blend-mode: overlay;
    z-index: -1;
  }
  .vi2-container,
  .vi2-container::before,
  .vi2-container::after {
    height: 25rem;
  }
  .vi2-bleed-container,
  .vi2-bleed-container::before,
  .vi2-bleed-container::after {
    height: 25rem;
  }
    @media (max-width: 48rem) {
        .vi2-container,
        .vi2-container::before,
        .vi2-container::after{
            height: 20rem;
            margin-bottom: 3rem;
        }
    }
</style>
 <div class="post-title-container
  vi2 vi2-bleed-container bleed-hero-container
    ">
    <!--Post hero image source-->
   <div class="heading-container hero-heading hero-heading-post">
     <h1>
          Variational Inference Part 2
     </h1>
     <div class="post-meta">
        <span>05/07/2020</span>
        <span>
              <a href="/tag/theory">#theory</a>
              <a href="/tag/bayesian">#bayesian</a>
        </span>
     </div>
   </div>
 </div>
   <p class="lead">(30 min) Part 2 of 2 series on variational inference. This part dives into the more practical black-box variational inference. We discuss the REINFORCE algorithm and gradient variance reduction techniques (including the <em>neural baseline</em>).
<p>
<h1 id="1-introduction">1. Introduction</h1>
<p>In <a href="http://tlublog.com/posts/vi1">part I</a> of the series, we explored the origin of variational inference. As a quick recap, the posterior distribution <code class="highlighter-rouge">$p(\theta |X)$</code> is intractable to compute, as we need to explicitly compute the evidence term:
<p><code class="highlighter-rouge">$$p(X)=\int_{\theta} p(X \mid \theta) p(\theta)$$</code>
<p>this integration over the parameter space is very very expensive.
<p>We cook up a <em>variational distribution</em> <code class="highlighter-rouge">$q(\theta)$</code> that <em>approximates</em> the posterior. The problem of finding the posterior distribution therefore becomes an <strong>optimisation problem</strong>. Specifically, we wish to minimise:
<code class="highlighter-rouge">$$\text{KL}(q(\theta) \| p(\theta \mid X))$$</code>
we discussed the implication of the ordering between <code class="highlighter-rouge">$q$</code> and <code class="highlighter-rouge">$p$</code> inside the brackets. In order to perform the optimisation, we showed that the objective function above is equivalent to <strong>maximising</strong> the <em>evidence lower bound</em>:
<p><code class="highlighter-rouge">$$q^{*}=\operatorname{argmax}_{q \in Q}\text{ELBO} = \int_\theta q(\theta) \log p(\theta, X)d\theta - \int_\theta q(\theta)\log q(\theta)$$</code>
<code class="highlighter-rouge">$$ = \operatorname{argmax}_{q \in Q} \mathbb{E}_{\theta \sim q}[p(\theta, X)] + \mathcal{H}(q(\theta))$$</code>
<p>which intuitively speaking contains the maximum likelihood term <code class="highlighter-rouge">$p(\theta, X)$</code> that prefers point estimation, and the entropy term <code class="highlighter-rouge">$\mathcal{H}(q(\theta))$</code> that prefers <em>diffusive</em> estimation. Also note how this ELBO can be estimated easily with Monte-Carlo sampling. Since by construction, we know how to generate samples from <code class="highlighter-rouge">$p(\theta, X)$</code>.
<p>We then introduced an <strong>analytical method</strong> called <em>mean-field</em> approximation, that falls under the framework of ELBO maximisation. One major problem with the <em>mean-field</em> approach is that:
<ul>
 <li>It places heavy constraint on what our variational distribution <code class="highlighter-rouge">$q(\theta)$</code> (it assumes it to be fully factorised)
 <li>Like me, you probably do not want to go through pages of maths on differentiating some disgusting looking equations (checkout the maths on Bayesian Mixture Model or Latent Dirchlet Allocation if interested)
</ul>
<p>This post describes the more modern techniques used to do variational inference: the <strong>black-box</strong> family. Specifically, we will look at the <em>reparameterisation trick</em> and the famous <em>REINFORCE</em> algorithm. We will also be discussing some variance reduction techniques such as the <em>baseline</em> method. To make this post less dry, I will also be including some <em>Pyro</em> (probabilistic programming language based on <em>PyTorch</em>) just to demonstrate how easy black-box VI is.
</article>
<aside class="related">
<h2>Related posts</h2>
<ul class="related-posts">
   <li>
    <a href="/posts/vi1">
        <span>Variational Inference Part 1</span>
        <small>16/06</small>
    </a>
   <li>
    <a href="/posts/em">
        <span>Expectation Maximisation Deep Dive</span>
        <small>12/06</small>
    </a>
</aside>
           <footer>
    <span>© 2020. Powered by Jekyll & Dactl
</span>
    <span>written by Tom Lu</span>
</footer>
       </div>
<script type="text/javascript" src="/assets/js/theme.js"></script>
<script type="text/javascript" src="/assets/js/barefoot.js"></script>
<script src="//yihui.org/js/math-code.js"></script>
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
