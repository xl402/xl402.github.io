<!DOCTYPE html>
<html lang="en">
   <head>
       <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, user-scalable=no">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<title>Few-shot Learning &#9642; Lu Blog</title>
<!--
<meta name="description" content="(30 min) Literature review of popular methods in few-shot learning. With a focus on metric-learning, meta-learning and bayesian few-shots.
">
-->
<meta name="description" content="© 2020. Powered by Jekyll & Dactl
">
<meta name="keywords" content="practical, deep-learning">
<link rel="canonical" href="http://localhost:4000/posts/few-shot">
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Few-shot Learning" />
<meta name="twitter:description" content="© 2020. Powered by Jekyll & Dactl
" />
<meta name="twitter:image" content="http://localhost:4000" />
<meta name="author" content="">
<link rel="author" href="">
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Few-shot Learning">
<meta property="og:description" content="© 2020. Powered by Jekyll & Dactl
">
<meta property="og:url" content="http://localhost:4000/posts/few-shot">
<meta property="og:site_name" content="Lu Blog">
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source Serif Pro:400,400italic,700,700italic">
<style>
    html {
      font-family: "Source Serif Pro", -apple-system, BlinkMacSystemFont, "Helvetica Neue", sans-serif;
    }
</style>
<link rel="stylesheet" href="/assets/css/bf.css">
   <body>
       <div class="wrapper" id="blep">
          <header>
   <div class="menu">
     <div class="logo">
        <a href="/">Lu Blog</a>
        <object type="image/svg+xml" data="" class="logosvg">Your browser does not support svg images</object>
     </div>
       <ul>
           <li><a href="/">home</a>
           <li><a href="/about">about</a>
           <li><a href="/archive">archive</a>
       </ul>
   </div>
   <div class="social">
     <ul>
       <li>
            <a href="https://www.linkedin.com/in/tomlu97" target="_blank" class="smaller">
              <span class="icon-stackoverflow"></span>
            </a>
       <li>
            <a href="https://github.com/xl402" target="_blank" class="smaller">
              <span class="icon-github"></span>
            </a>
       <li>
            <a href="#" onclick="switchTheme()" title="Switch theme?">
              <span class="icon-invert_colors" id="theme-switcher"></span>
            </a>
     </ul>
   </div>
</header>
<article class="post">
<style>
  .few-shot {
    margin-bottom: 2em;
  }
  .few-shot::before {
    background-image: url('https://filmot.com/JrO3BLQ.jpg');
    background-size: cover;
    -webkit-filter: grayscale(1) brightness(0.5) contrast(0.5);
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    z-index: -2;
  }
  .few-shot::after {
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    background-color: 0.8);
    /*mix-blend-mode: darken;
    mix-blend-mode: color-burn;
    mix-blend-mode: hard-light;*/
    mix-blend-mode: overlay;
    z-index: -1;
  }
  .few-shot-container,
  .few-shot-container::before,
  .few-shot-container::after {
    height: 25rem;
  }
  .few-shot-bleed-container,
  .few-shot-bleed-container::before,
  .few-shot-bleed-container::after {
    height: 25rem;
  }
    @media (max-width: 48rem) {
        .few-shot-container,
        .few-shot-container::before,
        .few-shot-container::after{
            height: 20rem;
            margin-bottom: 3rem;
        }
    }
</style>
 <div class="post-title-container
  few-shot few-shot-bleed-container bleed-hero-container
    ">
    <!--Post hero image source-->
   <div class="heading-container hero-heading hero-heading-post">
     <h1>
          Few-shot Learning
     </h1>
     <div class="post-meta">
        <span>21/06/2020</span>
        <span>
              <a href="/tag/practical">#practical</a>
              <a href="/tag/deep-learning">#deep-learning</a>
        </span>
     </div>
   </div>
 </div>
   <p class="lead">(30 min) Literature review of popular methods in few-shot learning. With a focus on metric-learning, meta-learning and bayesian few-shots.
<p>
<h1 id="1-introduction">1. Introduction</h1>
<p>It is an understatement to say that modern Deep Neural Networks (DNNs) require large amount of data during training. This can be shown both in practice and through <a href="https://mostafa-samir.github.io/ml-theory-pt2/">Learning Theory</a>. For example, the classic benchmark datasets such as <a href="http://www.image-net.org/">ImageNet</a> contains 14 million images (for 1000 classes), language models such as <a href="https://arxiv.org/abs/2005.14165">GTP3</a> are literally trained on THE INTERNET.
<p>While these DNNs’ performance have been impressive, occasionally achieving super-human level accuracy/performance. In real life, data is not as abundant and easy to obtain. The problem is exacerbated when <strong>labelled</strong> data is required (because £££). Another key challenge in a real-life setting is that model output requirement changes over time. For example, imagine working for an identity-document classification company; at launch, your company may only support 4 types of IDs. Over time, the number of supported IDs will grow. Re-training the model itself is an art and can be very time consuming.
<p>When quality data is scarce and model reusability is key, <strong>few-shot</strong> learning have been employed successfully to discover patterns in data and make beneficial predictions. When combined with <strong>active learning</strong> can often bring a competitive edge at the inception of an ML company!
<h1 id="2-problem-definition-and-datasets">2. Problem Definition and Datasets</h1>
<p><em>Few-shot classification</em> is an instantiation of meta-learning in the field of supervised learning (I think the distinction between few-shot and meta-learning is rather arbitrary). The best way to understand what few-shot learning is trying to achieve is by examining a few common datasets used to train and evaluate few-shot models. Let’s begin by looking at the <em>K-shot N-class classification task</em>:
<p class="notice"><em>K-shot N-class classification task</em>: during training, we are provided with <strong>several datasets</strong> each containing <strong><em>N</em> classes with <em>K</em> examples</strong>, at test time, we are given <strong>a new dataset</strong>, and asked to make predictions based on the labelled examples.
<p>The term few-shot (or <em>K</em>-shot) therefore means the number of examples provided per class (or task). Pictorially, this is shown as a <strong>dataset of datasets</strong>:
<center>
<img src="https://i.filmot.com/XJtP39p.png" alt="drawing" width="850" />
<figcaption>
Fig 1. Typical dataset used for evaluating few-shot models. Note the unusualness of this "dataset of datasets". Image is taken from <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html">LiLian's blog</a>.
</figcaption>
</center>
<p>
<p>To be more concrete, the <a href="https://github.com/brendenlake/omniglot/">Omniglot</a> dataset contains 50 different alphabets, each containing different number (around 20) characters. Each character is drawn by 20 people, amounting to a total of 1623 characters.
<center>
<img src="https://i.filmot.com/qKEb5Mn.png" alt="drawing" width="700" />
<figcaption>
Fig 2. Omniglot dataset
</figcaption>
</center>
<p>
<p>Going back to the earlier definition, each alphabet’s OCR problem can be characterised as an <em>S</em>-class 20-shots. Where <em>S</em> corresponds to the size of each alphabet.
<p>One of the most challenging dataset used for few-shots learning is the <a href="https://www.kaggle.com/whitemoon/miniimagenet">mini-ImageNet</a> which contains 100 classes, each with only 600 examples. It is worth mentioning, the way train-val-test split for few-shots learning is <strong>done on the classes</strong>, in this case, we typically have a 64-16-20 split.
<center>
<img src="https://i.filmot.com/79Yb3nb.png" alt="drawing" width="700" />
<figcaption>
Fig 3. Mini-ImageNet dataset
</figcaption>
</center>
<p>
<p>Hopefully I have conveyed how challenging few-shot learning is. Personally, I think few-shot is more general than the typical supervised DNNs. People normally call this type of learning “learning to learn”, since our model needs to either have weights/architecture that generalises across different tasks, or to be able to rapidly change the weights to adapt to a new task.
<h1 id="3-methodsliterature-overview">3. Methods/Literature Overview</h1>
<p>One rather obvious (though practically difficult) thing to achieve few-shots learning is through <strong>data augmentation</strong>. The motivation is simply: we do not have enough example data per class, let’s just generate more. People can really go crazy on data augmentation, recent methods use ideas such as <a href="https://papers.nips.cc/paper/7504-metagan-an-adversarial-approach-to-few-shot-learning">GANs</a> and <a href="https://arxiv.org/abs/1904.03472">Sailency-guided Hallucinations</a>. Since we can always perform data augmentation in conjunction with other approaches (and frankly, the concept is not new), this class of method will not be discussed in this post.
<p>The other approach which is done by most ML practitioners is <strong>transfer learning</strong>. The idea of taking an off-the-shelf model trained by big research groups on millions of data points, then fine tune the last few layer’s weights on our own dataset is definitely not new. Therefore, we will also omit discussing these. In fact, for most methods demonstrated, assume we always do some sort of transfer learning via models like VGG net.
<p><strong>Metric-based learning</strong> such as the <a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"><em>Siamese networks</em></a> and <a href="https://arxiv.org/abs/1606.04080"><em>Matching networks</em></a> will be described in depth here. The fundamental idea is that these methods learn a distance metric between the querying data point and the support set (examples).
<p><strong>Optimisation-based learning</strong> such as the <a href="https://openreview.net/pdf?id=rJY0-Kcll"><em>LSTM Meta-Learner</em></a> and <a href="https://arxiv.org/abs/1703.03400"><em>Model-Agnostic Meta-Learning</em></a> modifies the classic gradient descent. Encouraging the loss function to converge within a small number of optimisation steps.
<p><strong>Model-based learning</strong> designs models specifically for fast-learning. This is normally achieved through making the network having explicit memories (much like Neural Turing Machines). These models are interesting but because they are quiet contrived, we will omit their discussions here.
<p>Finally, we will briefly look at <strong>Bayesian methods</strong> for few-shot learning, including <a href="http://bayesiandeeplearning.org/2018/papers/10.pdf">VERSA</a> by Richard Turner et al.
<h1 id="4-metric-based-approach">4. Metric-Based Approach</h1>
<p>The core-idea behind metric-based few-shot approaches can be thought as performing <strong>mixture modelling</strong> with some deep network. Let’s define a support set <code class="highlighter-rouge">$S = \{X, Y\}$</code> which is a set of <strong>labelled examples</strong> during training and/or inference. At a high-level, the metric-based approach produces outputs as weighted sums of support set labels:
<code class="highlighter-rouge">$$P_{\theta}(y \mid \mathbf{x}, S)=\sum_{\left(\mathbf{x}_{i}, y_{i}\right) \in S} k_{\theta}\left(\mathbf{x}, \mathbf{x}_{i}\right) y_{i}\quad\quad [1]$$</code>
where the weight is generated by some kernel function <code class="highlighter-rouge">$k_\theta$</code>.
<h2 id="41-siamese-network">4.1 Siamese Network</h2>
<p>The most naive form of metric-based few-shot learning is the Siamese network. Where we simply use a CNN <code class="highlighter-rouge">$f_\theta$</code> (i.e. VGG network) to extract features from two images. Then <code class="highlighter-rouge">$l_1$</code> (or any other differentiable distance metric like cosine distance) is computed between the two the embeddings:
<center><img src="https://i.filmot.com/Ei8dDIO.png" alt="drawing" width="450" /></center>
<p>The distance is converted to a pseudo-probability by a linear feedforward layer and a sigmoid (no theoretical grounds here…). To put the animation above in a mathematical form (with <code class="highlighter-rouge">$l_1$</code> distance metric):
<code class="highlighter-rouge">$$p\left(x_{i}, x_{j}\right)=\sigma\left(\mathbf{W}\left|f_{\theta}\left(x_{i}\right)-f_{\theta}\left(x_{j}\right)\right|\right)$$</code>
<p>Where <code class="highlighter-rouge">$\mathbf{W}$</code> can be trained in conjunction with the CNN via binary cross entropy loss (whether the two images are of the same class or not).
<p>To classify a query image, we do a ‘brute-force search’, by running our query image and <em>every class of support images</em> through the network above. Final prediction is simply an argmax over the <em>probability that the query image has the same class as the support image</em>:
<code class="highlighter-rouge">$$\hat{c}_{S}(x)=c\left(\arg \max _{x_{i} \in S} p\left(x, x_{i}\right)\right)$$</code>
where <code class="highlighter-rouge">$c(x)$</code> is the class label of an image <code class="highlighter-rouge">$x$</code>. This is animated below:
<center><img src="https://i.filmot.com/n2ZygDK.png" alt="drawing" width="450" /></center>
<p>Of course, you can shoe-horn the formula above into the format of equation <code class="highlighter-rouge">$[1]$</code> by performing softmax over all the <code class="highlighter-rouge">$p\left(x, x_{i}\right)$</code> pairs (i.e. re-normalising). But this does not change the performance of the method.
<h2 id="42-relation-network">4.2 Relation Network</h2>
<p>One obvious downside of the Siamese network is the choice of the distance function being quiet arbitrary. Why did we use <code class="highlighter-rouge">$l_1$</code> over <code class="highlighter-rouge">$l_2$</code>? One other disadvantage is that distances are calculated independently across the support set, which ignores higher-level interaction between the query image and the support set.
<p>Relation network overcomes these disadvantages by using another convolution network acting as the distance function. It’s inputs are the concatenated features between the query image and <strong>all images in the support set</strong> (one per class). This is animated below:
<center><img src="https://i.filmot.com/UbMjRGh.png" alt="drawing" width="450" /></center>
<p>One note on the loss function, since we are more interested in the relationship between the query image and the support set, which is more like a regression; MSE loss is used instead of cross entropy:
<code class="highlighter-rouge">$$\mathcal{L}=\sum_{\left(x_{i}, \mathbf{x}_{j}, y_{i}, y_{j}\right) \in S}\left(g_\theta(z)-1_{y_{i}=y_{j}}\right)^{2}$$</code>
where <code class="highlighter-rouge">$z$</code> is the concatenated output by the feature extractor <code class="highlighter-rouge">$f_\phi$</code>.
<h2 id="42-prototyping-networks">4.2 Prototyping Networks</h2>
<p>One major disadvantage of both methods above is that <strong>we have been using one example image per class</strong>. This is a <code class="highlighter-rouge">$K$</code>-shot learning when <code class="highlighter-rouge">$K=1$</code>, i.e. 1-shot learning. We typically want a model that increases in performance as labelled examples increase (larger and larger <code class="highlighter-rouge">$K$</code> and different numbers of <code class="highlighter-rouge">$K$</code> per class). Methods such as <strong>Matching networks</strong> use an (Bi-)LSTM to ‘read’ the entirety of the query-set.
<p>A simpler, more intuitive method is the idea of <em>prototyping</em>. This idea is similar to manifold learning, where a <em>prototype</em> <code class="highlighter-rouge">$v_c$</code> is the <strong>average</strong> of all examples of the same class <code class="highlighter-rouge">$S_c$</code> in some latent space:
<code class="highlighter-rouge">$$v_{c}=\frac{1}{\left|S_{c}\right|} \sum_{\left(x_{i}, y_{i}\right) \in S_{c}} f_{\theta}\left(x_{i}\right)$$</code>
In the animation below, we see how a <strong>prototyping network</strong> works, with red squares on the latent map (right) representing prototypes for the three different classes.
<center><img src="https://i.filmot.com/H4wVk7g.png" alt="drawing" width="700" /></center>
<p>At test-time, the distribution over classes for a given test input <code class="highlighter-rouge">$x$</code> is a softmax over the inverse of distances between the test data embedding and prototype vectors:
<p><code class="highlighter-rouge">$$p(y=c \mid x)=\operatorname{softmax}\left(-d_{\varphi}\left(f_{\theta}(x), v_{c}\right)\right)$$</code>
<p>This is slightly more general than just picking the argmin over distances between prototypes and the query image’s latent embedding. Since the distance function <code class="highlighter-rouge">$d_\psi$</code> can be another neural network with learnt weights.
</article>
<aside class="related">
<h2>Related posts</h2>
<ul class="related-posts">
</aside>
           <footer>
    <span>© 2020. Powered by Jekyll & Dactl
</span>
    <span>written by Tom Lu</span>
</footer>
       </div>
<script type="text/javascript" src="/assets/js/theme.js"></script>
<script type="text/javascript" src="/assets/js/barefoot.js"></script>
<script src="//yihui.org/js/math-code.js"></script>
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
