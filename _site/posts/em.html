<!DOCTYPE html>
<html lang="en">
   <head>
       <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, user-scalable=no">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<title>Expectation Maximisation Deep Dive &#9642; Lu Blog</title>
<!--
<meta name="description" content="(45min) This post covers the in-depth theory regarding the EM algorithm, with python implementation for a Gaussian Mixture Model from scratch.
">
-->
<meta name="description" content="© 2020. Powered by Jekyll & Dactl
">
<meta name="keywords" content="theory, bayesian">
<link rel="canonical" href="http://localhost:4000/posts/em">
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Expectation Maximisation Deep Dive" />
<meta name="twitter:description" content="© 2020. Powered by Jekyll & Dactl
" />
<meta name="twitter:image" content="http://localhost:4000" />
<meta name="author" content="">
<link rel="author" href="">
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Expectation Maximisation Deep Dive">
<meta property="og:description" content="© 2020. Powered by Jekyll & Dactl
">
<meta property="og:url" content="http://localhost:4000/posts/em">
<meta property="og:site_name" content="Lu Blog">
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source Serif Pro:400,400italic,700,700italic">
<style>
    html {
      font-family: "Source Serif Pro", -apple-system, BlinkMacSystemFont, "Helvetica Neue", sans-serif;
    }
</style>
<link rel="stylesheet" href="/assets/css/bf.css">
   <body>
       <div class="wrapper" id="blep">
          <header>
   <div class="menu">
     <div class="logo">
        <a href="/">Lu Blog</a>
        <object type="image/svg+xml" data="" class="logosvg">Your browser does not support svg images</object>
     </div>
       <ul>
           <li><a href="/">home</a>
           <li><a href="/about">about</a>
           <li><a href="/archive">archive</a>
       </ul>
   </div>
   <div class="social">
     <ul>
       <li>
            <a href="https://www.linkedin.com/in/tomlu97" target="_blank" class="smaller">
              <span class="icon-stackoverflow"></span>
            </a>
       <li>
            <a href="https://github.com/xl402" target="_blank" class="smaller">
              <span class="icon-github"></span>
            </a>
       <li>
            <a href="#" onclick="switchTheme()" title="Switch theme?">
              <span class="icon-invert_colors" id="theme-switcher"></span>
            </a>
     </ul>
   </div>
</header>
<article class="post">
<style>
  .em {
    margin-bottom: 2em;
  }
  .em::before {
    background-image: url('https://filmot.com/5zMQ7ys.jpg');
    background-size: cover;
    -webkit-filter: grayscale(1) brightness(0.5) contrast(0.5);
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    z-index: -2;
  }
  .em::after {
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    background-color: rgba(255,0,100,0.8);
    /*mix-blend-mode: darken;
    mix-blend-mode: color-burn;
    mix-blend-mode: hard-light;*/
    mix-blend-mode: overlay;
    z-index: -1;
  }
  .em-container,
  .em-container::before,
  .em-container::after {
    height: 25rem;
  }
  .em-bleed-container,
  .em-bleed-container::before,
  .em-bleed-container::after {
    height: 25rem;
  }
    @media (max-width: 48rem) {
        .em-container,
        .em-container::before,
        .em-container::after{
            height: 20rem;
            margin-bottom: 3rem;
        }
    }
</style>
 <div class="post-title-container
  em em-bleed-container bleed-hero-container
    ">
    <!--Post hero image source-->
   <div class="heading-container hero-heading hero-heading-post">
     <h1>
          Expectation Maximisation Deep Dive
     </h1>
     <div class="post-meta">
        <span>12/06/2020</span>
        <span>
              <a href="/tag/theory">#theory</a>
              <a href="/tag/bayesian">#bayesian</a>
        </span>
     </div>
   </div>
 </div>
   <p class="lead">(45min) This post covers the in-depth theory regarding the EM algorithm, with python implementation for a Gaussian Mixture Model from scratch.
<p>
<ol>
 <li><a href="#introduction">Introduction</a>
 <li><a href="#theory-of-em">Theory of EM</a>
   <ul>
     <li>2.1 <a href="#what-is-it-trying-to-solve?">What is it trying to solve?</a>
     <li>2.2 <a href="#2.2">Graphical representation of the solution</a>
     <li>2.3 <a href="#2.3">Detailed derivation</a>
   </ul>
 <li><a href="#3">EM in Practice: Gaussian Mixture Model</a>
   <ul>
     <li>3.1 <a href="#3.1">Derivation</a>
     <li>3.2 <a href="#3.2">Implementation</a>
   </ul>
</ol>
<h1 id="1-introduction-">1. Introduction <a name="introduction"></a></h1>
<p>Expectation maximisation (EM) is one of those things that I was taught several times at University, each time it was explained slightly differently, and I was confused by it every single time. The confusion is mostly because we never implemented any of the EM algorithms through Python, and EMs were always introduced to solve a <strong>specific</strong> problem.
<p>Without a doubt, EM serves as a foundation for understanding variational inference. Here, I will try to go in-depth and derive and <em>E</em> and <em>M</em> steps under a <strong>general</strong> framework. Later I will derive and implement a <em>Gaussian Mixture Model</em> using EM.
<p>Maths behind this post are based on Coursera’s <a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning"><em>Bayesian Methods for Machine Learning</em></a> specialisation course. Intermediate-level probability theory and ML knowledge are assumed. Knowledge of the <a href="https://www.youtube.com/watch?v=ErfnhcEV1O8"><em>Kullback–Leibler</em></a> (KL) divergence and <a href="https://www.youtube.com/watch?v=HfCb1K4Nr8M"><em>Jensen’s inequality</em></a>  are also expected (link to videos explaining the concepts).
<h1 id="2-theory-of-em-">2. Theory of EM <a name="theory-of-em"></a></h1>
<h2 id="21-what-is-it-trying-to-solve-">2.1 What is it trying to solve? <a name="what-is-it-trying-to-solve?"></a></h2>
<p>EM aims to tackle a family of models termed <strong>latent variable models</strong> (examples like GMM, LDA, etc). Their graphical representation can be compactly represented as:
<center><img src="https://i.filmot.com/U6RLrrJ.png" alt="drawing" width="350" /></center>
<p>
<p>where <code class="highlighter-rouge">$x_i$</code> is our data-points in the dataset <code class="highlighter-rouge">$X = \{x_0, \cdots x_N\}$</code> and <code class="highlighter-rouge">$z_i$</code> is the latent variables associated with each datapoint. The edge represents that <code class="highlighter-rouge">$x_i$</code> is conditioned by <code class="highlighter-rouge">$z_i$</code>. Albeit this graphical model looks simple, to fully specify the model, we need to:
<ul>
 <li>Specify prior distribution of <code class="highlighter-rouge">$z_i$</code>
 <li>Specify the conditional distribution <code class="highlighter-rouge">$p(x_i|z_i)$</code>
</ul>
<p>both these two distributions can be arbitrarily complex (which is a problem during both model fitting and inference). If we <strong>summarise</strong> all the model parameters associated with our problem to be <code class="highlighter-rouge">$\theta$</code>, our goal becomes finding a set of parameters <code class="highlighter-rouge">$\theta$</code> which <strong>maximises the likelihood</strong> of our dataset <code class="highlighter-rouge">$X$</code> given our model:
<code class="highlighter-rouge">$$
\max_{\theta}p(X|\theta)
$$</code>
We assume our datapoints are <em>i.i.d</em> under our model, therefore:
<code class="highlighter-rouge">$$
\max_{\theta}p(X|\theta) = \max_\theta \prod p(x_i|\theta)
$$</code>
simplifying the product terms with a sum by maximising the log-likelihood:
<code class="highlighter-rouge">$$
\max_{\theta}\log p(X|\theta) = \max_\theta \sum_i \log p(x_i|\theta)
$$</code>
now we explicitly add in the conditional distribution using the rule of marginalisation:
<code class="highlighter-rouge">$$
\max_{\theta}\sum_i \log\left(\sum_c p(x_i, z_i=c|\theta)\right)
$$</code>
where for simplicity, we assume <strong>discrete distribution</strong> for <code class="highlighter-rouge">$z_i$</code> (derivation holds if you replace the sum with an integral). The support for <code class="highlighter-rouge">$z_i$</code> is <code class="highlighter-rouge">$[0, 1, \cdots, c]$</code>, i.e. it is categorical.
<p>That’s it! EM tries to find <code class="highlighter-rouge">$\theta$</code> that maximises the likelihood of our data, and through some maths we have shown:
<p class="notice"><code class="highlighter-rouge">$$
\max _{\theta} p(X | \theta) = \max _{\theta} \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)
$$</code>
<p>Of course, one question is why can’t we directly differentiate the (log) likelihood? As we will see later with GMM, we cannot obtain an analytical equation for <code class="highlighter-rouge">$\theta$</code> via direct differentiation!
<h2 id="22-graphical-representation-of-the-solution-">2.2 Graphical representation of the solution <a name="2.2"></a></h2>
<p>Here we jump straight into the solution of EM and obtain a graphical representation of the iterative update rule.
<p>We wish to find a set of parameters <code class="highlighter-rouge">$\theta$</code> which maximises the likelihood <code class="highlighter-rouge">$\log(p(X|\theta))$</code>. This distribution can be arbitrarily complex, therefore at each timestep, we approximate this likelihood using a simpler distribution that forms a <em>lower bound</em> <code class="highlighter-rouge">$\mathcal{L}(q, \theta)$</code>, which is <strong>below the likelihood</strong> at all <code class="highlighter-rouge">$\theta$</code> values. At timestep <code class="highlighter-rouge">$k$</code>, the EM update rule are as follows:
<ul>
 <li><strong>E</strong>-step: fix <code class="highlighter-rouge">$\theta$</code>, find some arbitrary <em>variational distribution</em> <code class="highlighter-rouge">$q(z_i)$</code> which maximises the lower-bound, i.e. <code class="highlighter-rouge">$q^{k+1} = \arg\max_{q}\mathcal{L}(\theta^k, q)$</code>.
 <li><strong>M</strong>-step: fix <code class="highlighter-rouge">$q$</code>, find model parameters <code class="highlighter-rouge">$\theta$</code> that maximises our current lower-bound, i.e. <code class="highlighter-rouge">$\theta^{k+1} = \arg\max_\theta \mathcal{L}(\theta, q^{k+1})$</code>
</ul>
<p>One thing to get our head around is that in <strong>E</strong>-step, we are essentially taking derivative w.r.t. a <strong>function</strong> <code class="highlighter-rouge">$q(z_i)$</code>. These two steps can be seen more clearly in the picture below.
<p>
<center><img src="https://i.filmot.com/8egXBCs.gif" alt="drawing" width="600" /></center>
<p>Derivation later will show exactly how we have arrived at <strong>E</strong> and <strong>M</strong> steps, but jumping ahead, these two steps are equivalent to:
<div class="notice">
 <ul>
   <li><strong>E</strong>-step: set <em>variational distribution</em> <code class="highlighter-rouge">$q(z_i)$</code> to be equal (or approximately equal) to the posterior distribution, i.e. <code class="highlighter-rouge">$q^k(z_i) = p(z_i|\theta_i^k, x_i)$</code> where <code class="highlighter-rouge">$k$</code> denotes the timestep in our iteration.
   <li><strong>M</strong>-step: under our distribution <code class="highlighter-rouge">$q(z_i)$</code>, find a set of parameters <code class="highlighter-rouge">$\theta$</code> which maximises the data likelihood, i.e. <code class="highlighter-rouge">$\theta^{k} = \arg\max_\theta \sum_i \mathbb{E}_{z_i\sim q^k}\left[\log p(x_i, z_i | \theta)\right]$</code>
 </ul>
</div>
<p>
<p>Hence EM algorithm boils down to the fact that we cannot directly maximise our likelihood function, hence we maximise its lower-bound under our own <strong>variational distribution</strong> which is a simple function. At each timestep, we approximate or set this function to the posterior distribution <code class="highlighter-rouge">$p(Z|X,\theta)$</code> (<strong>E</strong>-step), and we maximises the likelihood of our data-points under this function instead (<strong>M</strong>-step). Alternating these two procedures, we are guaranteed to reach a <strong>local optima</strong> (in the picture above, depending on initialisation, we could have easily ended up on the left peak as our final solution).
<h2 id="23-detailed-derivation-">2.3 Detailed Derivation <a name="2.3"></a></h2>
<h3 id="e-step">E-Step</h3>
<p class="notice"><strong>TL;DR</strong>: <code class="highlighter-rouge">$\;\arg\max _{q\left(z_{i}\right)} \mathcal{L}\left(\theta, q\right)=p\left(z_{i} | x_{i}, \theta\right)$</code>, to minimise the gap between our lower-bound and the likelihood under current parameters <code class="highlighter-rouge">$\theta$</code>, set our variational distribution to be the posterior.
<p><strong>Proof</strong>: first, lets define how exactly a lower-bound arises and where on earth this <strong>variational distribution</strong> <code class="highlighter-rouge">$q$</code> comes from. Decompose the likelihood:
<code class="highlighter-rouge">$$\log p(X | \theta) = \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)$$</code>
Idea here is we cannot directly maximise this equation (if you can, then do not use EM ;)), instead, we multiply top and bottom by a distribution of our choice <code class="highlighter-rouge">$q$</code> (coz why not?):
<code class="highlighter-rouge">$$\log p(X | \theta) = \sum_{i} \log\left( \sum_{c} q\left(z_{i}=c\right) \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)$$</code>
which is equivalent to:
<code class="highlighter-rouge">$$\log p(X|\theta) = \sum_{i} \log \mathbb{E}_{z_{i} \sim q} \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}$$</code>
now we can utilise Jensen’s inequality to obtain a lower bound for this equality. As a reminder, for any concave function (such as <code class="highlighter-rouge">$\log(x)$</code> in our case), <code class="highlighter-rouge">$f (\mathbb{E}[x]) \geqslant \mathbb{E}[f (x)]$</code>. We can therefore write the equation above as:
<code class="highlighter-rouge">$$
\log p(X|\theta)\geq \sum_{i} \mathbb{E}_{z_{i}\sim q} \log \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}
$$</code>
expanding the expectation out:
<code class="highlighter-rouge">$$
\log p(X|\theta)\geq\sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} = \mathcal{L}(\theta, q)
$$</code>
Our objective has the become <strong>minimising the gap</strong> between this lower-bound by optimising our arbitrary distribution <code class="highlighter-rouge">$q$</code>:
<code class="highlighter-rouge">$$
\arg\min_q \text{Objective} = \arg\min_q \left( \log p(X|\theta) - \mathcal{L}(\theta, q)\right)
$$</code>
more maths:
<code class="highlighter-rouge">$$
\text{Objective} = \sum_{i} \log p\left(x_{i} | \theta\right) - \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$</code>
for the first term, we multiply it with <code class="highlighter-rouge">$1 = \sum_c q(z_i=c)$</code>:
<code class="highlighter-rouge">$$
 =\sum_{i} \left(\log p\left(x_{i} | \theta\right)\sum_{c} q\left(z_{i}=c\right)\right) - \sum_{i} \sum_{c} \cdots
$$</code>
now we can move both summations out:
<code class="highlighter-rouge">$$
=\sum_{i}\sum_{c}\left(\log p\left(x_{i} | \theta\right) q\left(z_{i}=c\right) - q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} \right)
$$</code>
<code class="highlighter-rouge">$$
 =\sum_{i} \sum_{c} \left(q\left(z_{i}=c\right) \left(\log p\left(x_{i} | \theta\right) - \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)\right)
$$</code>
<code class="highlighter-rouge">$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p\left(x_{i}, z_{i}=c | \theta\right)}\right)\right)\right)
$$</code>
re-write the joint distribution of the denominator inside the log:
<code class="highlighter-rouge">$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p(x_i|\theta)p(z_i=c|\theta, x_i)}\right)\right)\right)
$$</code>
cancelling out the <code class="highlighter-rouge">$p\left(x_{i} | \theta\right)$</code> we obtain:
<code class="highlighter-rouge">$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\log \left(\frac{ q\left(z_{i}=c\right)}{ p\left(z_{i}=c | \theta, x_{i}\right)}\right)\right)
$$</code>
observe that the summation under <code class="highlighter-rouge">$c$</code> term corresponds to the KL divergence between <code class="highlighter-rouge">$q$</code> and <code class="highlighter-rouge">$p$</code>, we therefore obtain:
<code class="highlighter-rouge">$$
\text{Objective} = \text{KL}(q(z_i)|| p(z_i|\theta, x_i))
$$</code>
after all this faff, we see to minimise the gap (objective) between the likelihood and our lower-bound is equivalent to:
<code class="highlighter-rouge">$$
\arg \min _{q}(\log p(X | \theta)-\mathcal{L}(\theta, q)) = \arg \min _{q}\mathrm{KL}\left(q\left(z_{i}\right) \| p\left(z_{i} | \theta, x_{i}\right)\right)
$$</code>
so effectively, <strong>E</strong>-step becomes an optimisation problem with the objective function being the KL divergence between variational distribution <code class="highlighter-rouge">$q$</code> w.r.t. model posterior <code class="highlighter-rouge">$p\left(z_{i} | \theta, x_{i}\right)$</code>. There is <strong>no guarantee</strong> that the posterior has a closed-form representation. This is where the rich literature of <strong>variational inference</strong> comes in. Fortunately, with a GMM (Gaussian mixture model), the posterior has a closed form solution! Hence at each iteration, we are able to set <code class="highlighter-rouge">$q$</code> to be equal to the posterior.
<h3 id="m-step">M-Step</h3>
<p class="notice"><strong>TL;DR</strong> given a fixed variational distribution <code class="highlighter-rouge">$q$</code>, maximise the data and latent variable’s joint likelihood w.r.t. model parameters <code class="highlighter-rouge">$\theta$</code>, i.e. <code class="highlighter-rouge">$\arg\max_\theta \mathcal{L}(\theta, q) = \arg \max _{\theta} \mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]$</code> where latent variables <strong>come from our variational ditribution</strong> <code class="highlighter-rouge">$q$</code>.
<p>Recall from <strong>E</strong>-step, maximising the lower-bound is essentially maximising the likelihood w.r.t. <code class="highlighter-rouge">$\theta$</code>, the only difference is <strong>we can easily perform this maximisation</strong> since we know our distribution <code class="highlighter-rouge">$q$</code> (normally it is a Gaussian distribution). Writing the whole expectation term out:
<code class="highlighter-rouge">$$
\arg\max_\theta \mathcal{L}(\theta, q) = \arg\max_\theta \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$</code>
splitting the terms within the log:
<code class="highlighter-rouge">$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right) \left(\log p\left(x_{i}, z_{i}=c | \theta\right) - \log q\left(z_{i}=c\right)\right)
$$</code>
the last term is independent of <code class="highlighter-rouge">$\theta$</code>:
<code class="highlighter-rouge">$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right)\log p\left(x_{i}, z_{i}=c | \theta\right) + \text{const}.
$$</code>
condensing the sum over <code class="highlighter-rouge">$c$</code> into expectation, dropping the constant:
<code class="highlighter-rouge">$$
\arg \max _{\theta}\sum_{i}\mathbb{E}_{z_i}\left[ \log p(x_i,z_i=c|\theta)\right] = \mathbb{E}_{Z\sim q}\left[ \log p(X,Z|\theta)\right]
$$</code>
So the <strong>M</strong>-step is simply maximising the joint distribution between collection of latent variables <code class="highlighter-rouge">$Z$</code> and data points <code class="highlighter-rouge">$X$</code> <strong>under the variational distribution</strong> <code class="highlighter-rouge">$q$</code>:
<code class="highlighter-rouge">$$
\hat{\theta} = \arg \max _{\theta}\mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]
$$</code>
Again, we definitely know what <code class="highlighter-rouge">$p(X, Z | \theta)$</code> is, since we needed it to set up our graphical model. However, maximisation w.r.t. <code class="highlighter-rouge">$\theta$</code> can be non-trivial. In the case of GMM, with a lot of maths, we can obtain closed form solutions for model parameters <code class="highlighter-rouge">$\theta$</code>.
<h1 id="3-em-in-practice-gaussian-mixture-model-">3. EM in Practice: Gaussian Mixture Model <a name="3"></a></h1>
<p>Here we unleash the EM algorithm on Gaussian Mixture Model (GMM), For a quick intuition on how GMM update rules work, here is a link to <a href="https://www.youtube.com/watch?v=REypj2sy_5U&amp;t=329s"><em>StatQuest’s</em></a> video. I have to say, update rule for GMM is fairly intuitive, and videos like this is a tiny bit misleading. As it is not easy to shoehorn the general EM algorithm into this sort of intuition. Instead, we will not skip (too much) on the maths and will derive exactly how GMM update rules arise from EM.
<h2 id="31-derivation-">3.1 Derivation <a name="3.1"></a></h2>
<h3 id="model-definition">Model Definition</h3>
<p>Remember the graph earlier?
<center><img src="https://i.filmot.com/U6RLrrJ.png" alt="drawing" width="350" /></center>
<p>
<p>Here we define exactly what the model parameters are for a GMM. First let’s look at what GMM is trying to achieve:
<center><img src="https://i.filmot.com/AGe3avl.png" alt="drawing" width="600" /></center>
<p>Left figure above is the data points that you have obtained, through some prior knowledge or by inspecting the plots, <strong>you believe</strong> that the data points come from <strong>three Gaussian distributions</strong>, and you wish to fit the Gaussian centres and variances to the data points (shown on the right).
<p>To apply latent variable modelling, we think about “how do we <strong>generate</strong> these data points with a probabilistic model?”. Let <code class="highlighter-rouge">$z_i$</code> follow a categorical distribution (three categories) which define the proportion of data points that belong to each cluster. When <strong>conditioned</strong> on the categories, the data point <code class="highlighter-rouge">$x_i$</code> follows a gaussian distribution. Model parameters are therefore <code class="highlighter-rouge">$\theta = [\pi_1, \cdots, \pi_c, \mu_1, \cdots, \mu_c, \sigma^2_1, \cdots \sigma^2_c]$</code>, which are the distribution over categories, mean and standard deviaiton of each cluster centre. We restrict ourselves to fitting <strong>isotropic</strong> Gaussians, although in practice, you can find the MLE covariance matrix (maths will of course be more disgusting).
<center><img src="https://i.filmot.com/1Mb2T0E.png" alt="drawing" width="600" /></center>
<p>Given this mode, we can write down the <strong>likelihood of our data points</strong> under the model:
<p><code class="highlighter-rouge">$$
p(x_i |\theta) = \sum_c p(x_i|z_i=c, \theta)p(z_i=c|\theta) = \sum_c p(x_i|z_i=c, \theta) \pi_c
$$</code>
<code class="highlighter-rouge">$$
p(X |\theta) = \prod_i \sum_c p(x_i|z_i=c, \theta) \pi_c= \prod_i \sum_c \mathcal{N}(x_i;\mu_c, \sigma^2_c) \pi_c
$$</code>
This completes our model definition for a GMM. <strong>Why don’t we directly maximise the likelihood w.r.t. model parameters?</strong> Let’s see:
<code class="highlighter-rouge">$$
\log p(X |\theta) = \sum_i \log \sum_{c} \mathcal{N}\left(x_{i} ; \mu_{c}, \sigma_{c}^{2}\right) \pi_{c}
$$</code>
<code class="highlighter-rouge">$$
\nabla_\theta \log p(X|\theta) = \sum_i  \nabla_\theta \log\left( \sum_{c} \mathcal{N}\left(x_{i} ; \mu_{c}, \sigma_{c}^{2}\right) \pi_{c}\right)
$$</code>
And we are stuck… good luck dealing with the log of sum of exponential term above. This is why EM is used to solve GMMs.
<h3 id="e-step-1">E-Step</h3>
<p>Recall in EM, the <strong>E</strong>-step is equivalent to setting our variational distribution <code class="highlighter-rouge">$q(z_i)$</code> to be equal or approximately equal to the posterior <code class="highlighter-rouge">$p(z_i|x_i,\theta)$</code>. In the case of GMM we can obtain the analytical posterior. Applying Bayes’ rule:
<code class="highlighter-rouge">$$
q(z_i=c) = p(z_i=c|x_i,\theta) = \dfrac{p(x_i|z_i=c,\theta)p(z_i=c|\theta)}{\sum_k p(x_i|z_i=k,\theta)p(z_i=k|\theta)}
$$</code>
Don’t be confused by the index <code class="highlighter-rouge">$k$</code> in the denominator, we are just marginalising over all cluster centre categories (in our example, 1 to 3). In <strong>E-step</strong> we <strong>fix</strong> all model parameters, the posterior above therefore takes the form:
<code class="highlighter-rouge">$$
q(z_i=c) = \dfrac{\mathcal{N}(x_i|\mu_c,\sigma^2_c)\pi_c}{\sum_k \mathcal{N}(x_i|\mu_k,\sigma^2_k)\pi_k}
$$</code>
<p class="notice">This result is fairly intuitive -
you are effectively asking: what is the chance of this data point belonging to cluster <code class="highlighter-rouge">$c$</code>, given the current cluster centres and deviations.
<p>For each data point, we do a soft (probabilistic) assignment on which cluster it comes from. Which turns out to be the probability of it <strong>being generated</strong> by cluster <code class="highlighter-rouge">$c$</code> (numerator), divided by the sum of probabilities of it being generated by all clusters (denominator).
<h3 id="m-step-1">M-Step</h3>
<p>Recall in <strong>M</strong>-step, we maximise the joint probability distribution under our own variational distribution <code class="highlighter-rouge">$q(z_i)$</code>. I will just copy-paste from the previous section:
<code class="highlighter-rouge">$$
\theta^* = \arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log p\left(x_{i}, z_{i}=c | \theta\right) = \arg\max_\theta \mathcal{G}
$$</code>
where <code class="highlighter-rouge">$\mathcal{G}$</code> is just a short hand for the joint density, and <code class="highlighter-rouge">$\theta$</code> are the mixture proportion <code class="highlighter-rouge">$\pi_c$</code> terms, the Gaussian centres <code class="highlighter-rouge">$\mu_c$</code> and <code class="highlighter-rouge">$\sigma^2_c$</code> terms.
<p>Note that here, <code class="highlighter-rouge">$q(z_i)$</code> is fixed, even though from the <strong>E</strong>-step they depend on our parameters. Writing <code class="highlighter-rouge">$\mathcal{G}$</code> out fully:
<p><code class="highlighter-rouge">$$
\mathcal{G} = \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log\left(\dfrac{1}{\sigma_c \sqrt{2\pi}}\exp\left(\dfrac{-(x_i-\mu_c)^2}{2\sigma_c^2}\right)\pi_c\right)
$$</code>
<code class="highlighter-rouge">$$
= \sum_{i} \sum_{c} q\left(z_{i}=c\right)  \left(\log \dfrac{\pi_c}{\sigma_c\sqrt{2\pi}} - \dfrac{(x_i-\mu_c)^2}{2\sigma_c^2}\right)
$$</code>
This is all we need to obtain the analytical gradient with respect to model parameters <code class="highlighter-rouge">$\theta$</code>. For example, to get <code class="highlighter-rouge">$\mu_k$</code> (<code class="highlighter-rouge">$k$</code>th cluster mean):
<code class="highlighter-rouge">$$
\nabla_{\mu_k}\mathcal{G} = \sum_i q(z_i=k) \dfrac{x_i-\mu_k}{\sigma_c^2}
$$</code>
since for all <code class="highlighter-rouge">$c\neq k$</code> the gradient is zero. Setting above to zero (solve for optimum <code class="highlighter-rouge">$\mu_k$</code>):
<code class="highlighter-rouge">$$
\mu_k = \dfrac{\sum_i q(z_i=k)x_i}{\sum_i q(z_i=k)}
$$</code>
The same can be done to <code class="highlighter-rouge">$\sigma_k^2$</code> (via high school calculus) to obtain:
<code class="highlighter-rouge">$$
\sigma_k^2 =\frac{\sum_{i}\left(x_{i}-\mu_{k}\right)^{2} q\left(z_{i}=k\right)}{\sum_{i} q\left(z_{i}=k\right)}
$$</code>
To get optimum mixing proportions <code class="highlighter-rouge">$\pi_k$</code> is more complicated, since we need to do <strong>constrained optimisation</strong> with <code class="highlighter-rouge">$\pi_k \geq 0$</code> and <code class="highlighter-rouge">$\sum_c \pi_c=1$</code> since it needs to be a probability distribution. This can be done through <a href="https://www.youtube.com/watch?v=yuqB-d5MjZA">Lagrange multipliers</a>. Here we actually omit the maths and jump straight into the result:
<code class="highlighter-rouge">$$
\pi_k = \dfrac{\sum_i q(z_i=k)}{N}
$$</code>
where <code class="highlighter-rouge">$N$</code> is the total number of data points. The three equations above should all be fairly intuitive, the common theme is to <strong>re-estimate model parameters with our ‘self-labelled’ data points</strong> (labelled by our own variational distribution <code class="highlighter-rouge">$q$</code>). One could have jumped straight into these formula (I couldn’t, but I’m sure some mathmos can). But here, we followed the EM algorithm in order to <strong>maximise likelihood of data points under our model</strong>.
<h2 id="32-implementation-">3.2 Implementation <a name="3.2"></a></h2>
<p>Of course, any sane person would call <code class="highlighter-rouge">sklearn.mixture.GaussianMixture</code> and just do <code class="highlighter-rouge">GMM.fit(X)</code>. But that is way too efficient. We start with some standard import:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1"># bit confusing, one is used to draw from, the other is used for evaluation
</span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">multivariate_normal</span> <span class="k">as</span> <span class="n">mv_n</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">numpy.testing</span> <span class="kn">import</span> <span class="n">assert_array_almost_equal</span></code></pre></figure>
<h3 id="321-dataground-truth-generation">3.2.1 Data/Ground Truth Generation</h3>
<p>First, let’s generate our dataset. Define the ground truth parameters that we wish to estimate:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">N</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># total number of data points
</span><span class="err">μ</span><span class="mi">1</span><span class="p">,</span> <span class="err">μ</span><span class="mi">2</span><span class="p">,</span> <span class="err">μ</span><span class="mi">3</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span> <span class="c1"># cluster mean d = 2
# cluster standard deviation
</span><span class="err">σ</span><span class="mi">1</span><span class="p">,</span> <span class="err">σ</span><span class="mi">2</span><span class="p">,</span> <span class="err">σ</span><span class="mi">3</span> <span class="o">=</span> <span class="mf">.05</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mf">.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mf">.15</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="err">π</span><span class="mi">1</span><span class="p">,</span> <span class="err">π</span><span class="mi">2</span><span class="p">,</span> <span class="err">π</span><span class="mi">3</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span> <span class="c1"># mixing proportions
# needs to be a valid categorical distribution
</span><span class="k">assert</span> <span class="nb">sum</span><span class="p">([</span><span class="err">π</span><span class="mi">1</span><span class="p">,</span> <span class="err">π</span><span class="mi">2</span><span class="p">,</span> <span class="err">π</span><span class="mi">3</span><span class="p">])</span><span class="o">==</span><span class="mi">1</span></code></pre></figure>
<p>The dataset is drawn according to our model:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="err">μ</span><span class="n">s</span><span class="p">,</span> <span class="err">σ</span><span class="n">s</span><span class="p">,</span> <span class="err">π</span><span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="err">μ</span><span class="mi">1</span><span class="p">,</span> <span class="err">μ</span><span class="mi">2</span><span class="p">,</span> <span class="err">μ</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="err">σ</span><span class="mi">1</span><span class="p">,</span> <span class="err">σ</span><span class="mi">2</span><span class="p">,</span> <span class="err">σ</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="err">π</span><span class="mi">1</span><span class="p">,</span> <span class="err">π</span><span class="mi">2</span><span class="p">,</span> <span class="err">π</span><span class="mi">3</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">normal</span><span class="p">(</span><span class="err">μ</span><span class="p">,</span> <span class="err">σ</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span><span class="o">*</span><span class="err">π</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span> <span class="k">for</span> <span class="err">μ</span><span class="p">,</span> <span class="err">σ</span><span class="p">,</span> <span class="err">π</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="err">μ</span><span class="n">s</span><span class="p">,</span> <span class="err">σ</span><span class="n">s</span><span class="p">,</span> <span class="err">π</span><span class="n">s</span><span class="p">)]</span>
<span class="c1"># Reshape to our N x d dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span> <span class="c1"># visualize data generated</span></code></pre></figure>
<p>we observe the dataset at the beginning of the section (three fairly distinct clusters).
<p>We also need to initialise guesses for our model parameters. Here we sample from a normal distribution for the cluster centres, and guess identity covariance matrix and uniform mixing proportion:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="err">μ</span><span class="n">s</span><span class="p">,</span> <span class="err">σ</span><span class="n">s</span><span class="p">,</span> <span class="err">π</span><span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="err">μ</span><span class="n">s</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="err">σ</span><span class="n">s</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="err">π</span><span class="n">s</span><span class="p">)</span>
<span class="err">μ</span><span class="mi">0</span><span class="p">,</span> <span class="err">σ</span><span class="mi">0</span><span class="p">,</span> <span class="err">π</span><span class="mi">0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="err">μ</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span><span class="o">*</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="err">π</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span></code></pre></figure>
<h3 id="322-e-step">3.2.2 E-Step</h3>
<p><strong>E</strong>-step update rule is reproduced here for convenience:
<code class="highlighter-rouge">$$
q\left(z_{i}=c\right)=\frac{\mathcal{N}\left(x_{i} | \mu_{c}, \sigma_{c}^{2}\right) \pi_{c}}{\sum_{k} \mathcal{N}\left(x_{i} | \mu_{k}, \sigma_{k}^{2}\right) \pi_{k}}
$$</code>
the corresponding code is simply:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">e_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="err">μ</span><span class="n">s</span><span class="p">,</span> <span class="err">σ</span><span class="n">s</span><span class="p">,</span> <span class="err">π</span><span class="n">s</span><span class="p">):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="err">π</span><span class="n">s</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span> <span class="c1"># q(z)
</span>    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span> <span class="c1"># p(x|z)
</span>    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
        <span class="c1"># density function evaluation at our data points
</span>        <span class="n">p</span><span class="p">[:,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="err">μ</span><span class="n">s</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="err">σ</span><span class="n">s</span><span class="p">[</span><span class="n">c</span><span class="p">])</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">*=</span> <span class="err">π</span><span class="n">s</span> <span class="c1"># weight by mixing proportion
</span>    <span class="n">q</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q</span></code></pre></figure>
<p>As a sanity check, under the variational distribution, likelihood of each data point for all clusters should sum to one:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">q</span> <span class="o">=</span> <span class="n">e_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="err">μ</span><span class="mi">0</span><span class="p">,</span> <span class="err">σ</span><span class="mi">0</span><span class="p">,</span> <span class="err">π</span><span class="mi">0</span><span class="p">)</span>
<span class="n">assert_array_almost_equal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">)))</span></code></pre></figure>
<h3 id="323-m-step">3.2.3 M-Step</h3>
<p><strong>M</strong>-step update rule is copied here for all three classes of parameters:
<code class="highlighter-rouge">$$
\mu_{k}=\frac{\sum_{i} q\left(z_{i}=k\right) x_{i}}{\sum_{i} q\left(z_{i}=k\right)}
$$</code>
<code class="highlighter-rouge">$$
\sigma_{k}^{2}=\frac{\sum_{i}\left(x_{i}-\mu_{k}\right)^{T}(x_{i}-\mu_{k}) q\left(z_{i}=k\right)}{\sum_{i} q\left(z_{i}=k\right)}$$</code>
<code class="highlighter-rouge">$$\pi_{k}=\frac{\sum_{i} q\left(z_{i}=k\right)}{N}
$$</code>
Note we slightly modified covariance matrix update rule so it involves an outer product (realised I probably should have worked in vector form earlier, but oh well). The code becomes:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">m_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="err">μ</span><span class="n">s</span><span class="p">,</span> <span class="err">σ</span><span class="n">s</span><span class="p">,</span> <span class="err">π</span><span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">d</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
        <span class="n">q_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">q</span><span class="p">[:,</span> <span class="n">c</span><span class="p">])</span> <span class="c1"># q summed over all data points
</span>        <span class="c1"># update cluster mean
</span>        <span class="err">μ</span><span class="n">s</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">q</span><span class="p">[:,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_sum</span>
        <span class="c1"># update cluster std
</span>        <span class="err">σ</span><span class="n">s</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">q</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">-</span> <span class="err">μ</span><span class="n">s</span><span class="p">[</span><span class="n">c</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">-</span> <span class="err">μ</span><span class="n">s</span><span class="p">[</span><span class="n">c</span><span class="p">,:])</span>\
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_sum</span>
        <span class="c1"># update mixing proportions
</span>        <span class="err">π</span><span class="n">s</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">q_sum</span> <span class="o">/</span> <span class="n">N</span>
    <span class="k">return</span> <span class="err">μ</span><span class="n">s</span><span class="p">,</span> <span class="err">σ</span><span class="n">s</span><span class="p">,</span> <span class="err">π</span><span class="n">s</span></code></pre></figure>
<h3 id="324-wrapping-up">3.2.4 Wrapping Up</h3>
<p>In theory, we need to check for convergence by evaluation the variational lower bound. This is omitted here. Instead, we run simulation for 150 time steps and obtain:
<center><img src="https://i.filmot.com/M7VCEve.png" alt="drawing" width="1000" /></center>
<p>So as anticipated, our GMM indeed fitted three Gaussians onto the data points. Visually the fit is not bad. But how do the estimated parameters fair with the ground truth? We visualise the phase portrait for cluster means (2D) and variances within the covariance matrix (also 2D) across iterations. Plots below show the ground truth of model parameters in black, and the trajectories GMM generated across the iteration. We observe very good agreement between the predicted locations and ground truth!
<center><img src="https://i.filmot.com/M27QOCA.png" alt="drawing" width="1000" /></center>
<p>In summary, we have show how EM algorithm at heart does maximum likelihood estimation of the dataset being generated by a given model. Again it is worth reflecting on the fact that neither <strong>E</strong> or <strong>M</strong> step guarantees closed form solution. This sets up the motivation for future blog posts regarding variational inference and Monte Carlo based methods. Finally, this post wouldn’t be complete if I didn’t include a link to Carl Rasmussen’s <a href="http://mlg.eng.cam.ac.uk/teaching/4f13/1819/expectation%20maximization.pdf">lecture notes</a> on EM (it is super condensed, but good as a quick memory refresher). The <a href="https://www.youtube.com/watch?v=rVfZHWTwXSA&amp;t=4230s">best talk</a> I can find online is given by Andrew Ng, which explains both the overall concept really well and does not shy away from the derivations.
</article>
<aside class="related">
<h2>Related posts</h2>
<ul class="related-posts">
   <li>
    <a href="/posts/vi2">
        <span>Variational Inference Part 2: Black-Box VI</span>
        <small>05/07</small>
    </a>
   <li>
    <a href="/posts/vi1">
        <span>Variational Inference Part 1: Intro and Mean-Field</span>
        <small>16/06</small>
    </a>
</aside>
           <footer>
    <span>© 2020. Powered by Jekyll & Dactl
</span>
    <span>written by Tom Lu</span>
</footer>
       </div>
<script type="text/javascript" src="/assets/js/theme.js"></script>
<script type="text/javascript" src="/assets/js/barefoot.js"></script>
<script src="//yihui.org/js/math-code.js"></script>
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
