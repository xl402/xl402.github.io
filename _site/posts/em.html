<!DOCTYPE html>
<html lang="en">
   <head>
       <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, user-scalable=no">
<title>Expectation Maximisation Deep Dive &#9642; Lu Blog</title>
<!--
<meta name="description" content="This post covers the in-depth theory regarding the EM algorithm, with python implementation for a Gaussian Mixture Model from scratch.
">
-->
<meta name="description" content="© 2020. Powered by Jekyll & Dactl
">
<meta name="keywords" content="theory, bayesian">
<link rel="canonical" href="http://localhost:4000/posts/em">
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Expectation Maximisation Deep Dive" />
<meta name="twitter:description" content="© 2020. Powered by Jekyll & Dactl
" />
<meta name="twitter:image" content="http://localhost:4000" />
<meta name="author" content="">
<link rel="author" href="">
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Expectation Maximisation Deep Dive">
<meta property="og:description" content="© 2020. Powered by Jekyll & Dactl
">
<meta property="og:url" content="http://localhost:4000/posts/em">
<meta property="og:site_name" content="Lu Blog">
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Rubik:400,400italic,700,700italic">
<style>
    html {
      font-family: "Rubik", -apple-system, BlinkMacSystemFont, "Helvetica Neue", sans-serif;
    }
</style>
<link rel="stylesheet" href="/assets/css/bf.css">
   <body>
       <div class="wrapper" id="blep">
          <header>
   <div class="menu">
     <div class="logo">
        <a href="/">Lu Blog</a>
        <object type="image/svg+xml" data="" class="logosvg">Your browser does not support svg images</object>
     </div>
       <ul>
           <li><a href="/about">about</a>
           <li><a href="/archive">archive</a>
       </ul>
   </div>
   <div class="social">
     <ul>
       <li>
            <a href="https://www.linkedin.com/in/tomlu97" target="_blank" class="smaller">
              <span class="icon-stackoverflow"></span>
            </a>
       <li>
            <a href="https://github.com/xl402" target="_blank" class="smaller">
              <span class="icon-github"></span>
            </a>
       <li>
            <a href="#" onclick="switchTheme()" title="Switch theme?">
              <span class="icon-invert_colors" id="theme-switcher"></span>
            </a>
     </ul>
   </div>
</header>
<article class="post">
<style>
  .em {
    margin-bottom: 2em;
  }
  .em::before {
    background-image: url('https://imgur.com/5zMQ7ys.jpg');
    background-size: cover;
    -webkit-filter: grayscale(1) brightness(0.5) contrast(0.5);
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    z-index: -2;
  }
  .em::after {
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    background-color: rgba(255,0,100,0.8);
    /*mix-blend-mode: darken;
    mix-blend-mode: color-burn;
    mix-blend-mode: hard-light;*/
    mix-blend-mode: overlay;
    z-index: -1;
  }
  .em-container,
  .em-container::before,
  .em-container::after {
    height: 25rem;
  }
  .em-bleed-container,
  .em-bleed-container::before,
  .em-bleed-container::after {
    height: 35rem;
  }
    @media (max-width: 48rem) {
        .em-container,
        .em-container::before,
        .em-container::after{
            height: 20rem;
            margin-bottom: 3rem;
        }
    }
</style>
 <div class="post-title-container
  em em-bleed-container bleed-hero-container
    ">
    <!--Post hero image source-->
   <div class="heading-container hero-heading hero-heading-post">
     <h1>
          Expectation Maximisation Deep Dive
     </h1>
     <div class="post-meta">
        <span>12/06/2020</span>
        <span>
              <a href="/tag/theory">#theory</a>
              <a href="/tag/bayesian">#bayesian</a>
        </span>
     </div>
   </div>
 </div>
   <p class="lead">This post covers the in-depth theory regarding the EM algorithm, with python implementation for a Gaussian Mixture Model from scratch.
<p>
<h1 id="1-introduction">1. Introduction</h1>
<p>Expectation maximisation (EM) is one of those things that I was taught several times at University, each time it was explained slightly differently, and I was confused by it every single time. The confusion is mostly because we never implemented any of the EM algorithms through Python, and EMs were always introduced to solve a <strong>specific</strong> problem.
<p>Without a doubt, EM serves as a foundation for understanding variational inference. Here, I will try to go in-depth and derive and <em>E</em> and <em>M</em> steps under a <strong>general</strong> framework. Later I will derive and implement a <em>Gaussian Mixture Model</em> using EM.
<p>This post is based on Coursera’s <a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning"><em>Bayesian Methods for Machine Learning</em></a> specialisation course. Intermediate-level probability theory and ML knowledge are assumed. Knowledge of the <a href="https://www.youtube.com/watch?v=ErfnhcEV1O8"><em>Kullback–Leibler</em></a> (KL) divergence and <a href="https://www.youtube.com/watch?v=HfCb1K4Nr8M"><em>Jensen’s inequality</em></a>  are also expected (link to videos explaining the concepts).
<h1 id="2-theory-of-em">2. Theory of EM</h1>
<h2 id="21-what-is-it-trying-to-solve">2.1 What is it trying to solve?</h2>
<p>EM aims to tackle a family of models termed <strong>latent variable models</strong> (examples like GMM, LDA, etc). Their graphical representation can be compactly represented as:
<center><img src="https://i.imgur.com/U6RLrrJ.png" alt="drawing" width="250" /></center>
<p>
<p>where <code class="highlighter-rouge">$x_i$</code> is our data-points in the dataset <code class="highlighter-rouge">$X = \{x_0, \cdots x_N\}$</code> and <code class="highlighter-rouge">$z_i$</code> is the latent variables associated with each datapoint. The edge represents that <code class="highlighter-rouge">$x_i$</code> is conditioned by <code class="highlighter-rouge">$z_i$</code>. Albeit this graphical model looks simple, to fully specify the model, we need to:
<ul>
 <li>Specify prior distribution of <code class="highlighter-rouge">$z_i$</code>
 <li>Specify the conditional distribution <code class="highlighter-rouge">$p(x_i|z_i)$</code>
</ul>
<p>both these two distributions can be arbitrarily complex (which is a problem during both model fitting and inference). If we <strong>summarise</strong> all the model parameters associated with our problem to be <code class="highlighter-rouge">$\theta$</code>, our goal becomes finding a set of parameters <code class="highlighter-rouge">$\theta$</code> which <strong>maximises the likelihood</strong> of our dataset <code class="highlighter-rouge">$X$</code> given our model:
<code class="highlighter-rouge">$$
\max_{\theta}p(X|\theta)
$$</code>
We assume our datapoints are <em>i.i.d</em> under our model, therefore:
<code class="highlighter-rouge">$$
\max_{\theta}p(X|\theta) = \max_\theta \prod p(x_i|\theta)
$$</code>
simplifying the product terms with a sum by maximising the log-likelihood:
<code class="highlighter-rouge">$$
\max_{\theta}\log p(X|\theta) = \max_\theta \sum_i \log p(x_i|\theta)
$$</code>
now we explicitly add in the conditional distribution using the rule of marginalisation:
<code class="highlighter-rouge">$$
\max_{\theta}\sum_i \log\left(\sum_c p(x_i, z_i=c|\theta)\right)
$$</code>
where for simplicity, we assume <strong>discrete distribution</strong> for <code class="highlighter-rouge">$z_i$</code> (derivation holds if you replace the sum with an integral). The support for <code class="highlighter-rouge">$z_i$</code> is <code class="highlighter-rouge">$[0, 1, \cdots, c]$</code>, i.e. it is categorical.
<p>That’s it! EM tries to find <code class="highlighter-rouge">$\theta$</code> that maximises the likelihood of our data, and through some maths we have shown:
<p class="notice"><code class="highlighter-rouge">$$
\max _{\theta} p(X | \theta) = \max _{\theta} \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)
$$</code>
<h2 id="22-graphical-representation-of-the-solution">2.2 Graphical representation of the solution</h2>
<p>Here we jump straight into the solution of EM and obtain a graphical representation of the iterative update rule.
<p>We wish to find a set of parameters <code class="highlighter-rouge">$\theta$</code> which maximises the likelihood <code class="highlighter-rouge">$\log(p(X|\theta))$</code>. This distribution can be arbitrarily complex, therefore at each timestep, we approximate this likelihood using a simpler distribution that forms a <em>lower bound</em> <code class="highlighter-rouge">$\mathcal{L}(q, \theta)$</code>, which is <strong>below the likelihood</strong> at all <code class="highlighter-rouge">$\theta$</code> values. At timestep <code class="highlighter-rouge">$k$</code>, the EM update rule are as follows:
<ul>
 <li><strong>E</strong>-step: fix <code class="highlighter-rouge">$\theta$</code>, find some arbitrary <em>variational distribution</em> <code class="highlighter-rouge">$q(z_i)$</code> which maximises the lower-bound, i.e. <code class="highlighter-rouge">$q^{k+1} = \arg\max_{q}\mathcal{L}(\theta^k, q)$</code>.
 <li><strong>M</strong>-step: fix <code class="highlighter-rouge">$q$</code>, find model parameters <code class="highlighter-rouge">$\theta$</code> that maximises our current lower-bound, i.e. <code class="highlighter-rouge">$\theta^{k+1} = \arg\max_\theta \mathcal{L}(\theta, q^{k+1})$</code>
</ul>
<p>One thing to get our head around is that in <strong>E</strong>-step, we are essentially taking derivative w.r.t. a <strong>function</strong> <code class="highlighter-rouge">$q(z_i)$</code>. These two steps can be seen more clearly in the picture below.
<p>
<center><img src="https://i.imgur.com/8egXBCs.gif" alt="drawing" width="600" /></center>
<p>Derivation later will show exactly how we have arrived at <strong>E</strong> and <strong>M</strong> steps, but jumping ahead, these two steps are equivalent to:
<div class="notice">
 <ul>
   <li><strong>E</strong>-step: set <em>variational distribution</em> <code class="highlighter-rouge">$q(z_i)$</code> to be equal (or approximately equal) to the posterior distribution, i.e. <code class="highlighter-rouge">$q^k(z_i) = p(z_i|\theta_i^k, x_i)$</code> where <code class="highlighter-rouge">$k$</code> denotes the timestep in our iteration.
   <li><strong>M</strong>-step: under our distribution <code class="highlighter-rouge">$q(z_i)$</code>, find a set of parameters <code class="highlighter-rouge">$\theta$</code> which maximises the data likelihood, i.e. <code class="highlighter-rouge">$\theta^{k} = \arg\max_\theta \sum_i \mathbb{E}_{z_i\sim q^k}\left[\log p(x_i, z_i | \theta)\right]$</code>
 </ul>
</div>
<p>
<p>Hence EM algorithm boils down to the fact that we cannot directly maximise our likelihood function, hence we maximise its lower-bound under our own <strong>variational distribution</strong> which is a simple function. At each timestep, we approximate or set this function to the posterior distribution <code class="highlighter-rouge">$p(Z|X,\theta)$</code> (<strong>E</strong>-step), and we maximises the likelihood of our data-points under this function instead (<strong>M</strong>-step). Alternating these two procedures, we are guaranteed to reach a <strong>local optima</strong> (in the picture above, depending on initialisation, we could have easily ended up on the left peak as our final solution).
<h2 id="23-detailed-derivation">2.3 Detailed Derivation</h2>
<h3 id="e-step">E-Step</h3>
<p>
<p class="notice"><strong>TL;DR</strong>: <code class="highlighter-rouge">$\;\arg\max _{q\left(z_{i}\right)} \mathcal{L}\left(\theta, q\right)=p\left(z_{i} | x_{i}, \theta\right)$</code>, to minimise the gap between our lower-bound and the likelihood under current parameters <code class="highlighter-rouge">$\theta$</code>, set our variational distribution to be the posterior.
<p><strong>Proof</strong>: first, lets define how exactly a lower-bound arises and where on earth this <strong>variational distribution</strong> <code class="highlighter-rouge">$q$</code> comes from. Decompose the likelihood:
<code class="highlighter-rouge">$$\log p(X | \theta) = \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)$$</code>
Idea here is we cannot directly maximise this equation (if you can, then do not use EM ;)), instead, we multiply top and bottom by a distribution of our choice <code class="highlighter-rouge">$q$</code> (coz why not?):
<code class="highlighter-rouge">$$\log p(X | \theta) = \sum_{i} \log\left( \sum_{c} q\left(z_{i}=c\right) \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)$$</code>
which is equivalent to:
<code class="highlighter-rouge">$$\log p(X|\theta) = \sum_{i} \log \mathbb{E}_{z_{i} \sim q} \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}$$</code>
now we can utilise Jensen’s inequality to obtain a lower bound for this equality. As a reminder, for any concave function (such as <code class="highlighter-rouge">$\log(x)$</code> in our case), <code class="highlighter-rouge">$f (\mathbb{E}[x]) \geqslant \mathbb{E}[f (x)]$</code>. We can therefore write the equation above as:
<code class="highlighter-rouge">$$
\log p(X|\theta)\geq \sum_{i} \mathbb{E}_{z_{i}\sim q} \log \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}
$$</code>
expanding the expectation out:
<code class="highlighter-rouge">$$
\log p(X|\theta)\geq\sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} = \mathcal{L}(\theta, q)
$$</code>
Our objective has the become <strong>minimising the gap</strong> between this lower-bound by optimising our arbitrary distribution <code class="highlighter-rouge">$q$</code>:
<code class="highlighter-rouge">$$
\arg\min_q \text{Objective} = \arg\min_q \left( \log p(X|\theta) - \mathcal{L}(\theta, q)\right)
$$</code>
more maths:
<code class="highlighter-rouge">$$
\text{Objective} = \sum_{i} \log p\left(x_{i} | \theta\right) - \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$</code>
for the first term, we multiply it with <code class="highlighter-rouge">$1 = \sum_c q(z_i=c)$</code>:
<code class="highlighter-rouge">$$
 =\sum_{i} \left(\log p\left(x_{i} | \theta\right)\sum_{c} q\left(z_{i}=c\right)\right) - \sum_{i} \sum_{c} \cdots
$$</code>
now we can move both summations out:
<code class="highlighter-rouge">$$
=\sum_{i}\sum_{c}\left(\log p\left(x_{i} | \theta\right) q\left(z_{i}=c\right) - q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} \right)
$$</code>
<code class="highlighter-rouge">$$
 =\sum_{i} \sum_{c} \left(q\left(z_{i}=c\right) \left(\log p\left(x_{i} | \theta\right) - \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)\right)
$$</code>
<code class="highlighter-rouge">$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p\left(x_{i}, z_{i}=c | \theta\right)}\right)\right)\right)
$$</code>
re-write the joint distribution of the denominator inside the log:
<code class="highlighter-rouge">$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p(x_i|\theta)p(z_i=c|\theta, x_i)}\right)\right)\right)
$$</code>
cancelling out the <code class="highlighter-rouge">$p\left(x_{i} | \theta\right)$</code> we obtain:
<code class="highlighter-rouge">$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\log \left(\frac{ q\left(z_{i}=c\right)}{ p\left(z_{i}=c | \theta, x_{i}\right)}\right)\right)
$$</code>
observe that the summation under <code class="highlighter-rouge">$c$</code> term corresponds to the KL divergence between <code class="highlighter-rouge">$q$</code> and <code class="highlighter-rouge">$p$</code>, we therefore obtain:
<code class="highlighter-rouge">$$
\text{Objective} = \text{KL}(q(z_i)|| p(z_i|\theta, x_i))
$$</code>
after all this faff, we see to minimise the gap (objective) between the likelihood and our lower-bound is equivalent to:
<code class="highlighter-rouge">$$
\arg \min _{q}(\log p(X | \theta)-\mathcal{L}(\theta, q)) = \arg \min _{q}\mathrm{KL}\left(q\left(z_{i}\right) \| p\left(z_{i} | \theta, x_{i}\right)\right)
$$</code>
so effectively, <strong>E</strong>-step becomes an optimisation problem with the objective function being the KL divergence between variational distribution <code class="highlighter-rouge">$q$</code> w.r.t. model posterior <code class="highlighter-rouge">$p\left(z_{i} | \theta, x_{i}\right)$</code>. There is <strong>no guarantee</strong> that the posterior has a closed-form representation. This is where the rich literature of <strong>variational inference</strong> comes in. Fortunately, with a GMM (Gaussian mixture model), the posterior has a closed form solution! Hence at each iteration, we are able to set <code class="highlighter-rouge">$q$</code> to be equal to the posterior.
<h3 id="m-step">M-Step</h3>
<p>
<p class="notice"><strong>TL;DR</strong> given a fixed variational distribution <code class="highlighter-rouge">$q$</code>, maximise the data and latent variable’s joint likelihood w.r.t. model parameters <code class="highlighter-rouge">$\theta$</code>, i.e. <code class="highlighter-rouge">$\arg\max_\theta \mathcal{L}(\theta, q) = \arg \max _{\theta} \mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]$</code> where latent variables <strong>come from our variational ditribution</strong> <code class="highlighter-rouge">$q$</code>.
<p>Recall from <strong>E</strong>-step, maximising the lower-bound is essentially maximising the likelihood w.r.t. <code class="highlighter-rouge">$\theta$</code>, the only difference is <strong>we can easily perform this maximisation</strong> since we know our distribution <code class="highlighter-rouge">$q$</code> (normally it is a Gaussian distribution). Writing the whole expectation term out:
<code class="highlighter-rouge">$$
\arg\max_\theta \mathcal{L}(\theta, q) = \arg\max_\theta \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$</code>
splitting the terms within the log:
<code class="highlighter-rouge">$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right) \left(\log p\left(x_{i}, z_{i}=c | \theta\right) - \log q\left(z_{i}=c\right)\right)
$$</code>
the last term is independent of <code class="highlighter-rouge">$\theta$</code>:
<code class="highlighter-rouge">$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right)\log p\left(x_{i}, z_{i}=c | \theta\right) + \text{const}.
$$</code>
condensing the sum over <code class="highlighter-rouge">$c$</code> into expectation, dropping the constant:
<code class="highlighter-rouge">$$
\arg \max _{\theta}\sum_{i}\mathbb{E}_{z_i}\left[ \log p(x_i,z_i=c|\theta)\right] = \mathbb{E}_{Z\sim q}\left[ \log p(X,Z|\theta)\right]
$$</code>
So the <strong>M</strong>-step is simply maximising the joint distribution between collection of latent variables <code class="highlighter-rouge">$Z$</code> and data points <code class="highlighter-rouge">$X$</code> <strong>under the variational distribution</strong> <code class="highlighter-rouge">$q$</code>:
<code class="highlighter-rouge">$$
\hat{\theta} = \arg \max _{\theta}\mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]
$$</code>
Again, we definitely know what <code class="highlighter-rouge">$p(X, Z | \theta)$</code> is, since we needed it to set up our graphical model. However, maximisation w.r.t. <code class="highlighter-rouge">$\theta$</code> can be non-trivial. In the case of GMM, with a lot of maths, we can obtain closed form solutions for model parameters <code class="highlighter-rouge">$\theta$</code>.
<h1 id="3-em-in-practice-gaussian-mixture-model">3. EM in Practice: Gaussian Mixture Model</h1>
<h2 id="31-derivation">3.1 Derivation</h2>
<h2 id="32-implementation">3.2 Implementation</h2>
</article>
<aside class="related">
<h2>Related posts</h2>
<ul class="related-posts">
</aside>
           <footer>
    <span>© 2020. Powered by Jekyll & Dactl
</span>
    <span>written by Tom Lu</span>
</footer>
       </div>
<script type="text/javascript" src="/assets/js/theme.js"></script>
<script type="text/javascript" src="/assets/js/barefoot.js"></script>
<script src="//yihui.org/js/math-code.js"></script>
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
