<!DOCTYPE html>
<html lang="en">
   <head>
       <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, user-scalable=no">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<title>Variational Inference Part 1 &#9642; Lu Blog</title>
<!--
<meta name="description" content="(30 min) Part 1 of 2 series on variational inference. This part first introduces high-level concepts and the mean-field approximation. We then demonstrate the mean-field theory on a Bayesian Gaussian Mixture Model (Bayesian GMM).
">
-->
<meta name="description" content="© 2020. Powered by Jekyll & Dactl
">
<meta name="keywords" content="theory, bayesian">
<link rel="canonical" href="http://localhost:4000/posts/vi1">
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Variational Inference Part 1" />
<meta name="twitter:description" content="© 2020. Powered by Jekyll & Dactl
" />
<meta name="twitter:image" content="http://localhost:4000" />
<meta name="author" content="">
<link rel="author" href="">
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Variational Inference Part 1">
<meta property="og:description" content="© 2020. Powered by Jekyll & Dactl
">
<meta property="og:url" content="http://localhost:4000/posts/vi1">
<meta property="og:site_name" content="Lu Blog">
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source Serif Pro:400,400italic,700,700italic">
<style>
    html {
      font-family: "Source Serif Pro", -apple-system, BlinkMacSystemFont, "Helvetica Neue", sans-serif;
    }
</style>
<link rel="stylesheet" href="/assets/css/bf.css">
   <body>
       <div class="wrapper" id="blep">
          <header>
   <div class="menu">
     <div class="logo">
        <a href="/">Lu Blog</a>
        <object type="image/svg+xml" data="" class="logosvg">Your browser does not support svg images</object>
     </div>
       <ul>
           <li><a href="/">home</a>
           <li><a href="/about">about</a>
           <li><a href="/archive">archive</a>
       </ul>
   </div>
   <div class="social">
     <ul>
       <li>
            <a href="https://www.linkedin.com/in/tomlu97" target="_blank" class="smaller">
              <span class="icon-stackoverflow"></span>
            </a>
       <li>
            <a href="https://github.com/xl402" target="_blank" class="smaller">
              <span class="icon-github"></span>
            </a>
       <li>
            <a href="#" onclick="switchTheme()" title="Switch theme?">
              <span class="icon-invert_colors" id="theme-switcher"></span>
            </a>
     </ul>
   </div>
</header>
<article class="post">
<style>
  .vi1 {
    margin-bottom: 2em;
  }
  .vi1::before {
    background-image: url('https://filmot.com/bCo5hZo.jpg');
    background-size: cover;
    -webkit-filter: grayscale(1) brightness(0.5) contrast(0.5);
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    z-index: -2;
  }
  .vi1::after {
    content:'';
    position:absolute;
    width: 100%;
    left: 0;
    top: 0;
    background-color: rgba(100,0,255,0.8);
    /*mix-blend-mode: darken;
    mix-blend-mode: color-burn;
    mix-blend-mode: hard-light;*/
    mix-blend-mode: overlay;
    z-index: -1;
  }
  .vi1-container,
  .vi1-container::before,
  .vi1-container::after {
    height: 25rem;
  }
  .vi1-bleed-container,
  .vi1-bleed-container::before,
  .vi1-bleed-container::after {
    height: 25rem;
  }
    @media (max-width: 48rem) {
        .vi1-container,
        .vi1-container::before,
        .vi1-container::after{
            height: 20rem;
            margin-bottom: 3rem;
        }
    }
</style>
 <div class="post-title-container
  vi1 vi1-bleed-container bleed-hero-container
    ">
    <!--Post hero image source-->
   <div class="heading-container hero-heading hero-heading-post">
     <h1>
          Variational Inference Part 1
     </h1>
     <div class="post-meta">
        <span>16/06/2020</span>
        <span>
              <a href="/tag/theory">#theory</a>
              <a href="/tag/bayesian">#bayesian</a>
        </span>
     </div>
   </div>
 </div>
   <p class="lead">(30 min) Part 1 of 2 series on variational inference. This part first introduces high-level concepts and the mean-field approximation. We then demonstrate the mean-field theory on a Bayesian Gaussian Mixture Model (Bayesian GMM).
<p>
<h1 id="1-introduction">1. Introduction</h1>
<p>Bayesian inference offers an attractive story with principled uncertainty estimation and the ability to combine expert knowledge into the model. For reasons that will hopefully become clear later, the challenge in Bayesian inference is to be able to provide both fast and scalable products during both development and serving. In this post, I will summarise what variational inference is trying to solve, it’s benefits and potential pitfalls. I will then introduce the mean-field method, and use it to solve a Bayesian Gaussian Mixture Model. In the second post, I will elaborate on black-box variational inference methods.
<p>VI is an advanced topic, so intermediate-level probability theory and ML knowledge are assumed. Knowledge of the <a href="https://www.youtube.com/watch?v=ErfnhcEV1O8"><em>Kullback–Leibler</em></a> (KL) divergence is also expected. To better understand Bayesian GMM (which will be maths heavy), I would like to recommend my previous post on <a href="https://xl402.github.io/posts/em">Expectation Maximisation</a> and <a href="https://www.youtube.com/watch?v=REypj2sy_5U&amp;t=329s">this</a> video for intuition. Let’s dive in!
<h1 id="2-theory-of-vi">2. Theory of VI</h1>
<h2 id="21-what-is-it-trying-to-solve">2.1 What Is It Trying to Solve?</h2>
<p>Let’s start with the classic bayesian inference equation:
<code class="highlighter-rouge">$$p(\theta|X) = \dfrac{p(X|\theta)p(\theta)}{p(X)}$$</code>
in words:
<code class="highlighter-rouge">$$\text{posterior} = \dfrac{\text{likelihood}\times\text{prior}}{\text{evidence}}$$</code>
following this equation, there are three steps involving developing a Bayesian inference model.
<ol>
 <li>Build a <strong>generative model</strong>: choose prior and choose likelihood, i.e. how can our data be generated using a probabilistic model? (GMM for discovering mixtures, LDA for discovering topics). This step results in an easily evaluated joint distribution <code class="highlighter-rouge">$p(X, \theta)$</code>
 <li>Compute posterior <code class="highlighter-rouge">$p(\theta|X)$</code> (from what distribution do our parameters actually come from after observing the data? As suppose to our belief, i.e. prior distribution)
 <li>Report a summary, e.g. posterior means and (co)variances. Inform your boss/client about the ‘insights’ provided by the model, i.e. “on average 30% of our data come from distribution A, with an uncertainty of ±5% etc.”
</ol>
<p>Each of the three steps alone requires a separate blog post(s). <strong>We only focus on step 2</strong>, i.e. how do we compute the posterior? This is in fact a very hard problem because of the normalising <code class="highlighter-rouge">$p(X)$</code> (evidence) term. The only way to evaluate it exactly is to compute:
<code class="highlighter-rouge">$$p(X) = \int_\theta p(X|\theta)p(\theta)$$</code>
suppose in our model, our parameter space includes things like “cluster mean” <code class="highlighter-rouge">$\mu_1, \cdots \mu_M$</code>, “cluster variance” <code class="highlighter-rouge">$\sigma^2_1, \cdots \sigma^2_M$</code>, etc, this integral over <code class="highlighter-rouge">$\theta$</code> becomes:
<code class="highlighter-rouge">$$p(X) = \int_{\mu_1}\cdots \int_{\mu_M}\int_{\sigma^2_1}\cdots \int_{\sigma^2_M} p(X|\mu_1, \cdots \sigma^2_M)d\mu_1\cdots d\sigma^2_M$$</code>
<p>not really looking pretty… since we are integrating over some pretty complicated, high dimensional function over a high dimensional space. As with a lot of real life problems, this integration typically has no closed form, and it will be extremely inefficient to use numerical integration tools due to the high dimensionality.
The gold standard approach to obtain the posterior <code class="highlighter-rouge">$p(\theta|X)$</code> is to use Markov Chain Monte Carlo (MCMC), which deserves a separate blog post. But in short MCMC provides an unbiased estimate of the posterior, i.e. if you run it for long enough, you are guaranteed to become arbitrarily close to the posterior. But we run into the same problem of “running it for long enough”, turns out, for large dataset and parameters, “long enough” is “too long to be acceptable for development/production”. What would be a <strong>fast</strong> and <strong>reliable</strong> (good enough approximate) of the posterior? Here comes variational inference!
<h2 id="22-vi-overview">2.2 VI Overview</h2>
<p>Essentially, variational inference turns the <em>numerical integral</em> problem into an <em>optimisation</em> problem. Remember what we want to obtain is the posterior distribution <code class="highlighter-rouge">$p(\theta|X)$</code>, which can be pretty complicated looking (we in fact don’t normally know what it looks like). In VI, we try to approximate this distribution with a <em>nice</em> distribution <code class="highlighter-rouge">$q(\theta)$</code> (more on <em>“nice”</em> later, for now, picture Gaussian). Graphically speaking, left figure below shows what we might want to achieve:
<center><img src="https://i.filmot.com/umR5NCD.png" alt="drawing" width="800" /></center>
<p>
<p>To think about this in terms of optimisation: the posterior distribution lives in some complicated space, we wish to find the optimum distribution <code class="highlighter-rouge">$q^*(\theta)$</code> which is <em>closest</em> to the posterior, and come from a family of <em>“nice”</em> distribution (pictorially shown on the right). i.e.
<code class="highlighter-rouge">$$q^* = \arg\min_{q\in Q}c(q(\theta), p(\theta|X))$$</code>
with <code class="highlighter-rouge">$c$</code> being the “closeness” function. Two things we need to further define - what is <em>“nice”</em> and what is <em>“close”</em>?
Let’s talk about <em>“nice”</em> first. Recall what we really want at the end of the day is to be able to report <strong>mean</strong> and <strong>variance</strong> of the (approximated) posterior distribution. Hence the family of distributions <code class="highlighter-rouge">$Q$</code> are typically:
<ul>
 <li>low dimensional (which motivates the idea of mean-field as we will see later)
 <li>often belong to the <a href="https://stats.stackexchange.com/questions/411893/advantages-of-the-exponential-family-why-should-we-study-it-and-use-it">exponential family</a> (Gaussian, exponential, Bernoulli, categorical, beta, Dirichlet + pretty much every distribution that came up during our undergraduate course except for binomial and multinomial)
</ul>
<p>Next on the topic of <em>“close”</em>, note I did not use the word <em>distance</em>, since <em>distance</em> between two distributions entails both symmetry and positive definiteness. The default “closeness” function <code class="highlighter-rouge">$c()$</code> that is used in VI is in fact the Kullback-Leibler (KL) divergence:
<code class="highlighter-rouge">$$KL(q(\theta)|| p(\theta | X))$$</code>
which is definitely <strong>not symmetric</strong>. While it seems to be a reasonable function to choose for measuring closeness between two distributions, we can certainly question:
<ol>
 <li>why KL? Why not other metrics?
 <li>why did we choose the order of divergence to be <code class="highlighter-rouge">$q||p$</code> but not <code class="highlighter-rouge">$p||q$</code>
</ol>
<p>the short answer for the first question is that - because it makes maths simpler and it is <strong>shown</strong> to provide good approximations. The complete maths for VI is as follows, our objective is:
<code class="highlighter-rouge">$$q^{*}=\operatorname{argmin}_{q \in Q} \operatorname{KL}(q(\theta) \| p(\theta | X))$$</code>
we know under KL measurement, if <code class="highlighter-rouge">$Q$</code> covers all the distributions there are, we just need to set <code class="highlighter-rouge">$q(\theta)=p(\theta | X)$</code> to achieve <code class="highlighter-rouge">$\operatorname{KL}=0$</code>. But <strong>we do not know what the posterior is</strong>, nor do we want to calculate it via that intractable integral. Instead, we write KL in it’s full form:
<code class="highlighter-rouge">$$\operatorname{KL}(q(\theta) \| p(\theta | X)) = \int_\theta q(\theta) \log \frac{q(\theta)}{p(\theta | X)} d \theta \quad [1]$$</code>
and re-write the posterior:
<code class="highlighter-rouge">$$\operatorname{KL}(\cdots)=\int_\theta q(\theta) \log \frac{q(\theta) p(X)}{p(\theta, X)} d \theta=\log p(X)-\int_\theta q(\theta) \log \frac{p(\theta, X)}{q(\theta)} d \theta \quad [2]$$</code>
and recall <code class="highlighter-rouge">$p(X)$</code> term is intractable, but it does not depend on <code class="highlighter-rouge">$q$</code>, hence to find the optimum <code class="highlighter-rouge">$q$</code> we can completely avoid this intractable term! <code class="highlighter-rouge">$p(\theta, X)$</code> can be evaluated easily since we needed it to construct our model in the first place (step one of the three procedures in the previous section).  Our objective function becomes:
<code class="highlighter-rouge">$$q^* = \operatorname{argmax}_{q \in Q}\int_\theta q(\theta) \log \frac{p(\theta, X)}{q(\theta)} d \theta$$</code>
(argmax because we are minimising the negative). This objective function is given the confusing name “Evidence lower bound” (ELBO), because if we look at equation <code class="highlighter-rouge">$[2]$</code>, KL divergence is none-negative, the evidence term <code class="highlighter-rouge">$\log p(X)$</code> is a constant, hence the remaining term is a lower bound on the evidence term.
I find this ELBO objective more intuitive by expanding it fully:
<code class="highlighter-rouge">$$q^{*}=\operatorname{argmax}_{q \in Q}\text{ELBO} = \int_\theta q(\theta) \log p(\theta, X)d\theta - \int_\theta q(\theta)\log q(\theta)$$</code>
<code class="highlighter-rouge">$$ = \operatorname{argmax}_{q \in Q} \mathbb{E}_{\theta \sim q}[p(\theta, X)] + \mathcal{H}(q(\theta))$$</code>
where <code class="highlighter-rouge">$\mathcal{H}(q(\theta))$</code> is the entropy of our posterior approximation (a.k.a variational distribution). We see maximising the ELBO objective has two contradictive and competing sub-objectives:
<ol>
 <li><code class="highlighter-rouge">$\mathbb{E}_{\theta \sim q}[p(\theta, X)]$</code> can be seen as maxising the likelihood of our dataset being generated by our parameters (drawn from <code class="highlighter-rouge">$q$</code>) and our model (fixed). <strong>Prefers point estimates of <code class="highlighter-rouge">$\theta$</code></strong>
 <li><code class="highlighter-rouge">$\mathcal{H}(q(\theta))$</code> entropy term, which <strong>prefers uncertain estimates of <code class="highlighter-rouge">$\theta$</code></strong>
</ol>
<p>It is these two competing objectives that makes Bayesian inference so attractive, and less prone to overfitting! i.e. we want a good fit to our dataset but also uncertainty in our parameters.
<p class="notice">To summarise, variational inference aims to approximate the intractable posterior <code class="highlighter-rouge">$p(\theta|X)$</code> with some “nice” distribution <code class="highlighter-rouge">$q^*(\theta)$</code>. Approximation is turned into an optimisation problem, where the objective function is ELBO: <code class="highlighter-rouge">$$q^{*}=\operatorname{argmax}_{q \in Q} \mathrm{ELBO} = \operatorname{argmax}_{q \in Q} \int_{\theta} q(\theta) \log \frac{p(\theta, X)}{q(\theta)} d \theta$$</code>
<p>Now we circle back to an earlier problem we raised - why did we take <code class="highlighter-rouge">$\text{KL}(q||p)$</code>? Why not <code class="highlighter-rouge">$\text{KL}(p||q)$</code>. In short, inspecting equation <code class="highlighter-rouge">$[1]$</code>, had we taken the divergence the other way round, we would end up taking expectation over our intractable posterior <code class="highlighter-rouge">$p(\theta|X)$</code>. This sort of maximisation is actually still possible, and is termed <em>Belief-Propagation</em> (which will not be discussed here). What is the effect of the order? Observe that:
<code class="highlighter-rouge">$$\mathrm{KL}(q(x) \| p(x)) = \int_x q(x)\dfrac{q(x)}{p(x)} dx $$</code>
in the cases where <code class="highlighter-rouge">$p(x)$</code> is very close or equal to zero (i.e. low-likelihood regions), the only way to minimise the divergence is to <strong>ensure <code class="highlighter-rouge">$q(x)$</code> is close to zero at these locations</strong>. Hence we say that <code class="highlighter-rouge">$\mathrm{KL}(q\| p)$</code> is <strong>zero-seeking</strong>. Similar argument may be made for <code class="highlighter-rouge">$\mathrm{KL}(p\| q)$</code> which is called <strong>zero-avoiding</strong>. This is more intuitive through graphical visualisation. Suppose we want to approximate a bimodal distribution with a Gaussian, graphs below shows the behaviour of the two minimisation methods:
<center><img src="https://i.filmot.com/GTYI3gm.png" alt="drawing" width="600" /></center>
<p>
<p>where the blue line is the true posterior <code class="highlighter-rouge">$p(\theta|X)$</code>, red lines are our variational approximations. So methods with <code class="highlighter-rouge">$q||p$</code> tends to <strong>under estimate</strong> the posterior variance, c.f. over estimation in <code class="highlighter-rouge">$p||q$</code>. Both can be quiet bad if you think about the real-life impact (under-estimating the uncertainty in getting cancer vs. over-estimating it). A lot of research is done on finding alternative methods/metrics, which of course is beyond the scope of this post.
<h2 id="23-mean-field-approximation">2.3 Mean-Field Approximation</h2>
<p>Recall we wish to approximate the posterior using a family of <em>“nice”</em> distributions <code class="highlighter-rouge">$Q$</code>, one of the criteria is low dimensionality. <code class="highlighter-rouge">$\theta$</code> contains all parameters in our model, hence it is typically high dimensional. What if we can instead fully factorise it:
<code class="highlighter-rouge">$$Q_{\text{MF}}:=\left\{q: q(\theta)=\prod_{j=1}^{J} q_{j}\left(\theta_{j}\right)\right\}$$</code>
so now we explicitly assume independence between parameters in our <strong>approximated distribution</strong>. This gives us the form of mean-field (MF) variational inference. Note this is <strong>NOT</strong> equivalent to assuming parameters are actually independent from one another.  <strong>Hence MF is not a modelling assumption</strong>, but an approximation assumption.
This factorized form of variational inference corresponds to an approximation framework developed in physics called <em>mean field theory</em>. Now we seek the distribution within the family <code class="highlighter-rouge">$Q_\text{MF}$</code> for which ELBO is the largest, substitution <code class="highlighter-rouge">$q(\theta)$</code> into ELBO:
<code class="highlighter-rouge">$$ \text{ELBO} = \int_\theta \prod_j q_j(\theta_j) \left\{ \log p(X,\theta) - \sum_j \log q_j (\theta) \right\} d\theta$$</code>
<p>It can be shown (maths is quiet involved, so ignored here), that the general expression for the optimal solution <code class="highlighter-rouge">$q^*_k (\theta_k)$</code> satisfies:
<p class="notice"><code class="highlighter-rouge">$$\log q_k^*(\theta_k) = \mathbb{E}_{\theta \sim q_{j\neq k}}[ \log p(X, \theta)] + \text{const.}\quad [3]$$</code>
which says that the log of the optimal solution for the factor <code class="highlighter-rouge">$q_k$</code> is obtained by considering the log of the joint distribution and then taking the expectation with respect to all the other factors <code class="highlighter-rouge">$\{q_j\} \;\forall j\neq k$</code>
<p>This means we can apply <strong>coordinate ascent</strong> to obtain <code class="highlighter-rouge">$q^*(\theta)$</code> by iteratively setting each factor <code class="highlighter-rouge">$q_{k}\left(\theta_{k}\right)$</code> to <code class="highlighter-rouge">$q^*_{k}\left(\theta_{k}\right)$</code> according to equation [3]. The bad news is that - in order to do MF, you have to analytically derive each update steps (just like expectation maximisation for Gaussian Mixture Models), note in equation [3] the constant term is effectively the normalising denominator if you take exponent on both side to get the actual density function <code class="highlighter-rouge">$q_{k}^{*}\left(\theta_{k}\right)$</code>. In practice, people just find <code class="highlighter-rouge">$q_{k}^{*}\left(\theta_{k}\right)$</code> by inspection, after writing down the analytical form of [3].
<p>Suppose we have gone through the trouble, deriving the update rule for each parameter in our model, and we obtain our final <code class="highlighter-rouge">$q^*(\theta)$</code>. One natural question to ask: how correct are we from the true posterior? <a href="https://www.inference.org.uk/itprnn/book.pdf">Mackay’s book</a> introduces a very simple problem: given a set of one dimensional data points <code class="highlighter-rouge">$X = [x_0, \cdots x_N]$</code>, we would like to model the distribution with a Gaussian and report it’s mean and variance.
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="err">μ</span><span class="p">,</span> <span class="err">σ</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="err">μ</span> <span class="o">+</span> <span class="err">σ</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span></code></pre></figure>
<p>I mean … if you are sane, you would probably do maximum likelihood estimation and guess that <code class="highlighter-rouge">$\mu_{\text{MLE}} = \bar{X}$</code> and <code class="highlighter-rouge">$\sigma^2_{\text{MLE}} = \text{Var}[X]$</code>. But what if we want to go Bayesian? Let’s place a Gamma prior on the precision of the normal distribution, and a normal prior on our mean:
<center><img src="https://i.filmot.com/3Y3vDVF.png" alt="drawing" width="800" /></center>
<p>
<p>Because we have used conjugate priors, we can actually get an analytical posterior via computing:
<code class="highlighter-rouge">$$p(\mu, \sigma |X) = \dfrac{p(X|\mu,\sigma)p(\mu, \sigma)}{p(X)}$$</code>
(the exact format is actually quiet involved). One can show that this definitely does not factorize, i.e. <code class="highlighter-rouge">$p(\mu, \sigma | X)\neq f_1(\mu)f_2(\sigma)$</code>. For those who are insane, we can actually use mean-field approximation on this posterior (for which we know the analytical form), i.e. we will consider a factorised variational approximation to the posterior given by:
<code class="highlighter-rouge">$$q(\mu, \sigma) = q_\mu (\mu)q_\sigma (\sigma)$$</code>
one can painstakingly derive the update rules according to equation [3] and do coordinate descent to obtain:
<center><img src="https://i.filmot.com/ulKVOW2.png" alt="drawing" width="1000" /></center>
<p>
<p>in figures above, green contours represent our true posterior, with x-axis representing posterior <code class="highlighter-rouge">$\mu$</code> and y-axis being posterior <code class="highlighter-rouge">$\sigma^{-1}$</code>. Blue lines are our variational mean-field approximation at each time-step. Red lines show our converged posterior approximation. As a sanity check this shows that MF provides a decent fit for our toy example (moments seem to match)!
What about models where we do not know the true posterior? (i.e. most real-life models)? Recall in section 2.1 we mentioned that another common method to obtain unbiased approximations of the posterior is through MCMC. If we trust MCMC (where the convergence checking is an art itself), then we can check our VI approximated mean values for parameters against MCMC’s mean. Figures below are from <a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/24305/Fosdick_washington_0250E_12238.pdf?sequence=1&amp;isAllowed=y">Fosdick 2013</a> where we see for a complicated model, VI and MCMC posteriors have good agreement.
<center><img src="https://i.filmot.com/g3Y6zZp.png" alt="drawing" width="1000" /></center>
<p>
<p>Note from figures above, VB stands for <em>variational Bayes</em>, which is used interchangeably with <em>variational inference</em>. <strong>Do not be misled by figures above, there are plenty cases where VI is unable to provide good posterior estimations!</strong> Therefore, one possible way to check VI accuracy is to use the one-time more computationally expensive MCMC (for the experiment above, MCMC took over one day), once you trust your VI (which in this case, takes less than 20 minutes), you can ditch MCMC and work with VI from there.
<h1 id="3-vi-in-practice-bayesian-gmm">3. VI in Practice: Bayesian GMM</h1>
<h2 id="31-bayesian-gmm-vs-gmm-theory-and-derivation">3.1 Bayesian GMM VS GMM: Theory and “Derivation”</h2>
<p>Mixture of Gaussians is a form of <em>latent variable models</em> where we wish to infer a set of <em>hidden variables</em> from the observed data. In our post <a href="https://xl402.github.io/posts/em">Expectation Maximisation Deep Dive</a>, we investigated in depth the update rules associated with maximum likelihood estimation of parameters for a Gaussian Mixture Model (GMM). As a recap, we have <code class="highlighter-rouge">$N$</code> observed data points whose positions <code class="highlighter-rouge">$x_n$</code> are influenced by the cluster they come from. We call <code class="highlighter-rouge">$z_n$</code>, a K-dimensional binary random variable that “one-hot encodes” which cluster the data point comes from (total number of clusters is <code class="highlighter-rouge">$K$</code>). Hence:
<code class="highlighter-rouge">$$p(x_n|z_{nk}=1) =  \mathcal{N}(x_n|\mu_k ,\Sigma_k)$$</code>
We also define the concept of mixing proportion, which is the marginal distribution over <code class="highlighter-rouge">$z$</code>:
<code class="highlighter-rouge">$$p(z_{nk}=1)=\pi_k, \quad p(z_n) = \prod_{k=1}^K \pi_k^{z_{nk}}$$</code>
Hence we can write down the joint distribution of <code class="highlighter-rouge">$x_n, z_n$</code>:
<code class="highlighter-rouge">$$p(x_n, z_n) = p(x_n|z_n)p(z_n) = \prod_{k=1}^K \pi_k^{z_{nk}} \mathcal{N}\left(x_{n} | \mu_{k}, \Sigma_{k}\right)^{z_{nk}} $$</code>
This model be compactly represented using the graphical representation shown in the left figure below.
<center><img src="https://i.filmot.com/gQD80i2.png" alt="drawing" width="650" /></center>
<p>
<p>Note in basic GMM, we did not place a prior over any model parameters (<code class="highlighter-rouge">$\pi,\; \mu, \; \Sigma\;$</code>), we instead use maximum likelihood (ML) to find ML estimates (MLE) of these parameters. We will show in the next section, MLE can easily lead to overfitting. To make a GMM Bayesian, all we do is to place priors over model parameters, i.e. let the mixing proportion <code class="highlighter-rouge">$\pi$</code> (which is multinomial) be drawn from a symmetric Dirchlet distribution:
<code class="highlighter-rouge">$$p(\pi) = \text{Dir}(\pi|\alpha_0)$$</code>
the mean and precision of each Gaussian mixture are drawn from an independent Gaussian-Wishart prior (Wishart is the conjugate prior to precision matrices):
<code class="highlighter-rouge">$$p(\mu, \Lambda)=p(\mu|\Lambda)p(\Lambda)=\prod_{k=1}^K \mathcal{N}(\mu_k|m_0, (\beta_0 \Lambda_k)^{-1})\mathcal{W} (\Lambda_k|W_0, \nu_0)$$</code>
where parameters <code class="highlighter-rouge">$\alpha_0, \beta_0, m_0, \nu_0, W_0$</code> govern the shape of prior distributions. Conditional distributions for <code class="highlighter-rouge">$z_n$</code> and <code class="highlighter-rouge">$x_n$</code> are almost identical to one described by the GMM:
<code class="highlighter-rouge">$$p(z_n|\pi) = \prod_{k=1}^{K} \pi_{k}^{z_{n k}}$$</code>
and
<code class="highlighter-rouge">$$ p(x_n|z_n,\mu, \Lambda) = \prod_{k=1}^{K} \mathcal{N}\left(x_{n} | \mu_{k} ,\Lambda_{k}^{-1}\right)^{z_{n k}}$$</code>
Right figure above shows the complete graphical representation of this Bayesian GMM model.
<p>In order to formulate a VI treatment of this model, we explicitly write down the joint distribution over all random variables:
<code class="highlighter-rouge">$$p(X, Z, \pi, \mu, \Lambda) = p(X|Z, \mu, \Lambda)p(Z|\pi)p(\pi)p(\mu|\Lambda)p(\Lambda) \quad [4]$$</code>
where <code class="highlighter-rouge">$X = \{x_0, \cdots x_N\}$</code> is our observed dataset and similarly <code class="highlighter-rouge">$Z = \{z_0, \cdots, z_N\}$</code>.
<p>We wish to obtain the posterior distribution:
<code class="highlighter-rouge">$$p(Z, \pi, \mu, \Lambda|X)$$</code> which is intractable. Instead, consider a variational distribution which factorises between the latent variables and the parameters so that:
<code class="highlighter-rouge">$$q(Z, \pi, \mu, \Lambda) = q(Z) q(\pi, \mu ,\Lambda)$$</code>
this follows the mean-field approach to VI, turns out, this is the only assumption we need to make to obtain a tractable solution to our Bayesian mixture model! Following the mean-field framework (equation [3]), to update the factor <code class="highlighter-rouge">$q(Z)$</code>, set
<code class="highlighter-rouge">$$\log q^{\star}(Z)=\mathbb{E}_{\pi, \mu, \Lambda}[\log p({X}, {Z}, \pi, \mu, \Lambda)]+\mathrm{const.}$$</code>
the exact form of <code class="highlighter-rouge">$q^*(Z)$</code> requires quiet a lot of maths, but essentially we just substitute the joint density term (equation [4]) in and take expectations w.r.t. the parameters. Jumping to the solution:
<code class="highlighter-rouge">$$q^{\star}(\mathbf{Z})=\prod_{n=1}^{N} \prod_{k=1}^{K} r_{n k}^{z_{n k}}$$</code>
where <code class="highlighter-rouge">$r_{n k} \propto \pi_{k}\left|\boldsymbol{\Lambda}_{k}\right|^{1 / 2} \exp \left\{-\frac{1}{2}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)\right\}$</code> is quiet intuitive, i.e. it is the “responsibility” term (almost identical to the GMM model), that indicates the contribution from each cluster towards our observed data points.
Next, we obtain the variational posterior for <code class="highlighter-rouge">$\pi, \mu, \Lambda$</code>:
<code class="highlighter-rouge">$$\ln q^{\star}({\pi}, {\mu}, {\Lambda}) = \mathbb{E}_Z[\log p(X, Z, \pi, \mu, \Lambda)]+\text { const. }$$</code>
it can be shown that:
<code class="highlighter-rouge">$$q^{\star}({\pi})=\operatorname{Dir}({\pi} | {\alpha}), \quad \alpha_k = \alpha_0+N_k \quad [5]$$</code>
and
<code class="highlighter-rouge">$$q^{\star}\left({\mu}_{k}, {\Lambda}_{k}\right)=\mathcal{N}\left({\mu}_{k} | {m}_{k},\left(\beta_{k} {\Lambda}_{k}\right)^{-1}\right) \mathcal{W}\left({\Lambda}_{k} | {W}_{k}, \nu_{k}\right) \quad [6]$$</code>
where
<p><code class="highlighter-rouge">$$ N_{k}=\sum_{n=1}^{N} r_{n k}, \quad\quad \bar{x}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k} {x}_{n}, \quad\quad {S}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k}\left({x}_{n}-\bar{x}_{k}\right)\left({x}_{n}-\overline{x}_{k}\right)^{\mathrm{T}}$$</code>
<p>define the “pseudo count”, “cluster mean” and “cluster variance respectively”;
<p><code class="highlighter-rouge">$$\beta_{k}=\beta_{0}+N_{k}$$</code>
<code class="highlighter-rouge">$${m}_{k}=\frac{1}{\beta_{k}}\left(\beta_{0} {m}_{0}+N_{k} \overline{x}_{k}\right)$$</code>
<code class="highlighter-rouge">$${W}_{k}^{-1}={W}_{0}^{-1}+N_{k} {S}_{k}+\frac{\beta_{0} N_{k}}{\beta_{0}+N_{k}}\left(\overline{x}_{k}-{m}_{0}\right)\left(\overline{x}_{k}-{m}_{0}\right)^{\mathrm{T}}$$</code>
<code class="highlighter-rouge">$$\nu_{k}=\nu_{0}+N_{k}$$</code>
<p>are used to parameterise the VI posterior Gaussian clusters <code class="highlighter-rouge">$q^{\star}\left(\mu_{k}, \Lambda_{k}\right)$</code>. We should not be too daunted by the form of our VI posterior. We see the form of both [5] and [6] makes sense, since our priors were Dirchlet and Gaussian-Wishart, and our VI posterior ended up being the same kind of distributions. We also see that the VI posteriors are parameterised by both prior parameters like <code class="highlighter-rouge">$\alpha_0, \beta_0, W_0$</code> and <code class="highlighter-rouge">$\nu_0$</code> and taking into account of observed data.
<p>It is worth noting the computational cost between GMM and Bayesian GMM, by inspecting the update equations above, there is nothing fundamentally different in Bayesian GMM (computationally speaking), the most expensive step are matrix inverts, which also manifests in GMM.
<h2 id="32-implementation-and-results">3.2 Implementation and Results</h2>
<p>For my sanity, unlike the GMM algorithm, I will not be implementing Bayesian GMM from scratch. Instead, <code class="highlighter-rouge">scikit-learn</code> has readily available <code class="highlighter-rouge">BayesianGaussianMixture</code> module that does all the hard work for us. Starting with the imports:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span><span class="p">,</span> <span class="n">BayesianGaussianMixture</span></code></pre></figure>
<p>where we use <code class="highlighter-rouge">make_blobs</code> function to generate our dataset:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="err">σ</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>
                  <span class="n">cluster_std</span><span class="o">=</span><span class="err">σ</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c"># reshape to simplify plotting indexing</span></code></pre></figure>
<p>Both GMM and Bayesian GMM are implemnted and presented for comparision:
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">bgmm</span> <span class="o">=</span> <span class="n">BayesianGaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">weight_concentration_prior_type</span><span class="o">=</span><span class="s">"dirichlet_distribution"</span><span class="p">,</span>
                               <span class="n">init_params</span><span class="o">=</span><span class="s">'random'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">bgmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">blabels</span> <span class="o">=</span> <span class="n">bgmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></code></pre></figure>
<p>That’s it! Forget about the maths above. Figures below show the fitted model and posterior for cluster size equal to 3 (sorry about the notation, <code class="highlighter-rouge">$M$</code> in the plots means the same as <code class="highlighter-rouge">$K$</code>, i.e. number of clusters). We see both GMM and Bayesian GMM provide very good estimates.
<center><img src="https://i.filmot.com/26hk5pY.png" alt="drawing" width="1000" /></center>
<p>
<p>The power of Bayesian GMM is observed when <strong>you do not know how many clusters there are</strong>, which is a very common thing to high dimensional, unknown dataset. Let’s set number of clusters to be equal to 20. Figures below show that GMM completely overfits the dataset, whereas Bayesian GMM is not fooled!
<center><img src="https://i.filmot.com/tN24Yck.png" alt="drawing" width="800" /></center>
<p>
<p>Indeed, if we plot out the posterior mixing proportions, we see GMM places almost uniform proportion across all clusters, whereas Bayesian GMM has managed to identify that there are only three clusters!
<center><img src="https://i.filmot.com/YS03aiH.png" alt="drawing" width="1000" /></center>
<p>
<p>Of course, one can argue that through cross-validation, we can tune the optimum number of clusters for GMM. But a) Bayesian GMM has the same computational cost as GMM and b) We can obtain uncertainty in posterior parameters which is an added bonus! So in short, Bayesian GMM beats GMM :)
<p>An obvious downside of the mean-field approach is the maths one needs to go through (we have in fact omitted 90% of the actual derivation), another issue lies in scalability. Part 2 of the post will cover the concept of black-box inference.
</article>
<aside class="related">
<h2>Related posts</h2>
<ul class="related-posts">
   <li>
    <a href="/posts/vi2">
        <span>Variational Inference Part 2</span>
        <small>05/07</small>
    </a>
   <li>
    <a href="/posts/em">
        <span>Expectation Maximisation Deep Dive</span>
        <small>12/06</small>
    </a>
</aside>
           <footer>
    <span>© 2020. Powered by Jekyll & Dactl
</span>
    <span>written by Tom Lu</span>
</footer>
       </div>
<script type="text/javascript" src="/assets/js/theme.js"></script>
<script type="text/javascript" src="/assets/js/barefoot.js"></script>
<script src="//yihui.org/js/math-code.js"></script>
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
