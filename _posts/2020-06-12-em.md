---
layout: post
title:  "Expectation Maximisation Deep Dive"
tags:
  - theory
  - bayesian
hero: https://imgur.com/5zMQ7ys.jpg
overlay: red
published: true
---
This post covers the in-depth theory regarding the EM algorithm, with python implementation for a Gaussian Mixture Model from scratch.
{: .lead}
<!–-break-–>

# 1. Introduction
Expectation maximisation (EM) is one of those things that I was taught several times at University, each time it was explained slightly differently, and I was confused by it every single time. The confusion is mostly because we never implemented any of the EM algorithms through Python, and EMs were always introduced to solve a **specific** problem.

Without a doubt, EM serves as a foundation for understanding variational inference. Here, I will try to go in-depth and derive and *E* and *M* steps under a **general** framework. Later I will derive and implement a *Gaussian Mixture Model* using EM.

This post is based on Coursera's [*Bayesian Methods for Machine Learning*](https://www.coursera.org/learn/bayesian-methods-in-machine-learning) specialisation course. Intermediate-level probability theory and ML knowledge are assumed. Knowledge of the [*Kullback–Leibler*](https://www.youtube.com/watch?v=ErfnhcEV1O8) (KL) divergence and [*Jensen's inequality*](https://www.youtube.com/watch?v=HfCb1K4Nr8M)  are also expected (link to videos explaining the concepts).

# 2. Theory of EM
## 2.1 What is it trying to solve?
EM aims to tackle a family of models termed **latent variable models** (examples like GMM, LDA, etc). Their graphical representation can be compactly represented as:

<center><img src="https://i.imgur.com/U6RLrrJ.png" alt="drawing" width="250"/></center>
<p></p>

where `$x_i$` is our data-points in the dataset `$X = \{x_0, \cdots x_N\}$` and `$z_i$` is the latent variables associated with each datapoint. The edge represents that `$x_i$` is conditioned by `$z_i$`. Albeit this graphical model looks simple, to fully specify the model, we need to:
- Specify prior distribution of `$z_i$`
- Specify the conditional distribution `$p(x_i|z_i)$`

both these two distributions can be arbitrarily complex (which is a problem during both model fitting and inference). If we **summarise** all the model parameters associated with our problem to be `$\theta$`, our goal becomes finding a set of parameters `$\theta$` which **maximises the likelihood** of our dataset `$X$` given our model:
`$$
\max_{\theta}p(X|\theta)
$$`
We assume our datapoints are *i.i.d* under our model, therefore:
`$$
\max_{\theta}p(X|\theta) = \max_\theta \prod p(x_i|\theta)
$$`
simplifying the product terms with a sum by maximising the log-likelihood:
`$$
\max_{\theta}\log p(X|\theta) = \max_\theta \sum_i \log p(x_i|\theta)
$$`
now we explicitly add in the conditional distribution using the rule of marginalisation:
`$$
\max_{\theta}\sum_i \log\left(\sum_c p(x_i, z_i=c|\theta)\right)
$$`
where for simplicity, we assume **discrete distribution** for `$z_i$` (derivation holds if you replace the sum with an integral). The support for `$z_i$` is `$[0, 1, \cdots, c]$`, i.e. it is categorical.

That's it! EM tries to find `$\theta$` that maximises the likelihood of our data, and through some maths we have shown:

`$$
\max _{\theta} p(X | \theta) = \max _{\theta} \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)
$$`
{: .notice}

## 2.2 Graphical representation of the solution

Here we jump straight into the solution of EM and obtain a graphical representation of the iterative update rule.

We wish to find a set of parameters `$\theta$` which maximises the likelihood `$\log(p(X|\theta))$`. This distribution can be arbitrarily complex, therefore at each timestep, we approximate this likelihood using a simpler distribution that forms a *lower bound* `$\mathcal{L}(q, \theta)$`, which is **below the likelihood** at all `$\theta$` values. At timestep `$k$`, the EM update rule are as follows:
- **E**-step: fix `$\theta$`, find some arbitrary *variational distribution* `$q(z_i)$` which maximises the lower-bound, i.e. `$q^{k+1} = \arg\max_{q}\mathcal{L}(\theta^k, q)$`.
- **M**-step: fix `$q$`, find model parameters `$\theta$` that maximises our current lower-bound, i.e. `$\theta^{k+1} = \arg\max_\theta \mathcal{L}(\theta, q^{k+1})$`

One thing to get our head around is that in **E**-step, we are essentially taking derivative w.r.t. a **function** `$q(z_i)$`. These two steps can be seen more clearly in the picture below.

<p></p>
<center><img src="https://i.imgur.com/8egXBCs.gif" alt="drawing" width="600"/></center>

Derivation later will show exactly how we have arrived at **E** and **M** steps, but jumping ahead, these two steps are equivalent to:

<div class="notice" markdown="1">
- **E**-step: set *variational distribution* `$q(z_i)$` to be equal (or approximately equal) to the posterior distribution, i.e. `$q^k(z_i) = p(z_i|\theta_i^k, x_i)$` where `$k$` denotes the timestep in our iteration.
- **M**-step: under our distribution `$q(z_i)$`, find a set of parameters `$\theta$` which maximises the data likelihood, i.e. `$\theta^{k} = \arg\max_\theta \sum_i \mathbb{E}_{z_i\sim q^k}\left[\log p(x_i, z_i | \theta)\right]$`
</div>
<p></p>

Hence EM algorithm boils down to the fact that we cannot directly maximise our likelihood function, hence we maximise its lower-bound under our own **variational distribution** which is a simple function. At each timestep, we approximate or set this function to the posterior distribution `$p(Z|X,\theta)$` (**E**-step), and we maximises the likelihood of our data-points under this function instead (**M**-step). Alternating these two procedures, we are guaranteed to reach a **local optima** (in the picture above, depending on initialisation, we could have easily ended up on the left peak as our final solution).

## 2.3 Detailed Derivation
### E-Step
<p></p>
**TL;DR**: `$\;\arg\max _{q\left(z_{i}\right)} \mathcal{L}\left(\theta, q\right)=p\left(z_{i} | x_{i}, \theta\right)$`, to minimise the gap between our lower-bound and the likelihood under current parameters `$\theta$`, set our variational distribution to be the posterior.
{: .notice}
**Proof**: first, lets define how exactly a lower-bound arises and where on earth this **variational distribution** `$q$` comes from. Decompose the likelihood:
`$$\log p(X | \theta) = \sum_{i} \log \left(\sum_{c} p\left(x_{i}, z_{i}=c | \theta\right)\right)$$`
Idea here is we cannot directly maximise this equation (if you can, then do not use EM ;)), instead, we multiply top and bottom by a distribution of our choice `$q$` (coz why not?):
`$$\log p(X | \theta) = \sum_{i} \log\left( \sum_{c} q\left(z_{i}=c\right) \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)$$`
which is equivalent to:
`$$\log p(X|\theta) = \sum_{i} \log \mathbb{E}_{z_{i} \sim q} \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}$$`
now we can utilise Jensen's inequality to obtain a lower bound for this equality. As a reminder, for any concave function (such as `$\log(x)$` in our case), `$f (\mathbb{E}[x]) \geqslant \mathbb{E}[f (x)]$`. We can therefore write the equation above as:
`$$
\log p(X|\theta)\geq \sum_{i} \mathbb{E}_{z_{i}\sim q} \log \frac{p\left(x_{i}, z_{i} | \theta\right)}{q\left(z_{i}\right)}
$$`
expanding the expectation out:
`$$
\log p(X|\theta)\geq\sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} = \mathcal{L}(\theta, q)
$$`
Our objective has the become **minimising the gap** between this lower-bound by optimising our arbitrary distribution `$q$`:
`$$
\arg\min_q \text{Objective} = \arg\min_q \left( \log p(X|\theta) - \mathcal{L}(\theta, q)\right)
$$`
more maths:
`$$
\text{Objective} = \sum_{i} \log p\left(x_{i} | \theta\right) - \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$`
for the first term, we multiply it with `$1 = \sum_c q(z_i=c)$`:
`$$
 =\sum_{i} \left(\log p\left(x_{i} | \theta\right)\sum_{c} q\left(z_{i}=c\right)\right) - \sum_{i} \sum_{c} \cdots
$$`
now we can move both summations out:
`$$
=\sum_{i}\sum_{c}\left(\log p\left(x_{i} | \theta\right) q\left(z_{i}=c\right) - q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)} \right)
$$`
`$$
 =\sum_{i} \sum_{c} \left(q\left(z_{i}=c\right) \left(\log p\left(x_{i} | \theta\right) - \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}\right)\right)
$$`
`$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p\left(x_{i}, z_{i}=c | \theta\right)}\right)\right)\right)
$$`
re-write the joint distribution of the denominator inside the log:
`$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\left(\log \left(\frac{p\left(x_{i} | \theta\right) q\left(z_{i}=c\right)}{p(x_i|\theta)p(z_i=c|\theta, x_i)}\right)\right)\right)
$$`
cancelling out the `$p\left(x_{i} | \theta\right)$` we obtain:
`$$
=\sum_{i} \sum_{c}\left(q\left(z_{i}=c\right)\log \left(\frac{ q\left(z_{i}=c\right)}{ p\left(z_{i}=c | \theta, x_{i}\right)}\right)\right)
$$`
observe that the summation under `$c$` term corresponds to the KL divergence between `$q$` and `$p$`, we therefore obtain:
`$$
\text{Objective} = \text{KL}(q(z_i)|| p(z_i|\theta, x_i))
$$`
after all this faff, we see to minimise the gap (objective) between the likelihood and our lower-bound is equivalent to:
`$$
\arg \min _{q}(\log p(X | \theta)-\mathcal{L}(\theta, q)) = \arg \min _{q}\mathrm{KL}\left(q\left(z_{i}\right) \| p\left(z_{i} | \theta, x_{i}\right)\right)
$$`
so effectively, **E**-step becomes an optimisation problem with the objective function being the KL divergence between variational distribution `$q$` w.r.t. model posterior `$p\left(z_{i} | \theta, x_{i}\right)$`. There is **no guarantee** that the posterior has a closed-form representation. This is where the rich literature of **variational inference** comes in. Fortunately, with a GMM (Gaussian mixture model), the posterior has a closed form solution! Hence at each iteration, we are able to set `$q$` to be equal to the posterior.

### M-Step
<p></p>
**TL;DR** given a fixed variational distribution `$q$`, maximise the data and latent variable's joint likelihood w.r.t. model parameters `$\theta$`, i.e. `$\arg\max_\theta \mathcal{L}(\theta, q) = \arg \max _{\theta} \mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]$` where latent variables **come from our variational ditribution** `$q$`.
{: .notice}
Recall from **E**-step, maximising the lower-bound is essentially maximising the likelihood w.r.t. `$\theta$`, the only difference is **we can easily perform this maximisation** since we know our distribution `$q$` (normally it is a Gaussian distribution). Writing the whole expectation term out:
`$$
\arg\max_\theta \mathcal{L}(\theta, q) = \arg\max_\theta \sum_{i} \sum_{c} q\left(z_{i}=c\right) \log \frac{p\left(x_{i}, z_{i}=c | \theta\right)}{q\left(z_{i}=c\right)}
$$`
splitting the terms within the log:
`$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right) \left(\log p\left(x_{i}, z_{i}=c | \theta\right) - \log q\left(z_{i}=c\right)\right)
$$`
the last term is independent of `$\theta$`:
`$$
\arg \max _{\theta} \sum_{i} \sum_{c} q\left(z_{i}=c\right)\log p\left(x_{i}, z_{i}=c | \theta\right) + \text{const}.
$$`
condensing the sum over `$c$` into expectation, dropping the constant:
`$$
\arg \max _{\theta}\sum_{i}\mathbb{E}_{z_i}\left[ \log p(x_i,z_i=c|\theta)\right] = \mathbb{E}_{Z\sim q}\left[ \log p(X,Z|\theta)\right]
$$`
So the **M**-step is simply maximising the joint distribution between collection of latent variables `$Z$` and data points `$X$` **under the variational distribution** `$q$`:
`$$
\hat{\theta} = \arg \max _{\theta}\mathbb{E}_{Z \sim q}[\log p(X, Z | \theta)]
$$`
Again, we definitely know what `$p(X, Z | \theta)$` is, since we needed it to set up our graphical model. However, maximisation w.r.t. `$\theta$` can be non-trivial. In the case of GMM, with a lot of maths, we can obtain closed form solutions for model parameters `$\theta$`.

# 3. EM in Practice: Gaussian Mixture Model

## 3.1 Derivation


## 3.2 Implementation
