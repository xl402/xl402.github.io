---
layout: post
title:  "Variational Inference Part 1"
tags:
  - theory
  - bayesian
hero: https://filmot.com/bCo5hZo.jpg
overlay: purple
published: true
---
(30 min) Part 1 of 2 series on variational inference. This part first introduces high-level concept and the mean-field approximation. We then demonstrate the mean-field theory on a Bayesian Gaussian Mixture Model (Bayesian GMM).
{: .lead}
<!–-break-–>

# 1. Introduction
Bayesian inference offers an attractive story with principled uncertainty estimation and the ability to combine expert knowledge into the model. For reasons that will hopefully become clear later, the challenge in Bayesian inference is to be able to provide both fast and scalable products during both development and serving. In this post, I will summarise what variational inference is trying to solve, it's benefits and potential pitfalls. I will then introduce the mean-field method, and use it to solve a Bayesian Gaussian Mixture Model. In the second post, I will elaborate on black-box variational inference methods.

VI is an advanced topic, so intermediate-level probability theory and ML knowledge are assumed. Knowledge of the [*Kullback–Leibler*](https://www.youtube.com/watch?v=ErfnhcEV1O8) (KL) divergence is also expected. To better understand Bayesian GMM (which will be maths heavy), I would like to recommend my previous post on [Expectation Maximisation](https://xl402.github.io/posts/em) and [this](https://www.youtube.com/watch?v=REypj2sy_5U&t=329s) video for intuition. Let's dive in!

## 2. Theory of VI
### 2.1 What is it trying to solve?
Let's start with the classic bayesian inference equation:
`$$p(\theta|X) = \dfrac{p(X|\theta)p(\theta)}{p(X)}$$`
in words:
`$$\text{posterior} = \dfrac{\text{likelihood}\times\text{prior}}{\text{evidence}}$$`
following this equation, there are three steps involving developing a Bayesian inference model.
1. Build a **generative model**: choose prior and choose likelihood, i.e. how can our data be generated using a probabilistic model? (GMM for discovering mixtures, LDA for discovering topics)
2. Compute posterior
3. Report a summary, e.g. posterior means and (co)variances. i.e. inform your boss/client about the 'insights' provided by the model.

Each of the three steps alone requires a separate blog post(s). We only focus on step 2, i.e. how do we compute the posterior? This is in fact a very hard problem because of the normalising `$p(X)$` (evidence) term. The only way to evaluate it is to compute:
`$$p(X) = \int_\theta p(X|\theta)p(\theta)$$`
suppose in our model, our parameter space includes things like "cluster mean" `$\mu_1, \cdots \mu_M$`, "cluster variance" `$\sigma^2_1, \cdots \sigma^2_M$`, etc, this integral over `$\theta$` becomes:
`$$p(X) = \int_{\mu_1}\cdots \int_{\mu_M}\int_{\sigma^2_1}\cdots \int_{\sigma^2_M} p(X|\mu_1, \cdots \sigma^2_M)d\mu_1\cdots d\sigma^2_M$$`

not really looking pretty... since we are integrating over some pretty complicated, high dimensional function over a high dimensional space. As with a lot of real life problems, this integration typically has no closed form, and it will be extremely inefficient to use numerical integration tools due to the high dimensionality.
The gold standard approach to obtain the posterior `$p(\theta|X)$` is to use Markov Chain Monte Carlo (MCMC), which deserves a separate blog post. But in short MCMC provides an unbiased estimate of the posterior, i.e. if you run it for long enough, you are guaranteed to become arbitrarily close to the posterior. But we run into the same problem of "running it for long enough", turns out, for large dataset and parameters, "long enough" is "too long to be acceptable for development/production". What would be a **fast** and **reliable** (good enough approximate) of the posterior? Here comes variational inference!

### 2.1 VI Overview
Essentially, variational inference turns the *numerical integral* problem into an *optimisation* problem. Remember what we want to obtain is the posterior distribution `$p(\theta|X)$`, which can be pretty complicated looking (we in fact don't normally know what it looks like). In VI, we try to approximate this distribution with a *nice* distribution `$q(\theta)$` (more on *"nice"* later, for now, picture Gaussian). Graphically speaking, left figure below shows what we might want to achieve:
<center><img src="https://i.filmot.com/umR5NCD.png" alt="drawing" width="800"/></center>
<p></p>

To think about this in terms of optimisation: the posterior distribution lives in some complicated space, we wish to find the optimum distribution `$q^*(\theta)$` which is *closest* to the posterior, and come from a family of *"nice"* distribution (pictorially shown on the right). i.e.
`$$q^* = \arg\min_{q\in Q}c(q(\theta), p(\theta|X))$$`
with `$c$` being the "closeness" function. Two things we need to further define - what is *"nice"* and what is *"close"*?
