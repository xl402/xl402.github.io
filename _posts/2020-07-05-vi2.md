---
layout: post
title:  "Variational Inference Part 2: Black-Box VI"
tags:
  - theory
  - bayesian
hero: https://filmot.com/myX95GO.jpg
overlay: grey
published: true
---
(30 min) Part 2 of 2 series on variational inference. This part dives into the more practical black-box variational inference. We discuss the REINFORCE algorithm and gradient variance reduction techniques (including the *neural baseline*).
{: .lead}
<!–-break-–>

# 1. Introduction

In [part I](http://tlublog.com/posts/vi1) of the series, we explored the origin of variational inference. As a quick recap, the posterior distribution `$p(\theta |X)$` is intractable to compute, as we need to explicitly compute the evidence term:

`$$p(X)=\int_{\theta} p(X \mid \theta) p(\theta)d\theta$$`

this integration over the parameter space is very very expensive.

To address this problem, we cook up a *variational distribution* `$q(\theta)$` that *approximates* the posterior. The problem of finding the posterior distribution therefore becomes an **optimisation problem**. Specifically, we wish to minimise:
`$$\text{KL}(q(\theta) \| p(\theta \mid X))$$`
we discussed the implication of the ordering between `$q$` and `$p$` inside the brackets. In order to perform the optimisation, we showed that the objective function above is equivalent to **maximising** the *evidence lower bound*:

`$$q^{*}=\operatorname{argmax}_{q \in Q}\text{ELBO} = \int_\theta q(\theta) \log p(\theta, X)d\theta - \int_\theta q(\theta)\log q(\theta)$$`
`$$ = \operatorname{argmax}_{q \in Q} \mathbb{E}_{\theta \sim q}[p(\theta, X)] + \mathcal{H}(q(\theta))\quad [1]$$`

which intuitively speaking contains the maximum likelihood term `$p(\theta, X)$` that prefers point estimation, and the entropy term `$\mathcal{H}(q(\theta))$` that prefers *diffusive* estimation. Also note how this ELBO can be estimated easily with Monte-Carlo sampling. Since by construction, we know how to generate samples from `$p(\theta, X)$`.

We then introduced an **analytical method** called *mean-field* approximation, that falls under the framework of ELBO maximisation. The major problems with the *mean-field* approach are that:

- It places heavy constraint on what our variational distribution `$q(\theta)$` (it assumes it to be fully factorised)
- Like me, you probably do not want to go through pages of maths on differentiating some disgusting looking equations (checkout the maths on Bayesian Mixture Model or Latent Dirchlet Allocation if interested)

This post describes the more modern techniques used to do variational inference: the **black-box** family. Specifically, we will look at the *reparameterisation trick* and the famous *REINFORCE* algorithm. We will also be discussing some variance reduction techniques such as the *baseline* method. To make this post less dry, I will also be including some *Pyro* (probabilistic programming language based on *PyTorch*) just to demonstrate how easy black-box VI is.

# 2. Theory of Black-Box VI

## 2.1 Why Is ELBO Backprop Hard?

One natural question to ask is - why can't we just do backpropogation through this ELBO objective function? Let us first write down our ELBO (in a more workable format):

`$$\mathrm{ELBO} = \mathbb{E}_{q_{\phi}(\theta)}\left[\log p(x, \theta)-\log q_{\phi}(\theta)\right]$$`

This is not too much change from equation [1], we made it more explicit that our variational distribution `$q$` is parameterised by `$\phi$`. Form simplicity, let's call variational distribution `$q$` the **guide** (this is commonly used in literature). The problem is that **ELBO is an expectation**, so we need to be able to compute unbiased estimates of

`$$\nabla_{\theta, \phi} \mathrm{ELBO}=\nabla_{\theta, \phi} \mathbb{E}_{q_{\phi}(\theta)}\left[\log p(x, \theta)-\log q_{\phi}(\theta)\right]$$`

Unfortunately, we **can't move the grad inside the expectation**, since our expectation is taken under our guide `$q$` that depends on `$\phi$`. This problem can be framed in a more generic way, we can drop the distinction between `$\theta$` and `$\phi$`, also let's use `$f_\phi(\theta)$` to denote any arbituary function within the bracket:

`$$\nabla_{\phi} \mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]  \quad [2]$$`

Equation [2] describes the *gradient through an expectation* and can be also found in the policy gradient algorithm in reinforcement learning. The remaining of the post aims to obtain **unbiased estimates** of equation [2].


## 2.2 Reparameterisation Trick

The reparameterisation trick also known as path-wise gradients allow us to compute equation [2] by rewritting samples from the guide `$q$` in terms of a noise variable `$\epsilon$`:

`$$\mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]=\mathbb{E}_{q(\epsilon)}\left[f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]$$`

concretely, `$\epsilon \sim q(\epsilon)$` and `$\theta = g_{\phi}(\epsilon)$`. Crucially all the `$\phi$` dependent terms have been moved inside of the expectation. This kind of reparameterisation can be done for many distributions (e.g. the normal distribution). An example of such reparameterisation can be highlighted by assuming that `$\theta$` is sampled from a Gaussian `$\theta \sim \mathcal{N}(\mu, \sigma)$`. The function `$g_{\phi}(\varepsilon)$` can then be expressed as

`$$g_{\phi}(\varepsilon)=\mu_{\phi}+\varepsilon \sigma_{\phi}$$`

where `$\epsilon \sim \mathcal{N}(0, 1)$`. Assuming `$f(.)$` and `$g(.)$` are sufficiently smooth, we can now get unbiased estimates of the gradient of interest by taking a Monte Carlo estimate of this expectation:

`$$\nabla_{\phi} \mathbb{E}_{q(\epsilon)}\left[f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]=\mathbb{E}_{q(\epsilon)}\left[\nabla_{\phi} f_{\phi}\left(g_{\phi}(\epsilon)\right)\right]$$`

In this case, not only do `$\theta$` need to come from specific distributions (i.e. the exponential family), we also need to be able to differentiate through `$f_\phi$`. In the case where we have discrete latent variables, the gradient of `$f_\phi$` is zero almost everywhere in the parameter space.

## 2.3 REINFORCE for Non-Reparameterizable Random Variables

I am introducing REINFORCE under the scope of variational inference, however, this trick is most commonly seen in reinforcement learning (policy gradient). The concept of REINFORCE is quiet simple: **re-write equation [2] to some form so we can obtain unbiased estimate via Monte-Carlo**. Specifically, we want to message a gradient of expectation into expectation of some gradient. We begin by expanding the terms:

`$$\nabla_{\phi}\text{ELBO}=\nabla_{\phi} \mathbb{E}_{q_{\phi}(\theta)}\left[f_{\phi}(\theta)\right]=\nabla_{\phi} \int d\theta q_{\phi}(\theta) f_{\phi}(\theta)$$`

applying chain rule:

`$$\int d \theta\left[\left(\nabla_{\phi} q_{\phi}(\theta)\right) f_{\phi}(\theta)+q_{\phi}(\theta)\left(\nabla_{\phi} f_{\phi}(\theta)\right)\right]$$`

next, we use the log-derivative trick:

`$$\nabla_{\phi} q_{\phi}(\theta)=q_{\phi}(\theta) \nabla_{\phi} \log q_{\phi}(\theta)$$`

to obtain

`$$\nabla_{\phi}\text{ELBO}=\mathbb{E}_{q_{\phi}(\theta)}\left[\left(\nabla_{\phi} \log q_{\phi}(\theta)\right) f_{\phi}(\theta)+\nabla_{\phi} f_{\phi}(\theta)\right] \quad [3]$$`

Equation [3] is the REINFORCE equation. It is quiet disgusting looking and I do not have any intuition to what it represent (frankly, the ELBO objective took me a while to digest). But just by looking at it, we can now obtain unbiased estimates of our ELBO gradient! One way to package this result (for implementation) is by introducing a surrogate function:

`$$\text { surrogate objective }= \log q_{\phi}(\theta) \overline{f_{\phi}(\theta)}+f_{\phi}(\theta)$$`

to be passed through the autograd, where `$\overline{f_{\phi}(\theta)}$` is held as a constant (detached during autograd). Equation [3] therefore becomes:

`$$\nabla_{\phi} \mathrm{ELBO}=\mathbb{E}_{q_{\phi}(\theta)}\left[\nabla_{\phi}(\text { surrogate objective })\right] \quad [4]$$`


It would be good to finish the story here. Unfortunately, equation [4] suffers from **high variance** for a range of `$f(.)$`. So in although we can theoretically obtain unbiased estimates of the true gradient, in reality, noone is going to wait for 10,000 samples per backprop iteration. We therefore **need ways to reduce the variance of our estimated ELBO gradient**.
