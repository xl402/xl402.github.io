---
layout: post
title:  "Variational Inference Part 2"
tags:
  - theory
  - bayesian
hero: https://filmot.com/myX95GO.jpg
overlay: grey
published: true
---
(30 min) Part 2 of 2 series on variational inference. This part dives into the more practical black-box variational inference. We discuss the REINFORCE algorithm and gradient variance reduction techniques (including the *neural baseline*).
{: .lead}
<!–-break-–>

# 1. Introduction

In [part I](http://tlublog.com/posts/vi1) of the series, we explored the origin of variational inference. As a quick recap, the posterior distribution `$p(\theta |X)$` is intractable to compute, as we need to explicitly compute the evidence term:

`$$p(X)=\int_{\theta} p(X \mid \theta) p(\theta)$$`

this integration over the parameter space is very very expensive.

We cook up a *variational distribution* `$q(\theta)$` that *approximates* the posterior. The problem of finding the posterior distribution therefore becomes an **optimisation problem**. Specifically, we wish to minimise:
`$$\text{KL}(q(\theta) \| p(\theta \mid X))$$`
we discussed the implication of the ordering between `$q$` and `$p$` inside the brackets. In order to perform the optimisation, we showed that the objective function above is equivalent to **maximising** the *evidence lower bound*:

`$$q^{*}=\operatorname{argmax}_{q \in Q}\text{ELBO} = \int_\theta q(\theta) \log p(\theta, X)d\theta - \int_\theta q(\theta)\log q(\theta)$$`
`$$ = \operatorname{argmax}_{q \in Q} \mathbb{E}_{\theta \sim q}[p(\theta, X)] + \mathcal{H}(q(\theta))$$`

which intuitively speaking contains the maximum likelihood term `$p(\theta, X)$` that prefers point estimation, and the entropy term `$\mathcal{H}(q(\theta))$` that prefers *diffusive* estimation. Also note how this ELBO can be estimated easily with Monte-Carlo sampling. Since by construction, we know how to generate samples from `$p(\theta, X)$`.

We then introduced an **analytical method** called *mean-field* approximation, that falls under the framework of ELBO maximisation. One major problem with the *mean-field* approach is that:

- It places heavy constraint on what our variational distribution `$q(\theta)$` (it assumes it to be fully factorised)
- Like me, you probably do not want to go through pages of maths on differentiating some disgusting looking equations (checkout the maths on Bayesian Mixture Model or Latent Dirchlet Allocation if interested)

This post describes the more modern techniques used to do variational inference: the **black-box** family. Specifically, we will look at the *reparameterisation trick* and the famous *REINFORCE* algorithm. We will also be discussing some variance reduction techniques such as the *baseline* method. To make this post less dry, I will also be including some *Pyro* (probabilistic programming language based on *PyTorch*) just to demonstrate how easy black-box VI is.
