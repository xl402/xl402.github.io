---
layout: post
title:  "Few-shot Learning"
tags:
  - practical
  - deep-learning
hero: https://filmot.com/JrO3BLQ.jpg
overlay: grey
published: true
---
(30 min) Literature review of popular methods in few-shot learning. With a focus on metric-learning, meta-learning and bayesian few-shots.
{: .lead}
<!–-break-–>

# 1. Introduction

It is an understatement to say that modern Deep Neural Networks (DNNs) require large amount of data during training. This can be shown both in practice and through [Learning Theory](https://mostafa-samir.github.io/ml-theory-pt2/). For example, the classic benchmark datasets such as [ImageNet](http://www.image-net.org/) contains 14 million images (for 1000 classes), language models such as [GTP3](https://arxiv.org/abs/2005.14165) are literally trained on THE INTERNET.

While these DNNs' performance have been impressive, occasionally achieving super-human level accuracy/performance. In real life, data is not as abundant and easy to obtain. The problem is exacerbated when **labelled** data is required (because £££). Another key challenge in a real-life setting is that model output requirement changes over time. For example, imagine working for an identity-document classification company; at launch, your company may only support 4 types of IDs. Over time, the number of supported IDs will grow. Re-training the model itself is an art and can be very time consuming.

When quality data is scarce and model reusability is key, **few-shot** learning have been employed successfully to discover patterns in data and make beneficial predictions. When combined with **active learning** can often bring a competitive edge at the inception of an ML company!

# 2. Problem Definition and Datasets

*Few-shot classification* is an instantiation of meta-learning in the field of supervised learning (I think the distinction between few-shot and meta-learning is rather arbitrary). The best way to understand what few-shot learning is trying to achieve is by examining a few common datasets used to train and evaluate few-shot models. Let's begin by looking at the *K-shot N-class classification task*:

*K-shot N-class classification task*: during training, we are provided with **several datasets** each containing ***N* classes with *K* examples**, at test time, we are given **a new dataset**, and asked to make predictions based on the labelled examples.
{: .notice}

The term few-shot (or *K*-shot) therefore means the number of examples provided per class (or task). Pictorially, this is shown as a **dataset of datasets**:

<center>
<img src="https://i.filmot.com/XJtP39p.png" alt="drawing" width="850" />
<figcaption>
Fig 1. Typical dataset used for evaluating few-shot models. Note the unusualness of this "dataset of datasets". Image is taken from <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html">LiLian's blog</a>.
</figcaption>
</center>

<p></p>

To be more concrete, the [Omniglot](https://github.com/brendenlake/omniglot/) dataset contains 50 different alphabets, each containing different number (around 20) characters. Each character is drawn by 20 people, amounting to a total of 1623 characters.

<center>
<img src="https://i.filmot.com/qKEb5Mn.png" alt="drawing" width="700" />
<figcaption>
Fig 2. Omniglot dataset
</figcaption>
</center>

<p></p>

Going back to the earlier definition, each alphabet's OCR problem can be characterised as an *S*-class 20-shots. Where *S* corresponds to the size of each alphabet.

One of the most challenging dataset used for few-shots learning is the [mini-ImageNet](https://www.kaggle.com/whitemoon/miniimagenet) which contains 100 classes, each with only 600 examples. It is worth mentioning, the way train-val-test split for few-shots learning is **done on the classes**, in this case, we typically have a 64-16-20 split.

<center>
<img src="https://i.filmot.com/79Yb3nb.png" alt="drawing" width="700" />
<figcaption>
Fig 3. Mini-ImageNet dataset
</figcaption>
</center>

<p></p>

Hopefully I have conveyed how challenging few-shot learning is. Personally, I think few-shot is more general than the typical supervised DNNs. People normally call this type of learning "learning to learn", since our model needs to either have weights/architecture that generalises across different tasks, or to be able to rapidly change the weights to adapt to a new task.

# 3. Methods/Literature Overview

One rather obvious (though practically difficult) thing to achieve few-shots learning is through **data augmentation**. The motivation is simply: we do not have enough example data per class, let's just generate more. People can really go crazy on data augmentation, recent methods use ideas such as [GANs](https://papers.nips.cc/paper/7504-metagan-an-adversarial-approach-to-few-shot-learning) and [Sailency-guided Hallucinations](https://arxiv.org/abs/1904.03472). Since we can always perform data augmentation in conjunction with other approaches (and frankly, the concept is not new), this class of method will not be discussed in this post.

The other approach which is done by most ML practitioners is **transfer learning**. The idea of taking an off-the-shelf model trained by big research groups on millions of data points, then fine tune the last few layer's weights on our own dataset is definitely not new. Therefore, we will also omit discussing these. In fact, for most methods demonstrated, assume we always do some sort of transfer learning via models like VGG net.

**Metric-based learning** such as the [*Siamese networks*](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) and [*Matching networks*](https://arxiv.org/abs/1606.04080) will be described in depth here. The fundamental idea is that these methods learn a distance metric between the querying data point and the support set (examples).

**Optimisation-based learning** such as the [*LSTM Meta-Learner*](https://openreview.net/pdf?id=rJY0-Kcll) and [*Model-Agnostic Meta-Learning*](https://arxiv.org/abs/1703.03400)













<center><img src="https://i.filmot.com/Ei8dDIO.png" alt="drawing" width="450"/></center>

<center><img src="https://i.filmot.com/n2ZygDK.png" alt="drawing" width="450"/></center>

<center><img src="https://i.filmot.com/UbMjRGh.png" alt="drawing" width="450"/></center>

<center><img src="https://i.filmot.com/H4wVk7g.png" alt="drawing" width="700"/></center>
