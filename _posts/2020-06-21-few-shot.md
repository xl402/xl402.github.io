---
layout: post
title:  "Few-shot Learning"
tags:
  - practical
  - deep-learning
hero: https://filmot.com/JrO3BLQ.jpg
overlay: grey
published: true
---
(30 min) Literature review of popular methods in few-shot learning. With a focus on metric-learning, meta-learning and bayesian few-shots.
{: .lead}
<!–-break-–>

It is an understatement to say that modern Deep Neural Networks (DNNs) require large amount of data for them to perform well. This can be shown both in practice and through [Learning Theory](https://mostafa-samir.github.io/ml-theory-pt2/). For example, the classic benchmark datasets such as [ImageNet](http://www.image-net.org/) contains 14 million images (for 1000 classes), language models such as [GTP3](https://arxiv.org/abs/2005.14165) are literally trained on THE INTERNET.

While these DNNs' performance have been impressive, occasionally achieving super-human level accuracy/performance. In real life, data is not as abundant and easy to obtain. The problem is exacerbated when **labelled** data is required (because £££). Another key challenge in a real-life setting is that model output requirement changes over time. For example, imagine working for an identity-document classification company; at launch, your company may only support 4 types of IDs. Over time, the number of supported IDs will grow. Re-training the model itself is an art and can be very time consuming.

When quality data is scarce and model reusability is key, **few-shot** learning have been employed successfully to discover patterns in data and make beneficial predictions. When combined with **active learning** can often bring a competitive edge at the inception of an ML company!





<center><img src="https://i.filmot.com/Ei8dDIO.png" alt="drawing" width="450"/></center>

<center><img src="https://i.filmot.com/n2ZygDK.png" alt="drawing" width="450"/></center>

<center><img src="https://i.filmot.com/UbMjRGh.png" alt="drawing" width="450"/></center>

<center><img src="https://i.filmot.com/H4wVk7g.png" alt="drawing" width="700"/></center>
